
\mychapter{Introduction}

\vskip 0.5in

\centerline{\Huge\bf Who Wants to be a Mathematician?}

\vskip 0.5in
\centerline{\Huge\bf \$50,000 Question\footnote{The instructor may not necessarily possess a \$50,000 prize fund.  The author most certainly does not.}}

\vskip 0.5in
\begin{center}
{\LARGE\bf Which of the following integrals can {\it not} be expressed as an elementary function?}
\end{center}

\begin{center}
{\Huge A. $\int \sin x \, \ud x$}

\vskip 12pt
{\Huge B. $\int e^{-x^2} \ud x$}

\vskip 12pt
{\Huge C.}\,\,{\huge $\int{{x\{(x^2e^{2x^2}-\ln^2(x+1))^2+2xe^{3x^2}(x-(2x^3+2x^2+x+1)\ln(x+1))\}}\over{(x+1)(\ln^2(x+1) - x^2e^{2x^2})^2}} \ud x$}

\vskip 12pt
{\Huge D. $\int {{2x^6+4x^5+7x^4-3x^3-x^2-8x-8}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \ud x$}

\end{center}

\vfill\eject

The answer to this ``\$50,000'' question is, somewhat surprisingly,
(B).  Simplifying (A) as $\int \sin \ud x = -\cos x + C$ is an easy exercise from
a first year calculus course.  (C) and (D), while appearing
more formidable, are solvable using the techniques of this book.

(C) is example
\ref{hard log-exp integral}, and can be written as:

$$\int{{x\{(x^2e^{2x^2}-\ln^2(x+1))^2+2xe^{3x^2}(x-(2x^3+2x^2+x+1)\ln(x+1))\}}\over{(x+1)(\ln^2(x+1) - x^2e^{2x^2})^2}} dx$$
$$= x - \ln(x+1) - {x e^{x^2}\ln(x+1)\over{\ln^2(x+1)-x^2 e^{2x^2}}}
+ {1\over2}\ln\frac{\ln(x+1) + x e^{x^2}}{\ln(x+1) - x e^{x^2}}$$

(D) is example \ref{Chebyshev's Integral}:

$$A(x) = 1023x^8+4104x^7+5048x^6+2182x^5+805x^4+624x^3+10x^2+28x$$
$$B(x) = 1025x^{10} + 6138x^9 + 12307x^8 + 10188x^7 + 4503x^6 + 3134x^5 + 1598x^4 + 140x^3 + 176x^2 +2$$
$$C(x) = 32x^{10}-80x^8+80x^6-40x^4+10x^2-1$$

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)y}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \,{\rm d}x
= {{(x+{1\over2})y}\over{2x^2-1}} + {1\over2}\ln{{A(x)y - B(x)}\over{C(x)}}
$$

Integral (B), on the other hand, can not be ``solved'' in
this manner, and example \ref{exp e^{-x^2}} proves this
claim of impossibility.

What does it mean to ``solve'' an integral?

Is there a formal procedure, an algorithm, that lets us solve
any integral, or prove that such a solution is impossible?

These questions have puzzled mathematicians for over 300 years, since
the invention of calculus, so much so that an introductory calculus
sequence can start to seem like a series of puzzle problems, each
chapter harder than the last.

This book aims to present our most sophisticated integration theory
that provides definitive answers to these questions, but the existance
of integrals like $\int e^{-x^2} \ud x$ without any elementary form shows that any such theory has
severe limitations.  Futhermore, the development of the electronic
computer, coupled with sophisticated numerical integration techniques,
has provided us with powerful approximation methods that significantly
reduce the importance of solving integrals.  Nevertheless, more
difficult differential equations continue to elude easy analysis, so
perhaps the greatest benefit of studying integration is the insight it
provides to solving differential equations in general.

\vfill\eject
\section{Calculus}
Let's consider again the integral $\int e^{-x^2} dx$.  We can {\it trivially} construct
an anti-derivative as follows:

$$E(x) = \int_0^x e^{-t^2} dt$$

I claim that $E(x)$ is an anti-derivative of $e^{-x^2}$.  Let's see...

First, is $E(x)$ well defined?  Let's recall some material from a standard
introductory calculus textbook, say, \cite{briggs}:

\begin{framed}
\cite{briggs} Definition - Definite Integral (p. 324)

A function $f$ defined on $[a,b]$ is {\bf integrable} on $[a,b]$ if $\lim_{\Delta\to0}\sum_{k=1}^{n}f(\bar{x}_k)\Delta x_k$
exists and is unique over all partitions of $[a,b]$ and all choices of $\bar{x}_k$ on a partition.
This limit is the {\bf definite integral of f from a to b}, which we write

$$\int_a^b f(x) dx = \lim_{\Delta\to0}\sum_{k=1}^{n}f(\bar{x}_k)\Delta x_k $$.
\end{framed}

\begin{framed}
\cite{briggs} Theorem 5.2 - Integrable Functions (p. 325)

If $f$ is continuous on $[a,b]$ or bounded on $[a,b]$ with a finite number of discontinuities,
then $f$ is integrable on $[a,b]$.
\end{framed}

So, $E(x) = \int_0^x e^{-t^2} dt$ is {\it integrable} on $[0,x]$ if $e^{-t^2}$ is continuous on $[0,x]$,
and $e^{-t^2}$ is continuous everywhere on the real line.  We can easily plot $e^{-t^2}$:

\begin{maximacode}
/* This code autogenerates filenames for pdf graphics so they */
/* can be easily embedded into LaTeX. */

pdf_graphics_fileno: 0$

next_pdf_filename() := (
   pdf_graphics_fileno : pdf_graphics_fileno + 1,
   concat("./GRAPH-", pdf_graphics_fileno, ".pdf")
)$

embed_latex_graphic(filename) :=
   ?princ(concat("\\optionalgraphics[width=0.7\\textwidth]{", filename, "}"))$

pdfplot2d([args]) := block(
   [filename: next_pdf_filename()],
   apply(plot2d, endcons([pdf_file, filename], args)),
   embed_latex_graphic(filename)
)$

pdfplot3d([args]) := block(
   [filename: next_pdf_filename()],
   apply(plot3d, endcons([pdf_file, filename], args)),
   embed_latex_graphic(filename)
)$
\end{maximacode}

\begin{figure}[H]
\begin{center}
\begin{maximacode}
pdfplot2d (exp(-x^2),
           [x, -6, 6], [ylabel, false], [grid2d, true], [legend, false],
           [color, red])$
\end{maximacode}
\end{center}
\caption{$e^{-t^2}$}
\end{figure}

It's obviously continuous, so Theorem 5.2 tells us that $E(x)$ is well
defined for any real number $x$ -- the limit used to construct the
Riemann sum exists and is unique.  We can also plot $E(x)$, using a
numerical integration routine to approximate the integral at each
point of the graph:

\begin{figure}[H]
\begin{center}
\begin{maximacode}
assume(x > 0)$
f(x) := exp(-x^2)$
F(x) := ''(integrate(f(x), x))$
pdfplot2d (F(x),
          [x, -6, 6], [ylabel, false], [grid2d, true], [legend, false],
          [color, red])$
\end{maximacode}
\end{center}
\caption{$\int_0^x e^{-t^2} dt$}
\end{figure}

We're plotting the {\it integral} now... the height of each point on the graph
was calculated by numerically approximating a Riemann sum.

Is this an anti-derivative of $e^{-t^2}$?  Plotting various tangent lines suggests
that it {\it might} be...

\begin{figure}[H]
\begin{center}
\begin{maximacode}
line(x,x0,l) := if abs(x-x0) < l then F(x0) + f(x0)*(x-x0) else %nan$
pdfplot2d ([F(x), 'line(x,-1.5,1), 'line(x,1,1)],
           [x, -6, 6], [y, -1, 1], [ylabel, false], [grid2d, true], [legend, false],
           [color, red, blue, blue], [xtics, 20], [ytics, 2])$
\end{maximacode}
\end{center}
\caption{$\int_0^x e^{-t^2} dt$ (with tangent lines at $x_0=-1.5$ and $x_0=1$)}
\end{figure}

The tangent lines in the graph were plotted using this formula:

$$ y(x) = E(x_0) + e^{-x_0^2}(x-x_0)$$

i.e, the point-slope equation of a straight line, with point $(x_0, E(x_0))$ and slope $e^{-x_0^2}$.

See, I used the $E(x) = \int_0^x e^{-t^2} dt$ formula for the $y$-coordinate of the point,
and the $e^{-x^2}$ formula for the slope.
If $E(x)$ is an anti-derivative of $e^{-x^2}$, then the derivative of $E(x)$ is $e^{-x^2}$, and
the formula will produce tangent lines for any value of $x_0$.  If $E(x)$ were {\it not} an anti-derivative of $e^{-x^2}$,
we'd get lines, but they wouldn't be tangent lines.

Varying the value of $x_0$ produces different lines (the two lines in the graph were generated using $x_0 = -1.5$ and $x_0=1$),
and they {\it appear} to be tangent
lines, so perhaps $E(x)$ is an anti-derivative of $e^{-x^2}$.

In fact, we can do far better than guess.  Remember the Fundamental Theorem of Calculus?


\begin{framed}
\cite{briggs} Theorem 5.3 (part 1) - Fundemental Theorem of Calculus (p. 338)

If $f$ is continuous on $[a,b]$, then the area function

$$A(x) = \int_a^x f(t)\, dt \qquad {\rm for\quad} a \le x \le b$$

is continuous on $[a,b]$ and differentiable on $(a,b)$.  The area function satisfies $A'(x) = f(x)$; or, equivalently,

$$A'(x) = \frac{d}{dx} \int_a^x f(t)\, dt = f(x),$$

which means that the area function of $f$ is an antiderivative of $f$.
\end{framed}

Pay particular attention to that last formula -- it says that the derivative of an integral with respect
to its upper bound of integration is just the integrand, with the name of the variable changed.

So, $E(x)$, defined like this:

$$E(x) = \int_0^x e^{-t^2} dt$$

is {\it trivially} an anti-derivative of $e^{-x^2}$, because the Fundamental Theorem of Calculus tells us that:

$$E'(x) = \frac{d}{dx} \int_0^x e^{-t^2} dt = e^{-x^2}$$

\cite{briggs} Theorem 5.2 tells us that $E(x)$ {\it exists} (because $e^{-x^2}$ is continuous), and \cite{briggs} Theorem 5.3 tells
us that $E(x)$ is an anti-derivative of $e^{-x^2}$.

Of course, we had something else in mind when we asked for an anti-derivative of $e^{-x^2}$.  We wanted
a simplified form, something like this:

$$\int x^2\,dx = \frac{1}{3} x^3 + C$$

not some mathematical smart aleck telling us that the answer is $\int x^2\,dx$!

The problem is that $\int e^{-x^2} dx$ doesn't have a simplified form.  It has an anti-derivative
(we plotted it, remember?), and it's completely well-defined as a mathematical function, but we
can't simplify it in the way that we can simplify $\int x^2\,dx$.

Another example is $\int \frac{\sin x}{x} dx$.  It's also continuous everywhere.  The only point
where that's at all in question is $x=0$, but L'Hospital's Rule\footnote{Using L'Hospital's Rule here is actually
a circular argument, because we had to evaluate this limit to prove that sine's derivative is cosine.} tells us that:

$$\lim_{x\to 0} \frac{\sin x}{x} = \lim_{x\to 0} \frac{\cos x}{1} = \frac{\cos 0}{1} = 1$$

which means that the division by zero in $\frac{\sin x}{x}$ is a {\it removable discontinuity}.
We can patch up our function like this:

\[ f(x) = \begin{cases} 
      \frac{\sin x}{x} & x \ne 0 \\
      1 & x = 0
   \end{cases}
\]

It's usually called the {\it sinc} function, and it's easy to plot:

\begin{figure}[H]
\begin{center}
\begin{maximacode}
sinc(x) := if x = 0 then 1 else sin(x)/x$
pdfplot2d (sinc(x),
           [x, -20, 20], [ylabel, false], [grid2d, true], [legend, false],
           [color, red])$
\end{maximacode}
\end{center}
\caption{${\rm sinc\,} t = \frac{\sin t}{t}$}
\end{figure}

Since sinc is continuous everywhere, this integral is well defined everywhere:

$${\rm Si}(x) = \int_0^x \frac{\sin(t)}{t} dt$$

...and we can plot it...

\begin{figure}[H]
\begin{center}
\begin{maximacode}
Si(x) := (quad_qag(sinc, t, 0, x, 1)[1])$
pdfplot2d (Si(x),
          [x, -20, 20], [ylabel, false], [grid2d, true], [legend, false],
          [color, red])$
\end{maximacode}
\end{center}
\caption{$\int_0^x \frac{\sin t}{t} dt$}
\end{figure}

...and we can check some of its tangent lines, using the formula:

$$ y(x) = {\rm Si}(x_0) + \frac{\sin x_0}{x_0}(x-x_0)$$

\begin{figure}[H]
\begin{center}
\begin{maximacode}
line(x,x0,l) := if abs(x-x0) < l then Si(x0) + sinc(x0)*(x-x0) else %nan$
pdfplot2d ([Si(x), 'line(x,-9,3), 'line(x,5,3)],
           [x, -20, 20], [y, -2, 2], [ylabel, false], [grid2d, true], [legend, false],
           [color, red, blue, blue], [xtics, 20], [ytics, 2])$
\end{maximacode}
\end{center}
\caption{$\int_0^x \frac{\sin t}{t} dt$ (with tangent lines at $x_0=-9$ and $x_0=5$)}
\end{figure}

Again, it's {\it trivial} that ${\rm Si}(x)$:

\begin{enumerate}
\item {\bf exists}, by \cite{briggs} Theorem 5.2 and the continuity of $\frac{\sin x}{x}$, and
\item is an {\bf anti-derivative} of $\frac{\sin x}{x}$, by \cite{briggs} Theorem 5.3 and the definition of ${\rm Si}(x)$:
$${\rm Si}(x) = \int_0^x \frac{\sin(t)}{t} dt \qquad\Longrightarrow\qquad {\rm Si}'(x) = \frac{d}{dx}\int_0^x \frac{\sin(t)}{t} dt = \frac{\sin(x)}{x}$$
\end{enumerate}

Yet, again, we have no simple closed form for ${\rm Si}(x)$.

Let's see... how could we find simple expressions for $\int e^{-x^2} dx$ and $\int \frac{\sin x}{x} dx$?

Could we try...

\begin{enumerate}
\item Integration by Parts
\item Trigonometric Substitution
\item Partial Fractions
\item ...some clever change of variables...
\item Google
\end{enumerate}

How about this instead -- let's prove that these two integrals have no simple forms.


\vfill\eject
\section{Algebra}

In high school, we study what the Arabs called ``al-jabr'', or what
the Encyclopaedia Britanncia calls ``a generalization and extension of
arithmetic''.  ``Elementary algebra," the encyclopedia goes on, ``is
concerned with properties of arbitrary numbers,'' and cites the
commutative law of addition $(a+b=b+a)$ as an example of such a
property.  We use only a few others: the commutative law of
multiplication; associative laws of both addition and multiplication;
the distributive law.  The key point is that all of these laws are
valid for any numbers whatsoever, so we are justified in applying them
to unknown numbers.

In addition to these basic laws, there is a language to be learned, as
well as the more general Principle of Equality: given two identical
quantities, the same operation applied to both must given identical
results.  This hold true no matter what the operation is, so long as
it is deterministic (i.e, has no randomness).  Thus, combining the
Principle of Equality with the commutative law of addition, I can
conclude that $\sin(a+b)=\sin(b+a)$, without any additional knowledge of
what ``$\sin$'' might be.

For example, consider the following sequence:

\begin{tabular}{r c l l @{\vbox to20pt{}}}
$(ax+{b\over2})^2$ &=& $(ax+{b\over2})(ax+{b\over2})$ & definition of square \cr
&=& $ax(ax+{b\over2}) + {b\over2}(ax+{b\over2})$ & distributive law \cr
&=& $axax+ax{b\over2} + {b\over2}(ax+{b\over2})$ & distributive law \cr
&=& $axax+ax{b\over2} + {b\over2}ax+{b\over2}{b\over2}$ & distributive law \cr
&=& $aaxx+{1\over2}abx + {1\over2}abx+{b\over2}{b\over2}$ & commutative law of multiplication (3 times)\cr
&=& $a^2x^2 + {1\over2}abx+ {1\over2}abx + {b^2\over4}$ & definition of square\cr
&=& $a^2x^2 + ({1\over2}+{1\over2})abx + {b^2\over4}$ & distributive law\cr
&=& $a^2x^2 + abx + {b^2\over4}$ & basic arithmetic\cr
$(ax+{b\over2})^2 - {b^2\over4} + ac$ &=& $a^2x^2 + abx + {b^2\over4}- {b^2\over4}+ ac$ & principle of equality\cr
$(ax+{b\over2})^2 - {b^2\over4} + ac$ &=& $a^2x^2 + abx + ac$ & definition of subtraction\cr
\end{tabular}
\vfill\eject

So, if $ax^2+bx+c=0$, then

\begin{tabular}{r c l l @{\vbox to20pt{}}}
$ax^2+bx+c$ &=& $0$ & \cr
$a(ax^2+bx+c)$ &=& $0a$ & principle of equality \cr
$a(ax^2+bx+c)$ &=& $0$ & zero theorem\footnote{$0a=0a+0a-0a=(0+0)a-0a=0a-0a=0$, showing that zero's unique behavior under multiplication is a direct result of the distributive law and zero's role as the identity element under addition}\cr
$a^2x^2+abx+ac$ &=& $0$ & distributive law\cr
$(ax+{b\over2})^2 - {b^2\over4} + ac$ &=& $0$ & principle of equality\footnote{using the last equality from the previous page}\cr
$(ax+{b\over2})^2 - {b^2\over4} + ac + {b^2\over4} - ac$ &=& ${b^2\over4} - ac$ & principle of equality\cr
$(ax+{b\over2})^2 $ &=& ${b^2\over4} - ac$ & definition of subtraction\cr
$4(ax+{b\over2})^2 $ &=& $4{b^2\over4} - 4ac$ & principle of equality\cr
$4(ax+{b\over2})^2 $ &=& $b^2 - 4ac$ & definition of division\cr
$2^2(ax+{b\over2})^2 $ &=& $b^2 - 4ac$ & definition of square\cr
$(2(ax+{b\over2}))^2 $ &=& $b^2 - 4ac$ & commutative law of multiplication\footnote{In the form $a^2b^2=aabb=abab=(ab)^2$}\cr
$(2ax+2{b\over2})^2 $ &=& $b^2 - 4ac$ & distributive law \cr
$(2ax+b)^2 $ &=& $b^2 - 4ac$ & definition of division \cr
$\sqrt{(2ax+b)^2} $ &=& $\sqrt{b^2 - 4ac}$ & principle of equality \cr
$(2ax+b) $ &=& $\sqrt{b^2 - 4ac}$ & !?!?!??! \cr
$(2ax+b)-b $ &=& $\sqrt{b^2 - 4ac} - b$ & principle of equality \cr
$2ax $ &=& $\sqrt{b^2 - 4ac} - b$ & definition of subtraction \cr
${1\over2a}2ax $ &=& ${1\over2a}(\sqrt{b^2 - 4ac} - b)$ & principle of equality \cr
$x $ &=& ${1\over2a}(\sqrt{b^2 - 4ac} - b)$ & definition of division \cr

\end{tabular}

At each step in the sequence (except one), we're just applying one of
the basic rules above.  The problem with the ``mystery step'' isn't so
much that we're taking the square root, since the principle of
equality tells us that we can perform the same operation on both sides
of the equal sign, but rather that it cancels out the square in some
undefined way.  So, assuming that we can perform the mystery step, and
noting that the division in the next to last step is only defined if
$a\ne0$, we can legitimately conclude that the final result is true
for any $a$, $b$, and $c$ whatsoever.

The mystery step leads us to introduce complex numbers,
typically when we want to use this equation to solve polynomials such
as $x^2+1=0$.  At this point, the alert student, having been lured in
to a false sense of security by the encyclopedia's ``numbers'', and
now finding himself facing a whole new type of number entirely, can
rightly ask, ``What is a number?''

To which we wave our hands and reply, ``It's, you know, a number!''
I am reminded of the time that I was asked to sub in a
seventh grade pre-algebra class, and was promptly asked by one of the
students to explain the difference between ``3'' and ``2.9999999\ldots''
I think I mumbled something lame like ``I don't know, what do you
think?'' I certainly hadn't come to class prepared to discuss Cauchy
sequences!

In college we are no longer satisfied with this answer, and here is
really the launching point for ``higher'' algebra.  Our ``numbers''
become objects in a set, and our simple concepts of addition and
multiplication morph into operations which map pairs of objects into
other objects.  When asked, ``What is a number?'', we now confidently
reply, ``Anything whose operations obey the axioms!'', which really
isn't all that surprising an answer (anymore) because our entire
theory had been built around those axioms to begin with.

The program of higher algebra (in fact much of modern mathematics)
goes thus.  We postulate the existance of one or more sets of objects
and one or more operations, which are simply mappings defined on the
objects of those sets.  We write out a list of axioms that we assume
those sets and operations obey.  Which axioms are those?  Whichever we
find useful (or at least interesting).  Then we develop as little or
much of a theory as we can, reasoning always from the base axioms.
Finally, we take some specific set of objects (like the integers),
demonstrate that they obey our set of axioms, and conclude that the
entire theory developed for those axioms must apply, therefore, to the
integers.  Sometimes we reverse the process by finding axioms obeyed
by some specific set of objects we wish to study, then developing a
theory around them.\footnote{How do we demonstrate that a certain set
obeys certain axioms?  By using more axioms, of course!  Mathematics
is probably the most self-contained of all major academic fields of study.
Many other fields use its results, but math itself references nothing.
It's impossible to get started without assuming {\it something}, so
the entire process becomes a bit of a chicken-and-egg operation, which
leads you to wonder$...$ which {\it did} come first?}

The most important (i.e, repeatedly used) sets of axioms are given
names, or more precisely the sets and operators which obey them are
given names.  Thus, a ``group'' is any set and operator which obey three
or four certain axioms.  A ``ring'' is any set and pair of operators
which obey about six axioms.  Add another axiom or two and it
becomes a ``field''.  If a different axiom is obeyed, it is a
``Noetherian ring''.

It's easy to get bogged down with terminology, especially in a
classroom environment where you can't raise your hand during a test
and ask, ``Excuse me, what's a semigroup again?''  Far more important,
I think, is to grasp the central idea that any of these terms refers
simultaneously to three things: a set of axioms, a theory logically
developed from those axioms, and any particular object(s) that obeys
those axioms, and therefore the theory.  The ultimate goal is to
develop far more sophisticated theories than are possible using the
``numbers'' of elementary algebra.

Our goal in this book is the development of an algebraic system that
allows us to represent as a single object any expression written using
elementary functions, putting $\sqrt{1 + \sin x}$ on par with
$3\over2$, introducing the concept of a derivative so that we can
write differential equations using these objects (it now becomes {\it
differential} algebra), and equipping this system with a theory
powerful enough to either integrate anything so expressed, or prove
that it can't be done, at least not using elementary functions.  This
is how computer programs like {\it Mathematica} or {\it AXIOM} solve
``impossible'' integrals.  Along the way, we will have cause to
at least survey some of the deepest waters of modern
mathematics.  Differential algebra is very much a 20$^{\rm th}$
century theory --- the integration problem was not solved until
roughly 1970; a really workable algorithm for the toughest cases
wasn't available until 1990; a key sub-problem (testing the
equivalence of constants) remains unsolved still.  Yet one thing is
for sure.  Three hundred years after the development of calculus, one
of its most basic and elusive problems has finally yielded not to
limits, sums, and series, but to rings, fields and polynomials.  Quite a
triumph for ``al-jabr''.

\vfill\eject
\section{Maxima}

Many of the more complex examples in the book will be solved
using the open source computer algebra system Maxima.

In the section, I'll collect several useful functions
that I'll use throughout the book.

First, TeX has two different ways of displaying the Greek letter
$\theta$, and Maxima lets us select which we will use.
I prefer $\theta$.

\begin{maximablock}
theta;
:lisp-quiet (defprop $theta "\\theta" texword)
theta;
\end{maximablock}

I often want to differentiate with respect to $x$, so I use a shorthand for this:

\begin{maximablock}
D(f) := diff(f,x)$
\end{maximablock}

Maxima has some basic support for namespaces, which allows us to hide
the definitions of auxiliary functions and only make the primary
functions visible.

\begin{maximablock}
load("share/contrib/namespaces/namespaces.lisp")$
\end{maximablock}

I use lambda expressions with a single variable so much that I find it
useful to create a version of {\tt map} that treats its first argument
as a lambda expression in $u$.

\begin{maximablock}
mapu(func, expr) ::=
   map(buildq([func], lambda([u], func)),
       ev(expr))$
mapu(u+4, [1,2,3]);
mapu(denom(u), [x,1,1/x,1/x^2]);
\end{maximablock}

The $cis$ function is a useful shorthand for constructing
roots of unity:

\begin{maximablock}
cis(theta) := %i*sin(theta) + cos(theta)$
\end{maximablock}

Example \ref{an integral Maxima can't solve} uses
trigonometric functions of $\frac{\pi}{8}$, which
Maxima does not expand by default.

\begin{maximablock}
matchdeclare(ii, integerp, xx, true)$

tellsimpafter(sin(ii*%pi/8),
   subst(x=ii*%pi/4, ev(sin(x/2), halfangles=true)))$
tellsimpafter(cos(ii*%pi/8),
   subst(x=ii*%pi/4, ev(cos(x/2), halfangles=true)))$
\end{maximablock}

\subsection{List Functions}

Here's a function that finds the index of an
element in a list:

\begin{maximablock}
which(n,u) :=
   sublist_indices(n, lambda([x], x=u))[1]$

which([a,b,c,d,e], b);
\end{maximablock}

This function converts the parts of a mathematical expression into a
list:

\begin{maximablock}
partlist(expr) := block(
   [partswitch: true, result:[]],
   for i:1 step 1 unless part(expr,i) = end
      do result:cons(part(expr,i), result),
   result)$
partlist(a+b+c);
\end{maximablock}


\subsection{Array Functions}

This next function is convenient for displaying Maxima arrays.
Conceptionally, it's just

\begin{maximaverbatim}
displayarray(b) :=
   map(display,
      map(lambda([u], arraymake(b,u)),
          rest(arrayinfo(b),2)))$
\end{maximaverbatim}

This doesn't work because {\tt arrayinfo} quotes its first argument
and interprets it as the name of an array, so the block of code above
would always return the array info for array {\tt b}.  Something a
little more complicated is required: a macro that evaluates its
argument and then calls {\tt arrayinfo}.

\begin{maximablock}
displayarray(b) ::=
   map(display,
      map(lambda([u], arraymake(''b,u)),
          rest(apply(arrayinfo, ''[b]),2)))$
\end{maximablock}

\subsection{Simplification}

The Maxima function {\tt ratsimp} simplifies a rational function to
its simplest form, and I use it so much, let's define a shorthand
notation to simplify an expression and assign it to a variable:

\begin{maximablock}
infix(":::", 180, 20)$
(x ::: y) := x :: ratsimp(y)$
(x+1)/(x^2-1) - 1/(x-1);
a ::: (x+1)/(x^2-1) - 1/(x-1);
\end{maximablock}

\begin{maximablock}
infix("===", 20, 20)$
(x === y) := is(ratsimp(x - y) = 0)$
\end{maximablock}
