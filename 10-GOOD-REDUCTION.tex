
\chapter{Good Reduction}

At this point, there is only one major missing piece in our
integration theory for simple radicals --- how do we limit the
multiples of a divisor to a testable set?  We've seen how to
repeatedly raise a divisor to higher and higher powers, but how do we
know when to stop?  At what point can we declare that a divisor has no
multiple that is principle?

We'll attack this problem the same way we attacked polynomial
factorization in Chapter ?, by mapping into a finite field, solving a
corresponding problem there, then somehow lifting the result back to
the original field.  The details differ, but the basic idea is the
same.  Of course, divisors, like polynomials, behave somewhat
different in finite fields, so our first task is to study
some of their unique properties in this domain.

\section{Simple Algebraic Extensions over Finite Fields}

Let's start with a simple but crucial observation:

\theorem

In an algebraic extension over a finite field, the evaluation field is
also finite.

\proof

Consider a finite field of constants ${\cal F}$, over which we'll
extend first into a rational function field ${\cal F}(x)$ and then add
an algebraic extension ${\cal F}(x,y)$, where $y$ satisfies some
minimial polynomial $f(x,y)=0$.  Start with the constant field, which
gives us a finite number of values for $x$.  Plugging each of these
values into the minimal polynomial gives a finite set of polynomials
$f(y_i)=0$.  By Theorem ?, we can extend ${\cal F}$ into a finite
extension field ${\cal F}[\gamma]$ where all the roots of the
polynomial exist.  Since there a only a finite number of polynomials,
we need at worst a finite set of extensions ${\cal
F}[\gamma_1,...,\gamma_k]$ to construct a field in which all the roots
of all the polynomials exist.  Using the Theorem of the Primitive
Element, we can collapse all of these into a single finite extension
field ${\cal F}[\phi]$.  Since all values of $x$ exist in ${\cal F}$,
and all values of $y$ exist in ${\cal F}[\phi]$, an evaluation
homomorphism carries any rational function in $x$ and $y$ into
${\cal F}[\phi]$.

\endtheorem

This theorem leads directly to the single more important difference
(to us) between divisors in an infinite field versus those in a finite
field.  {\it In a finite field, some multiple of every divisor is
principle.}  The reason is that the multiplicative group of the
evaluation field has finite order.  The simplest way to demonstrate
this is to construct theorems analogous to Theorems ? and ?:

\theorem

In an algebraic extension of a finite field with characteristic
greater than 2, a function can always be constructed with an $m^{\rm
th}$-order zero at a specified place $(\alpha, \beta)$ and zero order
at all other finite places, where $m$ is the multiplicative order of
the evaluation field.

\proof

The desired function is

$$(x-\alpha)^m + (y-\beta)^m$$.

Clearly, this function is zero at $(\alpha, \beta)$ and of $m^{\rm
th}$ order there (PROOF THIS).  At all other places one of the two
terms will be non-zero, and both exist in the evaluation field.  By
Theorem ?, any non-zero number raised to the multiplicative order of
its field is one.  Thus the value of this function will be either
$1+0$, $0+1$, or $1+1=2$, all finite and non-zero, and thus of zero
order.

\endtheorem

\theorem

In an algebraic extension of a finite field with characteristic
greater than 2, a function can always be constructed with an $m^{\rm
th}$-order pole at a specified place $(\alpha, \beta)$ and zero order
at all other finite places, where $m$ is the multiplicative order of
the evaluation field.

\proof

The desired function is

$${f(\alpha,y)^m\over(x-\alpha)^m(y-\beta)^m} + 1$$

where the division by $(y-\beta)^m$ is exact.
Clearly, this function has a pole at $(\alpha, \beta)$ and of $m^{\rm
th}$ order there (PROOF THIS).  CONSIDER OTHER PLACES OVER $\alpha$.
At all other places the denominator
term will be non-zero, and thus one, and the numerator will be
either zero or one (by Theorem ?)
Thus the value of this function at these places will be either
$0+1$ or $1+1=2$, both finite and non-zero, and thus of zero
order.

\endtheorem


\example

Show that some multiple of ${\mathrm Z}(1,1)$ is principle in
${\bf Z}_5(x,y); y^2=x$.

Let's first construct a multiplication table for ${\bf Z}_5$:

\begin{center}
\begin{tabular}{c|c c c c c}
  & 0 & 1 & 2 & 3 & 4 \cr
\hline
0 & 0 & 0 & 0 & 0 & 0 \cr
1 & 0 & 1 & 2 & 3 & 4 \cr
2 & 0 & 2 & 4 & 1 & 3 \cr
3 & 0 & 3 & 1 & 4 & 2 \cr
4 & 0 & 4 & 3 & 2 & 1 \cr
\end{tabular}
\end{center}

Now, let's list out the places on the Riemann surface for
${\bf Z}_5(x,y); y^2=x$.

\begin{center}
\begin{tabular}{c l}
$x$ & $(x,y)$ \cr
\hline
0 & (0,0) \cr
1 & (1,1) \quad (1,4) \cr
2 & $(2,\gamma) \quad (2,-\gamma); \quad \gamma^2 - 2 =0$ \cr
3 & $(3,\theta) \quad (3,-\theta); \quad \theta^2 - 3 =0$ \cr
4 & (4,2) \quad (4,3) \cr
\end{tabular}
\end{center}

It looks like we need ${\bf Z}_5[\gamma,\theta]$ to express these places.
It's simplest to collapse $\gamma$ and $\theta$ into a single algebraic
extension.  We could use the Theorem of the Primitive Element to
do this, but in this case just looking at the multiplication table
and noting that $3 = 2^3 = \gamma^6$ shows that $\theta = \pm \gamma^3$.
So, in fact, we only need ${\bf Z}_5[\gamma]$:

\begin{center}
\begin{tabular}{c l}
$x$ & $(x,y)$ \cr
\hline
0 & (0,0) \cr
1 & (1,1) \quad (1,4) \cr
2 & $(2,\gamma) \quad (2,-\gamma); \quad \gamma^2 - 2 =0$ \cr
3 & $(3,\gamma^3) \quad (3,-\gamma^3)$ \cr
4 & (4,2) \quad (4,3) \cr
\end{tabular}
\end{center}

Since ${\bf Z}_5[\gamma]$ has $5^2=25$ elements, its multiplicative
group has order one less than this.  We conclude that 24 is our
``magic'' multiple, and that ${\mathrm Z}^{24}(1,1)$ must be
principle in this field.  Its generator should be simply
$(x-1)^{24} + (y-1)^{24}$.  Clearly this function is zero for
$(x,y)=(1,1)$.  Let's verify that it's non-zero for some other
places on the Riemann surface:

\begin{eqnarray*}
(0,0) &:& (-1)^{24} + (-1)^{24} = 4^{24} + 4^{24} = 1+1 = 2 \cr
(1,4) &:& 3^{24} + 0^{24} = 1 + 0 = 1 \cr
(2,\gamma) &:& (\gamma-1)^{24} + (2-1)^{24} = 1+1 = 2 {\rm ,\quad since:} \cr
&&\cr
&& (\gamma-1)^2 = (\gamma^2-2\gamma+1) = 3-2\gamma \cr
&& (\gamma-1)^4 = (3-2\gamma)^2 = (9-12\gamma+4\gamma^2) = 2-2\gamma \cr
&& (\gamma-1)^8 = (2-2\gamma)^2 = (4-8\gamma+4\gamma^2) = 2-3\gamma \cr
&& (\gamma-1)^{12} = (2-2\gamma)(2-3\gamma) = (4-10\gamma+6\gamma^2) = 1 \cr
\end{eqnarray*}

In the final series of calculations, I used $\gamma^2=2$ and reduced
mod 5 repeatedly.  I think the pattern should be clear, and leave
further verification as an exercise.

\endexample

\section{Jacobian Varieties}

An algebraic extension is a simple example of what algebraic geometers
term a {\it variety}, which is the zero locus of a set of polynomials
defined over some field.  Thus, for example, the unit circle is a
variety (defined over the real numbers), because it is the zero locus
of $x^2+y^2=1$.  But the points $(1,0)$ and $(-1,0)$ are also a
variety, because they are the zero locus of the {\it set} of
polynomials $\{x^2=1; y=0\}$.

An {\it abelian variety} is a variety accompanied by a commutative
group structure on its elements, which typically includes picking an
arbitrary zero point as the identity element.  The circle is an
abelian variety, if we identify its points with their angles from the
x-axis and make $(1,0)$ our identity element.  Now any two points can
be ``added'' or ``subtracted'' (by adding or subtracting their
respective angles) to obtain a third point, and each point has an
inverse associated with it (its mirror image across the x-axis).  It
should be obvious that the choice of a zero point was totally
arbitrary.  Likewise, the points ${(1,0), (-1,0)}$ also form an
abelian variety; their group structure is isomorphic to ${\bf Z}_2$ and
the choice of one of them as the identity is, again, arbitrary.

Is every variety abelian?  No, but any complete, non-singular variety
can be homomorphicly mapped into an associated abelian variety
(typically of higher dimension), called its {\it Jacobian variety}.
This fact, combined with the extensive body of literature on abelian
varieties ([Mumford], [Birkenhake and Lange], [Lang], to mention a
few), makes the Jacobian variety an important object of study (though
David Mumford, in the preface to [Mumford], described it as a
``crutch'').

We will be needing only a tiny bit of this theory here, so my goal in
this section is only to demonstrate how the Riemann-Roch Theorem
allows us to set up an abelian group structures on an algebraic
extension.

First, let's review the Riemann-Roch theorem:

\theorem {\rm (Riemann-Roch)}

For any divisor $\mathfrak{b}$,

$$l(\mathfrak{b}) = \deg \mathfrak{b} + 1 - g + l(\mathfrak{c}-\mathfrak{b}) $$

where $l(\mathfrak{b})$ is the dimension of the vector space
$L(\mathfrak{b})$ of multiples of $-\mathfrak{b}$, $g$ is the genus of
the extension, and $\mathfrak{c}$ is any divisor of the canonical class of
differentials.

\endtheorem

It immediately follows (from $\mathfrak{b}={\bf 0}$) that
$l(\mathfrak{c})=g$, which can be taken as the definition of the
genus.

We can now pick $g$ independent differentials from $\mathfrak{c}$ and
use them (along with an arbitrary origin) to map into the torus
${\bf C}/\Lambda^g$.

Now, Abel's Theorem and the Jacobi inversion theorem ([Griffiths and
Harris], p. 235) shows that ${\rm Pic}^0$, the group of divisors of
degree zero modulo linear equivalence is isomorphic to ${\bf
C}/\Lambda^g$.

Alternately, ([Lang], II, \S1, Theorem 3), we can factor a mapping
of a product into an abelian variety into mappings on each factor.

Lang also characterizes Abel's theorem as follows:

\begin{quote}

Let $\omega_1, ..., \omega_g$ be a basis for the differential forms
on the first kind of V.  If $\mathfrak{a} = \sum n_i P_i$ is a
[divisor] of degree 0 on V, and P is a fixed point of V, then
the map into ${\bf C}/\Lambda^g$ given by:

$$\mathfrak{a} \to \sum n_i (\int_P^{P_i}\omega_1, ..., \int_P^{P_i}\omega_g)$$

is well defined modulo the periods... the kernel consists of those
divisors that are linearly equivalent to 0 (i.e, principle); this is
Abel's theorem.

\end{quote}


\section{Endomorphism Rings}

Any commutative group $G$ induces a (non-commutative) ring structure
on its endmorphisms, defined as follows (remember that an
endomorphism is a homomorphism from an object to itself):

Two endmorphisms $\phi(g): G \to G$ and $\gamma(g): G \to G$ are added
using $G$'s group operation on the images: $(\phi+\gamma)(g) =
\phi(g)\cdot\gamma(g)$, where $\cdot$ denotes the group operation.
The additive identify is the endmorphism that maps the entire group
onto its identity element.

Two endmorphisms $\phi(g): G \to G$ and $\gamma(g): G \to G$ are
multiplied using composition of mappings: $(\phi\gamma)(g) =
\phi(\gamma(g))$.  The multiplicative identity is the endmorphism that
maps every element in the group onto itself.

Let us now verify that these operations define a ring, the {\it endomorphism
ring} of G, which we shall denote ${\rm End}(G)$.  The properties
of the identity elements are fairly obvious, I think.  Almost as
obvious is that the associative and commutative properties of the
underlying group translate directly into additive associative and
commutative properties in the endmorphism ring.  The multiplicative
properties follow from composition of mappings being associative, but
not necessarily commutative.  The distributive law follows from the
easily verified identity $\phi(\gamma(g)\cdot\mu(g)) = \phi(\gamma(g)) \cdot
\phi(\mu(g))$, using the fact that $\phi$ is an endomorphism, and thus
a homomorphism, and therefore maps the group operator through.

The ring of integers ${\bf Z}$ can be mapped homomorphicaly\footnote{An easy
consequence of ${\bf Z}$'s repelling universal property in the category of
rings, see [Lang], p. ?} into any ring, and an endomorphism ring is no
exception.  We'll denote by $[m]$ the endmorphism mapped to by the
integer $m$. $[0]$ is clearly the additive identity mapping all
elements to the group identity.  $[1]$ is, of course, the
multiplicative identity mapping all elements to themselves.  $[2]$ is
$[1]+[1]$, the endmorphism that composes each element with itself
(using the group operator): $[2]: g \to g\cdot g$.  $[3]$ composes
each element with itself thrice: $[3]: g \to g\cdot g\cdot g$, etc.

Because ${\bf Z}$ is commutative, the subring $[m]$ it maps to is also
commutative, even though ${\rm End}(G)$ may not be.



\section{Good Reduction}
