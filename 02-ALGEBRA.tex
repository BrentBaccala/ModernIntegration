
\mychapter{Commutative Algebra}

\begin{comment}
The author of a mathematics text, really any text in a technical
subject, is faced with a difficult choice --- how much to leave out?
How often to refer readers to references?  How much knowledge
to presuppose?

Now, in the early twenty-first century, we are faced with
technological changes that affect the author's approach.  First, we
have the ability to distribute very large books, file size being a
slight or absent impediment.  Also, we have a political and economic
structure that relies heavily on using copyright to restrict and
control information flow.  These factor have influenced me to decide
in favor of writing a larger and more comprehensive text, that aims to
introduce abstract algebra as much it tries to teach integration
theory.
\end{comment}

In this chapter I will outline the basic algebraic structures
necessary to carry out the program sketched out in the previous
chapter.  This material is included mainly to provide a starting point
for the rest of the book. The pace of this chapter is
deliberately quick; I omit a lot of the more basic proofs and doubt
that this will substitute for a good introductory text on higher
algebra.\footnote{Need a good reference to such a text here.
I want to footnote all the missing proofs with references
back to two or three texts; maybe Lang, van der Waerden,
and an introductory undergraduate text of some kind.}

\mysection{Rings and Fields}
\qquad [van der Waerden], \S3.1

We begin with two key definitions that we will use throughout: the
{\it ring} and the {\it field}.  As I explained in the previous
chapter, both are associated primarily with sets of axioms.  Any
algebraic system that obeys the ring axioms is called a ring; any
algebraic system that obeys the field axioms is a field.

Both rings and fields are defined over sets with two binary operators,
conventionally called addition and multiplication.  It will appease
the nervous reader to know that for our purposes,
addition is addition and multiplication is multiplication --- the same
addition and multiplication we learned in grade school.  Of
course, in the general case, any pair of operations that obey the
axioms will suffice to form a ring or a field, but we won't need to
concern ourselves with this.

I'm going to use basic set-theoretic notation to define the axioms.
Read $\forall$ as ``for all'' and $\exists$ as ``there exists''.  Each
symbol is immediately followed by the new variable or a list of new
variables that it qualifies.  Their ordering is significant.  The rule
is that each variable is assigned in left-to-right order.  So,
``$\exists a, \forall b$'' means ``there exists an $a$ (independent of
$b$, because $b$ hasn't appeared yet) so that for all $b$\ldots'',
while ``$\forall b, \exists a$'' means ``for all $b$, there exists an
$a$ (possibly different for each $b$) so that\ldots''.  In the later
case, $a$ can be a function of $b$, but not in the first case.
Sometimes I'll add the set inclusion symbol $\in$, read ``in'', such
as ``$\forall a,b,c \in {\cal R}$...'', reading ``for all a, b, and c
that are members of ${\cal R}$...'', but I'll omit this from the next
table for simplicity, since everything is a member of ${\cal R}$.

A {\it commutative ring with unity} ${\cal R}$ obeys the following axioms:

\begin{center}
\begin{supertabular}{l l @{} l r}
   associative law of addition	& $\forall a,b,c,$ & $(a+b)+c = a+(b+c)$ &(R1)\cr
   associative law of multiplication & $\forall a,b,c,$ & $(ab)c = a(bc)$ &(R2)\cr
   commutative law of addition	& $\forall a,b,$ & $a+b = b+a$ &(R3)\cr
   distributive law   & $\forall a,b,c,$ & $a(b+c) = ab + bc$ &(R4)\cr
   existence of an additive identity (zero) & $\exists 0, \forall a$, & $0 + a = a$ &(R5)\cr
   invertibility of addition & $\forall a, \exists b$, & $a + b = 0$ &(R6)\cr
   & & &\cr
   commutative law of multiplication & $\forall a,b,$ & $ab = ba$ &(CR1)\cr
   existence of a multiplicative identity (unity) & $\exists 1, \forall a,$ & $1a = a$ &(RwU1)\cr
\end{supertabular}
\end{center}

You notice the commutative law of multiplication at the end as (CR1),
along with the existance of a unity element 1, labeled (RwU1).  A
substantial theory has been developed around {\it non-commutative}
rings, probably because matrix multiplication (a critically important
example) is non-commutative.  Most of our rings are commutative ring,
or {\it abelian} (the terms are synonymous).  Also, I required the
existence of a multiplicative identity.  Much of ring theory can be
developed without this axion, but some theorems require it, and I
don't want to belabor the point, since all of our rings will have a
unity element.  Therefore, I will omit any additional terminology,
adopt the CR1 and RwU1 axioms along with ring axioms R1-R6 to obtain a
{\it commutative ring with unity}, and call it informally a ``ring''
for the rest of the book.

A {\it field} ${\cal F}$ obeys all the ring axioms (thus all fields are also
rings), as well as the following axiom:

\begin{center}
\begin{tabular}{l l r}
   invertibility of multiplication & $\forall a \ne 0, \exists b, ab = 1$ &(F1)\cr
\end{tabular}
\end{center}

In plain English, rings are mathematical systems in which addition and
multiplication are cleanly defined.  Subtraction is also defined due
to (R6), the invertibility of addition, which allows a subtraction
problem to be turned into an addition problem.  Division, however, is
not, since it requires (F1).  Since the ring axioms do not require
multiplication to be invertible, there is no guarantee that we can
carry out division in a ring.  A field, on the other hand, is a
mathematical system in which all four elementary operations ---
addition, subtraction, multiplication, and division --- are defined.

\begin{comment}

\theorem (The Zero Theorem)

Given a ring ${\cal R}$, $\forall x\in{\cal R}, 0x=0$

\proof

From the equation $0x=0x+0x+(-0x)=(0+0)x+(-0x)=0x+(-0x)=0$.

\begin{tabular}{r c l l @{\vbox to20pt{}}}

$0x+(-0x)$ &=& $0$ & R6 applied to $0x$ \cr
$0+0x$ &=& $0x$ & R5 applied to $0x$ \cr
$0+0x + (-0x)$ &=& $0$ & principle of equality \cr

   & & &\cr

$(0+0x)+(-0x)$ &=& $0$ & principle of equality\cr
$(0\cdot 1+0x)+(-0x)$ &=& $0$ & RwU1\cr
$0(\cdot 1+x)+(-0x)$ &=& $0$ & R4\cr

\end{tabular}

$=(0+0)x+(-0x)=0x+(-0x)=0$.

applied
into , obtained from R5 applied to $0x$, then used R5 to
factor out $(0+0)$ and R6 to replace this with $0$, then R6 again to
conclude that $0x+(-0x)=0$.

Thus, multiplication by zero is a property of all rings.

\endtheorem

\end{comment}

The simplest example of a ring is the set of integers, which I shall
denote as {\bf Z} (after the German word for number, {\it zahl}).  A pair
of integers can be added, subtracted or multiplied to form another
integer.  Note, however, that a pair of integers can not necessarily
be divided to form another integer.  $3 \over 2$ is not an integer,
because multiplication (in {\bf Z}) is not necessarily invertible ---
there is no integer that when multiplied by 2 forms 1.
(F1) is not satisfied.
Thus {\bf Z} forms a ring but not a field.

\mysection{Quotient fields}
\subsection*{\qquad The field ${\bf Q}$}
\qquad [van der Waerden], \S3.3

We can remedy our inability to divide using only integers
by moving on the {\it rational numbers},
traditionally denoted {\bf Q} (probably for {\it quotient}).  This is the
simplest example of a {\it quotient field}, in this case formed over
the integers, {\bf Z}.  It is also our first example of a theme
we'll use repeatedly in this book, that of using a simple
algebraic system to construct a more complex one.

To form a quotient field from a ring, we take pairs of elements from
the ring (conventionally arranged into fractions) and establish an
{\it equivalence relationship} between them.  We also require that the
second element in the pair (the denominator) can not be zero.  Two
pairs $(a,b)$ and $(c,d)$ are equivalent if $ad=bc$, and we write them
$a\over b$ and $c\over d$.  We group equivalent pairs together into
{\it equivalence classes} and define our basic field operations as
follows:

\begin{center}
\begin{eqnarray*}
{a\over b} + {c\over d} &=& {{ad+bc}\over{bd}} \cr
{a\over b} - {c\over d} &=& {{ad-bc}\over{bd}} \cr
{a\over b} {c\over d} &=& {{ac}\over{bd}} \vbox to20pt{}\cr
{\,{a\over b}\, \over {c \over d}} &=& {{ad}\over{bc}} \vbox to24pt{}\cr
\end{eqnarray*}
\end{center}

The additive identity element is $0\over1$ and the multiplicative
identity is $1\over1$, using the original identities 0 and 1 from the
base ring.\footnote{Actually, a ring doesn't have to have a
multiplicative identity (check the axioms).  But all of
our rings will have 1, and it's not that difficult to extend
an arbitrary ring into a larger one that does.}
Note that the division by zero is not defined, nor do
our field axioms require it to be.

Notice that although we define all four field operations, we only
use the three ring operations to do it!  I.e, when we divide
$a\over b$ by $c\over d$, we need only to form $ad$ and $bc$
in order to form the $(ad,bc)$ pair, which we write as ${ad}\over{bc}$.
We thus divide $1\over2$ by $2\over3$ to obtain $3\over4$
without ever having to divide the {\it integers} ---
only multiplying them ($1\cdot3=3$ and $2\cdot2=4$).

In general, there is no guarantee that this kind of construction will
work.  We can't just pair numbers up however we want and call it a
field.  Several other conditions have to be met.  First of all, we
have to ensure that the equivalence relationship is well-defined.  If
$x=y$ (in the sense of equivalence) and $y=z$, then we must have
$x=z$, otherwise we can't even cleanly establish the critical notion
of an {\it equivalence class} (which says that $1\over2$ and $2\over4$
are basically the same thing).  I emphasize here that the new field,
and its new operations, are defined using the equivalence classes,
although we muddle this distinction by using the smallest fraction in
a class to represent it.  Strictly speaking, the multiplicative
identity is not ${1\over1}$ but
$\{{1\over1},{2\over2},{3\over3},\ldots\}$, the additive identify is
not ${0\over1}$ but $\{{0\over1},{0\over2},{0\over3},\ldots\}$, and my
example in the last paragraph should have read ``we thus divide
$\{{1\over2},{2\over4},{3\over6},\ldots\}$ by
$\{{2\over3},{4\over6},{6\over9},\ldots\}$ to obtain
$\{{3\over4},{6\over8},{9\over12},\ldots\}$...''

Having cleanly established equivalence
classes, we have to make sure that our operations actually work
consistently on them, since they are defined in terms of fractions
within the classes.  We need to verify that taking any fraction from
one equivalence class and any fraction from other, then applying one
of four operations to them, we always get an answer in a third
equivalence class.  The actual answer can (and will) vary depending on
the choice of representative fractions, but it has to always be in the
same class.  In this way, we confirm that the operations are cleanly
defined not just on the fractions, but on the classes.  I'm not going
to actually make this verification, but leave it as an exercise.

Which is why I excluded zero as a possible denominator.  We do this
because otherwise our operations aren't cleanly defined on these
equivalence classes. ${1\over0}$ is not equivalent to ${0\over1}$
(since $1\cdot1\ne0\cdot0$), so ${1\over0}$ must have a multiplicative
inverse (by axiom F1); i.e, some fraction ${a\over b}$ must exist
which when multiplied by ${1\over0}$ produces ${1\over1}$, yet by the
zero theorem, no such element $b$ can exist in the base ring so that
$1b=0$.  Excluding zero as a possible denominator ensures that our
field axioms are satisfied.

Yet in the quotient
field operations, where we multiply two denominators together to get
the result's denominator, what would happen if two non-zero elements can be
multiplied to form zero, producing a zero denominator?  Nothing
in the ring axioms prevents this from happening, so we add an
additional axiom.

An {\it integral domain} ${\cal I}$ obeys all the ring axioms,
as well as:

\begin{center}
\begin{tabular}{l l l r}
   non-existence of zero divisors & $\forall a\ne 0,b\ne 0 \in {\cal I},$ & $ab\ne 0$ &(I1)\cr
\end{tabular}
\end{center}

The quotient field construction is only defined on integral domains,
and I'll leave it as an exercise to show that ${\bf Z}$ is an integral
domain.  The main point of this section is to recognize that the
quotient field construction can be performed not only on the integers
${\bf Z}$ to obtain the rationals ${\bf Q}$, but on any integral
domain to obtain its quotient field.


\mysection{Polynomial rings and rational function fields}
\subsection*{\qquad The ring ${\cal F}[x]$ and the field ${\cal F}(x)$}
\qquad [van der Waerden], \S3.4

Having built a field from a ring, can we build a ring from a field?
The answer is yes, and the most important such construction is a {\it
polynomial ring}, whose elements are polynomials in some variable with
coefficients in the underlying field, all but a finite number of which
must be zero.\footnote{If we relax the finiteness requirement and
allow infinite polynomials, we obtain a ring of {\it power series}
over the field, typically written ${\cal F}[[x]]$.  We will have
little use for power series in this book.}  We write this ring using
the underlying field, brackets and the variable, so ${\cal F}[x]$ is
the ring of polynomials in $x$ with coefficients from the field ${\cal
F}$.

${\cal F}[x]$ is a ring but not a field.  It is, however, an
integral domain (left as an exercise), so we can form a quotient field
from it, which we write using parenthesis instead of brackets: ${\cal
F}(x)$.  Elements in ${\cal F}(x)$ are fractions, both the numerator
and denominator of which are polynomials in $x$.  So, for example,
${x}\over{x-1}$ is a element of ${\bf Q}(x)$.  Fractions of
polynomials are called {\it rational functions}, so ${\bf Q}(x)$ is
the {\it field of rational functions in $x$ over the rational numbers}.

Now, you might ask, ``Can't $(x-1)$ be zero?  Say, if $x$ is 1?''.
The answer is {\it no}.  $x$ is not 1 or any other number.  $x$ is
$x$, and in ${\bf Q}[x]$, $(x-1)$ is as different from zero as $3
\over 2$ is.  $(x-1)$, and things like it, are {\it
completely distinct elements} in the algebraic systems in which they
are defined.

Now, obviously, we can set $x$ to be 1.  But now we are no longer
working in ${\bf Q}[x]$ --- for starters, there is no longer a
distinct element $x$!  Now we are working in ${\bf Q}$.  Setting $x$
equal to 1 mapped everything from ${\bf Q}[x]$ into ${\bf Q}$.  This
is a simple example of an {\it evaluation homomorphism} --- a
homomorphism (a mapping which preserves operations) from one system to
another created by setting an independent variable equal to some
constant value.

So, you ask, ``what about $1\over2$ and $2\over4$?  Are they distinct
elements as well?''  {\it No}.  This time we are dealing with elements
that are basically the same. This is where the technical details of
the quotient field construction become significant.  Strictly
speaking, we are not working with elements like $1\over2$ at all.  We
are working with the {\it equivalence classes} defined above.
$1\over2$ is a {\it representative} of an equivalance class that
includes $2\over4$.

If all this seems a bit arbitrary, well, it is.  I could easily pick
two numbers from {\bf Z} and pair them into equivalence classes in
some other way than for {\bf Q}; if my basic axioms were satisified I
would even have a field!  Whether it would be useful for something
other than puzzling the few readers who would bother is a different
story.  Let me briefly cite a few other examples of {\it useful}
equivalence classes.  Take pairs of elements from a field $f$ and $g$
in the expression $f\,dg$ and form equivalence classes based on
whether the expression can be transformed to $f'\,dg'$; this is one
way to introduce differentials into an algebraic context.  Take points
in a Cartesian geometry $(x_1, x_2,\ldots x_n)$ and group them
together if they are related by a simple constant multiple $(\lambda
x_1, \lambda x_2,\ldots \lambda x_n)$; you now have lines through the
origin and the basis for projective geometry.  Take infinite
convergent sequences of rational numbers (from {\bf Q}) and group them
together if the differences between them converge to zero; the
equivalence classes are the real numbers.  I could keep going.  These
constructions are easy to form; their utility lies in our ability to
relate them to real world problems.

\mysection{Algebraic Extensions}
\subsection*{\qquad The field ${\cal F}[x]$ mod $n(x)$}

Both our quotient field and polynomial ring constructions are examples
of {\it extensions}.  Simply put, when we use an algebraic system to
construct a new algebraic system that includes the original system as
a subset, then the new system is an {\it extension} of the
original.\footnote{The key property is that the one system is a subset
of the other, not the exact method of construction.}  So, ${\bf Z}[x]$
is an extension of ${\bf Z}$ because we can identify ${\bf Z}$ as a
subset of ${\bf Z}[x]$ and, in fact, even homomorphicly map ${\bf Z}$
into ${\bf Z}[x]$.  Such an {\it inclusion homomorphism} should not be
confused with an evaluation homomorphism, which would map the other
way (from ${\bf Z}[x]$ into ${\bf Z}$).

The only remaining type of extension that will be important to us is
the {\it algebraic extension}.  It is another equivalence class
construction that we build starting with a polynomial ring over a
field, say ${\cal F}[x]$.  Our equivalence classes are all elements in
${\cal F}[x]$ whose differences are multiples of some distinguished
irreducible polynomial in ${\cal F}[x]$, called the {\it minimal
polynomial} of the field.  And I do say {\it field}, because we don't
need to use the quotient field construction with an algebraic
extension; the ring is already a field.

Polynomial long division (section \ref{sec:Long Division})
can be used to reduce modulo polynomials, which allows
us to perform addition, subtraction, and multiplication
in an modulo field, but how do we perform division?
The polynomial Diophantine equation algorithm
described in section \ref{sec:Polynomial Diophantine Equations}
can be used to solve the following equation:

$$sx + tn = \gcd(x,n)$$

Now, if $n$ is irreducible, its GCD with any polynomial
of lower degree will be 1, so our equation becomes:

$$sx + tn = 1$$

Reducing mod $n$, we obtain

$$sx \equiv 1 \mod n$$

So $s$, when multiplied by $x$, yields 1, which means
that $s$ and $x$ are multipicative inverses.
If $n$ was not irreducible, this construction would
not work for all values of $x$, and we would have
a ring but not a field.

\begin{comment}
Again, like with the quotient field, I tend to be a bit loose with the
notation.  Something like the Gaussian integers, which I wrote as
${\bf Z}[i]; i^2=-1$, really should be expressed as equivalence
classes modulo the polynomial $i^2+1$, i.e.  ${\bf Z}[i]/(i^2+1)$.
\end{comment}


\mysection{Trace and Norm}
\qquad [van der Waerden], \S5.7

Given an algebraic extension $E$ of a field $F$, two of the
coefficients in the minimal polynomial have a special significance
that often makes them particularly useful.  Our only use of them will
come in Chapter 3 in the proof of Liouville's theorem.

First, let's note that while we used a minimal polynomial to
construct the extension field,
in fact, every element in the extension field
has a minimal polynomial associated with it,
which can be constructed by raising the element to successive
powers until one of the powers is a linear combination
of the lower powers.

\definition
The {\it trace} of an
element $x$ in $E$, written ${\rm Tr}(x)$, is the coefficient
of the second-highest power in its minimal polynomial.
\enddefinition

\definition
The {\it norm} of an element, written ${\rm N}(x)$, is its zeroth-order coefficient.
\enddefinition

The special significance of these elements becomes more obvious if we
consider a {\it splitting field}, which is a futher field extension
(an extension of the extension) in which the minimal polynomial
factors completely into linear factors.  The element $x$ is
itself one of the roots in these factors; the remaining
roots are called the {\it conjugates} of $x$.

Write $x$'s minimal polynomial in the form:
$$\prod_{i=1}^n \left( t - x_i \right) = \sum_{k=0}^n (-1)^{k} a_k t^{n-k}$$
and note that ${\rm Tr}(x)$, the $n-1^{\rm th}$ coefficient in the polynomial,
is the sum of all the conjugates, while ${\rm N}(x)$, the zeroth order
coefficient in the polynomial, is the product of all the conjugates.

\vfill\eject

\mysection{Long Division}
\qquad [van der Waerden], \S3.4

As we all learned in grade school, polynomials can be divided
using long division.  To generalize this in our more abstract
context, let's consider a very simple calculation of this type:

\input{02-ALGEBRA-EXAMPLE2.inc}

Each step starts by dividing the leading terms, i.e, $x^2$ is divided
by $2x$ to form $x\over 2$.  Actually, we can be a bit more precise.
Each step starts by dividing the leading {\it coefficients},
since the variables are divided just by subtracting their
powers. $x^2$ divided by $x$ is just $x$.  We divide $1$ by $2$
to form $1\over2$ and in this manner obtain $x\cdot{1\over2}={x\over2}$.

Next, we multiply this value by the divisor to obtain a polynomial
that we will subtract from the dividend (or what remains of it after
prior steps).  Again, let's be more precise.  We multiply the
polynomial variable just by adding its powers.  What we really have to
{\it multiply} are the {\it coefficients}.  To multiple $x\over2$ by
$2x+1$ we multiply $1\over2$ by $2$ to obtain $1$, add the powers of
$x$ and $x$ to obtain $x^2$, and arrive at the first term $1\cdot
x^2=x^2$.  Next, we multiply $1\over2$ by $1$, get $1\over2$, add the
powers of $1$ and $x$ to obtain $x$, and have the second term
${1\over2}\cdot x={1\over2}x$.  Adding these terms we get
$x^2+{1\over2}x$ --- the first of the intermediate polynomials.

To perform the third step, we don't have to do anything with
the variables.  We just subtract the coefficients.  These
three steps are repeated until we are left with a remainder
of lower degree than the divisor.

So, to summarize, working with the polynomial variable is easy --- we
just add or subtract its integer powers.  We perform polynomial long
division by dividing, multiplying, and subtracting the {\it
coefficients}.  Now, these are three of the basic four operations
provided by a field.  It follows, therefore, that we can perform
polynomial long division on polynomials whose coefficients lie in any
field whatsoever.  Given ${\cal F}[x]$, a polynomial ring over a
field, we can use the field operations provided by ${\cal F}$ to
divide any two elements from ${\cal F}[x]$ using polynomial long
division and obtain a remainder and a quotient.

We can even say a bit more.  Just like with grade school long
division, we know that the degree of the quotient will be the
difference in degrees of the dividend and the divisor, and that the
degree of the remainder will be less than the degree of the divisor.
We just need to keep in mind that these degrees are measured relative
to the polynomial ring variable, not any other variable that might
appear as part of the underlying field.

\mysection{Greatest Common Divisors}
\qquad [van der Waerden], \S3.7, \S3.8, \S5.4 (multivariate rings)\hfil\break
\hbox{}\qquad [Geddes], Ch. 7

One of the most important uses of polynomial long division is to
compute greatest common divisors (GCDs), at least in theory.  In
practice, there are other, more efficient algorithms.\footnote{See
[Geddes], for example} However, because long division is a simple and
straightforward way to compute GCDs, because it provides a theoretical
underpinning for other methods, and because it leads us directly to
solving polynomial diophantine equations, I'll present it here in this
section.

The first thing to observe is that the long division equation, $D = qd
+ r$ (dividend equals quotient times divisor plus remainder), can be
rearranged to read $r = D - qd$, which shows that any common divisor
of the dividend and the divisor can be divided out from the right hand
side of the equation, so must divide the left hand side also.  Thus,
common divisors of the dividend and divisor are preserved in the
remainder.

Furthermore, since the remainder is always of lower degree than the
divisor, we can repeat the long division with the divisor as the new
dividend and the remainder as the new divisor.  The new remainder will
also preserve common divisors of the original dividend and divisor,
and will be of lower degree than the original remainder.  This process
can repeated, lowering the degree of the remainder at each step, until
we are left with a zero remainder, i.e. $D' = q' d'$, where I've used
primes to emphasize that we are no longer dealing with the original
dividend and divisor.  Since common divisors have been preserved
throughout by $D'$ and $d'$, it follows that $d'$, the divisor of the
last step, must be a common divisor.  It is, in fact, a greatest
common divisor, if GCDs exist in this ring.  This has been known since
the time of Euclid, at least in the case of integers.

I did say ``if'' GCDs exist, because nothing in our axioms guarantee
their existance.  The problem is that there might be a lattice of
divisors for a given element, instead of a strict ordering of them.
We'll remedy this, again, by introducing a new axiom.

A {\it unique factorization domain} ${\cal U}$ obeys all the ring axioms,
as well as:

\begin{center}
\begin{tabular}{l l r}
   unique factorization & $\forall a,b,c,d \in {\cal U},$ & \cr
      & $ab=cd \implies \exists x\in {\cal U}, ax=c \cup ax=d \cup bx=c \cup bx=d$ &(U1)\cr
\end{tabular}
\end{center}

U1 implies I1.  Take a unique factorization domain, and pick two
elements $c$ and $d$ which are multiples of zero, $cd=0$.  Obviously,
we can pick $a=0$ and $b=0$ and have $ab=0=cd$.  So, by U1, $x$ exists
so that $0x=c$ or $0x=d$, which by the zero theorem implies that
either $c=0$ or $d=0$.  This proves I1.  Thus, a unique factorization
domain is also an integral domain.  Also, ${\cal F}[x]$ is a unique
factorization domain (proof omitted).

U1 also implies the existence of GCDs.  Consider an element $x$ with
two different factors, say $a$ and $c$, so $x=ab$ and $x=cd$.  U1
immediately implies that either $a$ is a factor of $c$, if $aa'=c$,
or that $c$ is a factor of $a$, if $aa'=d$ and $x=caa'=ab$ and $ca'=b$.
FIX THIS.

Not all rings satisfy U1.  Consider, for example, ${\bf Z}[i];
i^2=-1$, the Gaussian integers.  This ring differs from the polynomial
ring ${\bf Z}[x]$ because polynomials of degree two and higher don't
exist since the square of $i$ is -1; $i$ is thus {\it algebraic} (see
below) and this makes all the difference.  The number 9 can be
factored two different ways in this ring: $9=3\cdot3=(4-i)(4+i)$.
It's not too hard to see that 3 can't be multiplied by any Gaussian
integer to form either $(4-i)$ or $(4+i)$, so U1 is not satisfied.
The Gaussian integers form an integral domain, but not a unique
factorization domain.

I did say a greatest common divisor, not the greatest common divisor,
because there can be more than one.\footnote{Actually, I haven't even
proved that GCDs exist at all, and in some algebraic systems, they
don't!}  A {\it unit} is an invertable element.  Put another way, $u$
is a unit if there exists $u'$ such that $uu'=1$.  Now, any divisor
can be transformed into another divisor by multiplying it by a unit,
since if $uu'=1$, then $ab=(ua)(u'b)$ for any $a$ and $b$ whatsoever.
In particular, a greatest common divisor can be transformed into
another greatest common divisor by multiplying it by a unit.  I leave
without proof the claims that in ${\cal F}[x]$, the units are all
elements in ${\cal F}$, and that all GCDs differ from each other by a
unit multiple.

A few words are in order here about GCDs in systems of the form ${\cal
U}[x]$, i.e, where the coefficients come from a unique factorization
domain that is not a field.  A factorization in ${\bf Q}(x)[y]$ or
${\bf Q}(y)[x]$ (both of the form ${\cal F}[x]$) is superficially so
similar to a factorization in ${\bf Q}[x,y]$ ({\it not} of the form
${\cal F}[x]$), that the distinction should be noted.  In both of the
first two cases, we form a quotient field with respect to one of the
two variables and thus obtain a polynomial ring (in the other
variable) over the quotient field.  In the case of ${\bf Q}[x,y]$ we
do not form a quotient field with respect to either variable; thus we
have a polynomial ring over not a field, but over another polynomial ring.

Now a polynomial ring over a unique factorization domain ${\cal U}[x]$
itself satisfies U1 (proof omitted), so by induction any finite series
of such polynomial rings over a unique factorization domain (like
${\bf Q}[x,y]$) is also a unique factorization domain.  This implies
that GCDs exist in ${\cal U}[x]$-type systems.  The problem is finding
them, since long division only works cleanly in an ${\cal F}[x]$-type
system.

The solution, invented by Gauss\footnote{check this}, is to first
factor out of each polynomial the GCD of the coefficients (calculated
in ${\cal U}$) which we call the {\it content} of the polynomial,
leaving a {\it primitive polynomial}.  It can be shown\footnote{van
der Waerden} that if a primitive polynomial factors at all, then it
factors into primitive polynomials.  We thus can compute a primitive
GCD of the primitive parts and multiply this by the GCD of the
contents to obtain a GCD in ${\cal U}[x]$.  A LITTLE UNCLEAR.

We will have little use for ${\cal U}[x]$ factorizations in this book,
since invariably we will calculate GCDs with respect to one variable,
and form quotient fields from any others, and thus always be working
in ${\cal F}[x]$ systems.  I mention this mainly to avoid confusion
between factoring in ${\cal F}[x,y]$ and ${\cal F}(x)[y]$, and have
thus omitted the proofs of Gauss' method; see the references for
details.

\vfill\eject

\example

Compute a GCD of $4x^4+13x^3+15x^2+7x+1$ and $2x^3+x^2-4x-3$ in ${\bf Q}[x]$.

\bigskip
\input{02-ALGEBRA-EXAMPLE1.inc}

The divisor of the last step, in this case ${35\over2}x^2+35x+{35\over2}$,
is the GCD, or I should say a GCD, since multiplying by any unit
will produce a different GCD.  In the case of a polynomial ring over
a field, the units are the elements of the underlying field,
so we can multiply by anything in {\bf Q} (i.e, any rational number)
and get another GCD.  For this example, the obvious thing to multiply
by is $2\over35$, which both clears the denominators and divides out
the common factor in the numerators to produce $x^2+2x+1$.  Both
answers are acceptable.

\endexample

\vfill\eject

\example

Compute the GCD of $5xy-5y^2-7x+7y$ and $2x^2-yx-y^2$ in ${\bf Q}[x,y]$.

This is a ${\cal U}[x]$-type system, so we'll need to work in a
${\cal F}[x]$-type system to perform the computation.  Our choices
are ${\bf Q}(x)[y]$ and ${\bf Q}(y)[x]$.

Let's start with ${\bf Q}(x)[y]$, and rearrange the polynomials
so that $y$ is the polynomial variable:

$$-5y^2+(5x+7)y-7x {\rm\qquad and\qquad} -y^2-xy+2x^2$$

The first step is to compute the content (GCD of the coefficients) of
each polynomial.  Clearly, the GCD of $-5$, $(5x+7)$, and $-7x$ is 1
and the GCD of $-1$, $-x$, and $2x^2$ is also 1, so both polynomials
are already primitive and we can just proceed with the GCD calculation
in ${\bf Q}(x)[y]$:

\input{02-ALGEBRA-EXAMPLE5a.inc}

This leads us to conclude that the last divisor,
$-(2x+{7\over5})y+(2x^2+{7\over5}x)$ is a GCD in ${\bf Q}(x)[y]$.  Now
we need to remove its content, which is the GCD of $-(2x+{7\over5})$
and $(2x^2+{7\over5}x)$, or $(2x+{7\over5})$.  Dividing through by
this polynomial (a polynomial in ${\bf Q}[x]$, and thus a unit in
${\bf Q}(x)[y]$) we obtain $-y+x$.  We now multiply by the GCD of our
original contents, but they were just 1, so we conclude that $x-y$
is our GCD in ${\bf Q}[x,y]$.

Now let's do all that again in ${\bf Q}(y)[x]$.  Our polynomials become:

$$(5y-7)x-(5y^2-7y) {\rm\qquad and\qquad} 2x^2-yx-y^2$$

The second one has unit content (the GCD of $2$, $-y$, and $-y^2$),
but the first one's content is $\gcd_y(5y-7,5y^2-7y)=5y-7$.
Dividing this out, we obtain:

$$x-y {\rm\qquad and\qquad} 2x^2-yx-y^2$$

and compute:

\input{02-ALGEBRA-EXAMPLE5.inc}

Thus, $x-y$ is the GCD of the primitive polynomials, and it has unit
content $\gcd_y(1,-y)$.  The GCD of the original contents
($1$ and $5y-7$) is 1, so the final result is again $x-y$.

\endexample

\vfill\eject

\mysection{Polynomial Diophantine Equations}

The same long division procedure used for GCD computations can also be
used solve a certain class of {\it polynomial Diophantine equations}.
A Diophantine equation is one whose variables are restricted to be
integers.  The most famous example is Fermat's equation,
$x^n+y^n=z^n$; Fermat's theorem states that this equation has no
solutions $x,y,z,n\in{\bf Z}$ for $n>2$.  A generalized Diophantine
equation is one whose variables are restricted to some algebraic
system, not necessarily ${\bf Z}$.  A polynomial Diophantine equation
is one whose variables are restricted to be polynomials of some form,
and the one we will consider here is this:

\begin{displaymath}
sa+tb=c; \qquad a,b,c\in{\cal F}[x] {\rm\, given}; \qquad
s,t\in{\cal F}[x] {\rm\, unknown}
\end{displaymath}

Thus, $xxx$ is an equation of this form.

Let's begin by noting that any common divisor of $a$ and $b$, and in
particular $\gcd(a,b)$, can be divided out from the left hand side of
the equation, and thus must also divide the right hand side, so $c$
must be be a multiple of $\gcd(a,b)$, or the equation has no solution.

This necessary condition is also sufficient, and the simplest way to
demonstrate this is to use the GCD computation in a constructive
proof.  Note that first step in computing $\gcd(a,b)$ is to solve
$a=qb+r$.  Rearranging this as $r=a-qb$ we see how the remainder can
be expressed in the Diophantine form $sa+tb$.  More generally, at each
step of the calculation, we solve $D=qd+r$, where $D$ and $d$ are each
either $a$, $b$, or a remainder from a previous step, so using
$r=D-qd$ we can write each remainder in the form $sa+tb$.  At the end
of the calculation, we will have expressed $\gcd(a,b)$ in the form
$sa+tb$.

We now use long division to divide $c$ by $\gcd(a,b)$.  Because of the
necessity demonstrated above, the division must be exact (i.e, zero
remainder) or the equation has no solution.  Having computed both
$\gcd(a,b)=sa+tb$ and $c=q\gcd(a,b)$ we can now combine these
expression to form $c=(qs)a+(qt)b$, which solves the original
equation.

This solution is not unique.  Given a solution to $c=sa+tb$, we can
form any multiple of $ab$, say $mab$, and write another solution
$c=(s-mb)a+(t+ma)b$.  Note however, that $(s-mb)$ has the form of a
remainder after dividing $s$ by $b$ ($m$ is the quotient).  Since the
degree of a remainder is always less than the degree of the divisor,
it follows that if $sa+tb=c$ can be solved, then we can always compute
an $s$ of lower degree than $b$, or a $t$ of lower degree than $a$.

If $\deg(c)<\deg(a)+\deg(b)$, then these conditions are not exclusive;
finding an $s$ of lower degree than $b$ implies a $t$ of lower degree
than $a$.  To see this, simply note that if $\deg(s)<\deg(b)$, then
$\deg(sa)=\deg(s)+\deg(a)<\deg(b)+\deg(a)$.  Since $tb=c-sa$, if
$\deg(c)<\deg(a)+\deg(b)$ and $\deg(sa)<\deg(a)+\deg(b)$, then
$\deg(tb)<\deg(a)+\deg(b)$, which implies $\deg(t)<\deg(a)$.

We will make repeated use of this polynomial Diophantine equation
throughout the book.

\vfill\eject

\example

Solve:

$$s(4x^4+13x^3+15x^2+7x+1) + t(2x^3+x^2-4x-3) = x^3 + 5x^2 + 7x +3$$

\quad for $s,t \in {\bf Q}[x]$ satisfying minimal degree bounds.

\bigskip
\input{02-ALGEBRA-EXAMPLE1.inc}

%These are the same polynomials used for the first GCD example.
%Using the notation

%\begin{eqnarray*}
%a &=& 4x^4+13x^3+15x^2+7x+1, \cr
%b &=& 2x^3+x^2-4x-3, {\rm\,and} \cr
%c &=& x^3 + 5x^2 + 7x +3 \cr
%\end{eqnarray*}

$$a = 4x^4+13x^3+15x^2+7x+1; \qquad
b = 2x^3+x^2-4x-3; \qquad
c = x^3 + 5x^2 + 7x +3$$

%we are trying to solve $sa+tb=c$.  The first step in the GCD
%calculation yielded:

\begin{eqnarray*}
a &=& (2x+{11\over2})b + ({35\over2}x^2+35x+{35\over2})
 %{\rm,\,or}
 \cr\cr
x^2+2x+1 &=& {2\over35}a - {1\over35}(4x+11)b \cr
\end{eqnarray*}

%Having concluded at the last step in the GCD calculation that
%$x^2+2x+1$ is a GCD of $a$ and $b$, we now divide it into $c$:

\input{02-ALGEBRA-EXAMPLE3.inc}

%The remainder is zero, so the problem has a solution.
%We substitute our expansion for $x^2+2x+1$ above into $c=(x+3)(x^2+2x+1)$
%and obtain:

%\begin{eqnarray*}
%c &=& {2\over35}(x+3)a - {1\over35}(x+3)(4x+11)b \cr\cr
%  &=& {2\over35}(x+3)a - {1\over35}(4x^2+23x+33)b \cr
%\end{eqnarray*}

$$c = (x+3)(x^2+2x+1)
%= {2\over35}(x+3)a - {1\over35}(x+3)(4x+11)b
 = {2\over35}(x+3)a - {1\over35}(4x^2+23x+33)b $$

%$\deg({2\over35}(x+3)) = 1 < \deg(b) = 3$ and $\deg({1\over35}(4x^2+23x+33))
%= 2 < \deg(a) = 4$, so the degree bounds are already met.

%Now verify this solution using Maxima, a computer algebra system:

\begin{computer-maxima}
(%i22) a: 4*x^4+13*x^3+15*x^2+7*x+1;

                           4       3       2
(%o22)                  4 x  + 13 x  + 15 x  + 7 x + 1
(%i23) b: 2*x^3+x^2-4*x-3;

                                 3    2
(%o23)                        2 x  + x  - 4 x - 3
(%i24) divide(a,b);

                                                  2
                                   4 x + 11  35 x  + 70 x + 35
(%o24)                            [--------, -----------------]
                                      2              2
(%i25) divide(b,divide(a,b)[2]);

                                            4 x - 6
(%o25)                                     [-------, 0]
                                              35
(%i26) gcdex(a,b);

                                          2
                            4 x + 11  35 x  + 70 x + 35
(%o26)/R/             [1, - --------, -----------------]
                               2              2
(%i27) c: x^3+5*x^2+7*x+3;

                               3      2
(%o27)                        x  + 5 x  + 7 x + 3
(%i28) d: c/gcdex(a,b)[3];

                                    2 x + 6
(%o28)/R/                           -------
                                      35
(%i29) gcdex(a,b)[1]*d;

                                    2 x + 6
(%o29)/R/                           -------
                                      35
(%i30) gcdex(a,b)[2]*d;

                                   2
                                4 x  + 23 x + 33
(%o30)/R/                     - ----------------
                                       35
(%i31) gcdex(a,b)[1]*d*a + gcdex(a,b)[2]*d*b;

                               3      2
(%o31)/R/                     x  + 5 x  + 7 x + 3
\end{computer-maxima}

\begin{comment}
{\small\begin{verbatim}
i36 : R=QQ[x]

o36 = R

o36 : PolynomialRing

i37 : a = 4*x^4+13*x^3+15*x^2+7*x+1

        4      3      2
o37 = 4x  + 13x  + 15x  + 7x + 1

o37 : R

i38 : b = 2*x^3+x^2-4*x-3

        3    2
o38 = 2x  + x  - 4x - 3

o38 : R

i39 : a//b

           11
o39 = 2x + --
            2

o39 : R

i40 : a%b

      35  2         35
o40 = --*x  + 35x + --
       2             2

o40 : R

i41 : b//(a%b)

       4      6
o41 = --*x - --
      35     35

o41 : R

i42 : b%(a%b)

o42 = 0

o42 : R

\end{verbatim}}
\end{comment}

{\small\begin{verbatim}
i36 : R=QQ[x]

o36 = R

o36 : PolynomialRing

i50 : a = 4*x^4+13*x^3+15*x^2+7*x+1;

i51 : b = 2*x^3+x^2-4*x-3;

i52 : c = x^3+5*x^2+7*x+3;

i53 : d = gcdCoefficients(a,b)

       35  2         35            11
o53 = {--*x  + 35x + --, 1, - 2x - --}
        2             2             2

o53 : List

i54 : c%d#0

o54 = 0

o54 : R

i55 : e = c//d#0

       2      6
o55 = --*x + --
      35     35

o55 : R

i56 : f = apply(d, i -> i*e)

        3     2            2      6     4  2   23     33
o56 = {x  + 5x  + 7x + 3, --*x + --, - --*x  - --*x - --}
                          35     35    35      35     35

o56 : List

i57 : a*f#1+b*f#2

       3     2
o57 = x  + 5x  + 7x + 3

o57 : R

\end{verbatim}}

\endexample

\vfill\eject


\mysection{Square-free factorization}

\hbox{}\qquad [Geddes], \S 8.2

A {\it square-free polynomial} is one with no repeated factors.
$x^2-1$ is square-free because it factors as $(x-1)(x+1)$.  $x^2+2x+1$
is not square-free because it factors as $(x+1)^2$.

Whether or not a polynomial is square-free is independent of the field
in which the factorization occurs.

A {\it square-free factorization} of a polynomial is a factorization
into square-free factors, each of which appears at a different power.
It is much easy to compute than a full factorization into irreducible
factors and for this reason will be quite useful to us.

Surprisingly, a polynomial's square-free factorization is independent
of its algebraic system!  For example, $x^2+1$ is irreducible in ${\bf
R}[x]$, so its square-free factorization is simply $x^2+1$.  On the
other hand, in ${\bf C}[x]$, $x^2+1$ factors as $(x+i)(x-i)$.  Yet
both of these factors combine together in the square-free
factorization (since they both appear to the first power), so
$x^2+1$'s square-free factorization in ${\bf C}[x]$ is\ldots $x^2+1$.

To compute square-free factorizations, we'll use an operation that,
for lack of a better word, I'll call ``differentiation.''  We
``differentiate'' a polynomial by multiplying each term by its power
and then lowering the power by one.

This ``differentiation'' not to be confused with the field operation
that I will define in the next chapter.  ``Differentiation'' is simply
a mechanical procedure of lowering powers and multiplying by
constants.  In particular, no attempt is made to ``differentiate''
the coefficients {\it even if they are not constants}.

To compute a square-free factorization, first we ``differentiate'' the
polynomial.  The result is a second polynomial with the degree of all
factors reduced by one.  Note in particular that any factors of unit
degree (and only those factors) disappear completely.  Dividing this
into the original polynomial, we obtain a polynomial with no square
factors --- all factors now appear with unit degree.  Computing the
GCD of this polynomial with the original one also produces a
polynomial with only factors of unit degree, except that the original
unit degree factors are missing.  We can divide this last two
polynomials into each other to determine the original unit degree
factors.  Going back to the ``differentiation'' step, we can keep
repeating the process until we have obtained all the square-free
factors.

\vfill\eject

\mysection{Partial Fractions Expansion}
\qquad [van der Waerden], \S5.10

As a first application of polynomial Diophantine equations, we use
them to construct partial fractions expansions.  Consider an element
$a$ from a polynomial quotient field ${\cal F}(x)$.  We can write
$a={n\over d}$ where $n,d\in{\cal F}[x]$.  If we are now given a
factorization of $d=d_1^{e_1} d_2^{e_2} \cdots d_k^{e_k}$, where
$d_i\in {\cal F}[x]$ and $\gcd_{{\cal F}[x]}(d_i,d_j)=1$ if $i\ne j$,
and assuming that $a$ is a proper fraction
($\deg_{{\cal F}[x]}n < \deg_{{\cal F}[x]}d$),
then we can construct a {\it partial fractions expansion} of $a$:

\begin{displaymath}
a={n\over d}=\sum_{i=1}^{n}\sum_{j=1}^{e_i} {n_{ij}\over {d_i}^j}
\qquad \deg_{{\cal F}[x]}(n_{ij}) < \deg_{{\cal F}[x]}(d_i)
\end{displaymath}

We begin by computing an expansion in the form:

\begin{displaymath}
a={n\over d}=\sum_{i=1}^{n} {n_i\over {d_i}^{e_i}}
\qquad \deg_{{\cal F}[x]}(n_i) < e_i\deg_{{\cal F}[x]}(d_i)
\end{displaymath}

$n_1$ is found by solving the following polynomial Diophantine
equation for $n_1$ and $r_1$:

\begin{eqnarray*}
n &=& n_1 \Big( \prod_{j\ne 1} {d_j}^{e_j} \Big) + r_1 (d_1^{\,e_1}) \cr
\end{eqnarray*}

Our degree bounds guarantee that
$\deg(n_1) < e_1\deg(d_1)$, and dividing through by $d$ shows:

\begin{eqnarray*}
{n\over d} &=& {{n_1}\over{d_1^{\,e_1}}} + {r_1\over{\prod_{j\ne 1} {d_j}^{e_j}}} \cr
\end{eqnarray*}

The second term on the right is a fraction in the original form,
but with one less factor in the denominator, so we can recurse
and separate out all the ${d_i}^{e_i}$ into seperate fractions.
Simple long division now suffices to seperate these fractions:

\begin{eqnarray*}
n_i &=& q_{ij} d_i + r_{ij} \cr\cr
{{n_i}\over{d_i^j}} &=& {{q_{ij}}\over{d_i^{\,j-1}}} + {r_{ij}\over{d_i^{\,j}}} \cr
\end{eqnarray*}

The $r_{ij}$ are our original $n_{ij}$, and the degree bounds on long
division ensure that $\deg_{{\cal F}[x]}(n_{ij}) < \deg_{{\cal F}[x]}(d_i)$.
We recurse on the first term until we have completed the desired construction.

\vfill\eject

\example

Compute the partial fractions expansion of $${x^2 + 3x + 2}\over{x^3-3x^2+4}$$

We begin by factoring the denominator.  While factoring can be quite
complicated in practice, in this case we need only try some small
integers to discover that either 2 or -1 solve the denominator,
leading directly to a factorization:

\begin{comment}
Differentiating to obtain $3x^2-6x$.
Computing the GCD of $x^3-3x^2+4$ and $3x^2-6x$:

\input{02-ALGEBRA-EXAMPLE4c.inc}

Thus, $-2x+4$ is a GCD, which we normalize by dividing through by -2
to obtain $x-2$.  We could now proceed by dividing $x^3-3x^2+4$ by
$x-2$ to obtain $x^2-x-2$ (all factors at unit power), compute the GCD
of $x-2$ and $x^2-x-2$ to obtain $x-2$ (all higher factors at unit
power), divide $x^2-x-2$ by $x-2$ to obtain $x+1$ (the unit
square-free factor), and repeat the process (trivially) with $x-2$ to
decide that $x-2$ is the second square-free factor.  Or, we could
shortcut the entire process by noting that since $x-2$ is linear,
it can only be the second square-free factor.  In any event, we conclude that:

\end{comment}

$${{x^2 + 3x + 2}\over{x^3-3x^2+4}} = {{x^2 + 3x + 2}\over{(x-2)^2(x+1)}}$$

Next, we solve the polynomial Diophantine equation:

$$x^2 + 3x + 2 = s(x-2)^2 + t(x+1) = sa+tb$$

Compute the GCD of $a=(x-2)^2=x^2-4x+4$ and $b=x+1$:

\input{02-ALGEBRA-EXAMPLE4.inc}

Since the remainder, 9, is a unit, $a$ and $b$ have no common
factors and their GCD is 1.  Of course, this result is hardly
surprising since $(x-2)$ and $(x+1)$
have no common factor between them.

$$a=(x-5)b+9$$
$$9=a-(x-5)b$$
$$1={1\over9}[a-(x-5)b]$$
$$x^2 + 3x + 2 = {1\over9}(x^2+3x+2)a-{1\over9}(x^2+3x+2)(x-5)b$$
$$x^2 + 3x + 2 = {1\over9}(x^2+3x+2)a-{1\over9}(x^3-2x^2-13x-10)b$$

Our degree bounds aren't met yet, so we divide $b=x+1$ into $x^2+3x+2$:

\input{02-ALGEBRA-EXAMPLE4a.inc}

The zero remainder means that the $a$ term drops away completely,
and after subtracting $(x+2)a=x^3-2x^2-4x+8$ from the $b$ coefficient,
we conclude that:

$$x^2 + 3x + 2 = -{1\over9}(-9x-18)b = (x+2)b$$

In other words (remember that $b=x+1$),

$${{x^2 + 3x + 2}\over{(x-2)^2(x+1)}} = {{(x+2)(x+1)}\over{(x-2)^2(x+1)}} = {{(x+2)}\over{(x-2)^2}}$$

We need only divide $(x+2)$ by $(x-2)$:

\input{02-ALGEBRA-EXAMPLE4b.inc}

% $$(x+2)=(x-2)+4$$

so,

$${{x^2 + 3x + 2}\over{x^3-3x^2+4}} = {4\over{(x-2)^2}} + {1\over{(x-2)}}$$

Again, verify this result using a computer algebra system.

\endexample

\vfill\eject


\mysection{Resultants}
\qquad [van der Waerden], \S5.8; [Lang], \S IV.8

At times, we will want a simple way of testing two polynomials over a
field to see if they have a GCD, without actually computing it.  This
is more than just a computational convenience.  The presence of the
polynomial's variable in the GCD often encumbers us.  On the other
hand, the {\it resultant} yields a simple element from the underlying
field that is zero if the polynomials have a non-trivial GCD and
non-zero otherwise.

For example, the polynomials $t x+x+t+1$ and $ty + y$ share $t+1$
as a GCD, so their $t$-resultant is 0.  MORE HERE - why a t-resultant,
not an x-resultant?

The resultant is defined as the determinant of the Sylvester matrix
$S_x(P,Q)$, which is the $m+n \times m+n$ matrix constructed from two
polynomials (in ${\cal F}[x]$) $P$ and $Q$ of degrees $m$ and $n$ (all
the blanks are zeros):

$$ P = \sum_{i=0}^m p_i \, x^i \qquad Q = \sum_{i=0}^n q_i \, x^i $$

$$ S_x(P,Q) = \begin{pmatrix}
  p_m & p_{m-1} & \ldots & p_0 & & & \cr
  & p_m & p_{m-1} & \ldots & p_0 & & \cr
  & & \ldots & & \ldots & & \cr
  & & & p_m & p_{m-1} & \ldots & p_0 \cr
  \vdots & & & \vdots & & & \vdots \cr
  q_n & q_{n-1} & \ldots & q_0 & & & \cr
  & q_n & q_{n-1} & \ldots & q_0 & & \cr
  & & \ldots & & \ldots & & \cr
  & & & q_n & q_{n-1} & \ldots & q_0 \cr
  \end{pmatrix} $$

In plain English, the matrix is constructed by forming the first row
from the first polynomial coefficients, adding $n-1$ trailing zeros at
the end of the row.  The second row is formed by shifting the first
row one position to the right.  This shifting is repeated a total of
$m-1$ times to obtain the first $m$ rows.  The last $n$ rows are
constructed in the same way from the second polynomial.

Now consider the following straightforward matrix identity:

$$ S_x(P,Q) \begin{pmatrix}x^{n+m-1}\cr x^{n+m-2}\cr \vdots \cr x \cr 1\end{pmatrix}
 = \begin{pmatrix}P x^{m-1}\cr P x^{m-2}\cr \vdots \cr Q x \cr Q\end{pmatrix} $$

If $\det S_x(P,Q)$ is non-zero, then the Sylvester matrix is
invertible, and we can form the following equation:

$$ \begin{pmatrix}x^{n+m-1}\cr x^{n+m-2}\cr \vdots \cr x \cr 1\end{pmatrix}
 = S_x(P,Q)^{-1} \begin{pmatrix}P x^{m-1}\cr P x^{m-2}\cr \vdots \cr Q x \cr Q\end{pmatrix} $$

Since the matrix is formed exclusively from the polynomials'
underlying field ${\cal F}$, its inverse must also be formed from
${\cal F}$.  Now consider the bottom element in the last equation.  It
must have the following form:

$$ 1 = f_0 P x^{m-1} + f_1 P x^{m-2} + \ldots + f_{n+m-1} Q x + f_{n+m} Q \qquad f_i \in {\cal F}$$
$$ 1 = A P + B Q \qquad A,B \in {\cal F}[x] $$

The only way this statement can be true is if $P$ and $Q$ (viewed
as polynomials in $x$) have a
trivial GCD, so a non-zero determinant of $S_x(P,Q)$ imply that $P$ and
$Q$ have only a trivial GCD.

Conversely, assume that $\gcd_x(P,Q) = 1$.  Then we can solve a series
of polynomial Diophantine equations to express $1, x, \ldots,
x^{n+m-1}$ as $AP+BQ$, where $\deg A < \deg Q = n$ and $\deg B <
\deg P = m$, which suffices to construct an inverse of the Sylvester
matrix.

We have thus proved:

\begin{theorem}\label{resultant theorem}
The resultant is zero iff the two polynomials have a non-trivial GCD.
\end{theorem}

Let me note two points.  First, though we used a field construction
for the proof, determinants are constructed using only ring
operations, so resultants can be computed in any ring and the result
will be a ring element.  The proof depends on the ring having a
well-defined fraction field, but since UFDs are integral domains, the
GCD concept doesn't make sense without a fraction field anyway.

Second, if the underlying ring involves multiple variables, the net
effect of the resultant is to eliminate one of them.  To see this,
imagine arbitrary values being assigned to the other variables.  The
resultant yields a condition on the remaining variables for the
original system to be solvable.


\example Compute the $t-$resultant of $t^2 - 1 -x$ and $t^3-t-y$.

$${\rm res}_t(t^2 - 1 -x, t^3-t-y) =
\det\left\vert \begin{matrix}
1 & 0 & -1 & -y & 0 \cr
0 & 1 & 0 & -1 & -y \cr
1 & 0 & -x-1 & 0 & 0 \cr
0 & 1 & 0 & -x-1 & 0 \cr
0 & 0 &1 & 0 & -x-1\end{matrix} \right\vert $$

Expanding the first two columns with cofactors, we obtain:

$$= \det\left\vert \begin{matrix}
1 & 0 & -1 & -y \cr
0 & -x-1 & 0 & 0 \cr
1 & 0 & -x-1 & 0 \cr
0 &1 & 0 & -x-1 \end{matrix} \right\vert
+ \det\left\vert \begin{matrix}
0 & -1 & -y & 0 \cr
1 & 0 & -1 & -y \cr
1 & 0 & -x-1 & 0 \cr
0 &1 & 0 & -x-1 \end{matrix} \right\vert $$

$$= \det\left\vert \begin{matrix}
 -x-1 & 0 & 0 \cr
 0 & -x-1 & 0 \cr
 1 & 0 & -x-1 \end{matrix} \right\vert
+ \det\left\vert \begin{matrix}
 0 & -1 & -y \cr
 -x-1 & 0 & 0 \cr
 1 & 0 & -x-1 \end{matrix} \right\vert $$

$$- \det\left\vert \begin{matrix}
 -1 & -y & 0 \cr
 0 & -x-1 & 0 \cr
 1 & 0 & -x-1 \end{matrix} \right\vert
+ \det\left\vert \begin{matrix}
 -1 & -y & 0 \cr
 0 & -1 & -y \cr
 1 & 0 & -x-1 \end{matrix} \right\vert $$

$$ = -(x+1)^3 + (x+1)^2 - \Big[-(x+1)^2\Big] + \Big[-(x+1) + y^2 \Big]
 = y^2 - x^3 - x^2 $$

\endexample


\vfill\eject

\begin{comment}

\mysection{Simple Field Extensions}

A {\it simple} extension of a field ${\cal F}$ is formed by adjoining
a single new element $\theta$ to the field, and then forming all
possible sums, differences, multiples, and quotients to form a new
field.  Any such simple extension will be isomorphic to either the
quotient field ${\cal F}(\theta)$ (if $\theta$ is transcendental) or
an algebraic extension ${\cal F}[\theta]$ (if $\theta$ is algebraic).

\mysection{Finite Fields}

By using the same reasoning to consider Diophantine equations over the
integers, it's not that hard to see that $2s+6t=1$ has no solution for
$s,t\in{\bf Z}$, because 2, the GCD of 2 and 6, does not divide the
right hand side of this equation.  On the other hand, if $p$ is a
prime number, then $xs+pt=1$ can always be solved for $s,t\in{\bf Z}$,
so long as $0<x<p$, since $\gcd(x,p)=1$.

This leads directly to the observation that ${\bf Z}_n$, the ring of
integers modulo $n$, is a field if and only if $n$ is in fact a prime
(and we write it ${\bf Z}_p$).  In order to invert a number $x$ in
${\bf Z}_n$, we need a solution $x'\in{\bf Z}_n$ to $xx'\equiv\, 1\,
(\pmod n)$, or $xx' = 1 + nt$, or $xx' - nt = 1$, i.e, we need
to solve the integer Diophantine equation considered above.  If $n$ is
prime, we can always solve the equation; if $n$ is composite, then the
equation can't be solved if $x$ is a factor of $n$.  Thus, everything
in ${\bf Z}_n$ is invertible if and only if $n$ is prime.

In fact, we can say more.  If ${\cal F}$ is a {\it finite field} (a
field with a finite number of elements), then its {\it prime subfield}
(its smallest subfield) must be isomorphic to ${\bf Z}_p$,
for some prime $p$.

A field can have a finite prime subfield, even if the field itself is
infinite.  Consider ${\bf Z}_5(x)$, the field of rational functions in
$x$ with coefficients in ${\bf Z}_5$.  Clearly, we can build
polynomials in ${\bf Z}_5(x)$ with as a high a degree as we want, and
they are all unique, so ${\bf Z}_5(x)$ is infinite.  Yet it
should also be clear that ${\bf Z}_5(x)$'s prime subfield
is ${\bf Z}_5$, which is finite.

Such fields, which share some properties of both purely infinite and
finite fields, are called {\it fields of characteristic $p$}, where
$p$ is the order of the prime subfield.  Fields with infinite prime
subfields (isomorphic to {\bf Z}) are called {\it fields of
characteristic zero}).  WHAT ABOUT RINGS OF INFINITE CHARACTERISTIC?

DEFINE ISOMORPHISM.

\end{comment}

\begin{comment}

\mysection{Linear Algebra}
\qquad [van der Waerden], Ch. 4, \S6.11 (trace of an field extension)

Determinants; trace.

\end{comment}

\mysection{Algebraic Closure}
\subsection*{\qquad The fields ${\bf R}$ and ${\bf C}$}
\qquad [van der Waerden], \S11

The most important way to characterize a field ${\cal F}$ is to extend
it to a polynomial ring ${\cal F}[x]$ and then study how polynomials
factor in ${\cal F}[x]$.

There exist fields ${\cal F}$ where all polynomials in ${\cal F}[x]$
can be completely factored into linear factors.  Such a field is said to
be {\it algebraically closed}.  The {\bf Fundamental Theorem of
Algebra} states that the complex field ${\bf C}$ is algebraically
closed.  There are several proof routes for this theorem.  The most
common involves complex analysis, and can be found in any standard
complex analysis text, usually near Liouville's Theorem on the
behavior of bounded entire functions.  I won't go into it here.

I do want to note the difference between ${\bf C}$ and ${\bf C}(x)$.
Both are fields.  ${\bf C}$ is algebraically closed because any
polynomial in ${\bf C}[x]$ can be completely factored into linear
factors, i.e, $x^3-3x^2+4=(x-2)^2(x+1)$.  ${\bf C}(x)$ is {\it not} algebraically closed.
If it were, then all polynomials in ${\bf C}(x)[y]$ could
be completely factored, but in fact, there exist polynomials
such as $y^2-x$ that can not be factored, so ${\bf C}(x)$
is not algebraically closed.

\begin{comment}

Here I will present another proof, Gauss's second method (cast into
modern terms by van der Waerden).  Not only is it more algebraic
in its flavor, but it is in fact a slightly stronger result.

Starting with unordered rings and fields, we introduce the concept of
{\it ordered} rings and fields field, where every element can be
compared to zero consistently with the axioms:

\begin{center}
\begin{supertabular}{l l l r}
   total order	& $\forall a,$ & one of $a<0, a=0, a>0$ holds &(O1)\cr
   consistency with negation & $\forall a,$ & one of $-a>0, a=0, a>0$ holds &(O1)\cr
   consistency with addition & $\forall a,b$ & $a>0 \cap b>0 \implies a+b>0$ &(O2)\cr
   consistency with multiplication & $\forall a,b$ & $a>0 \cap b>0 \implies ab>0$ &(O3)\cr
\end{supertabular}
\end{center}

I've introduced new logic symbols $\cap$ for logical and $\implies$
for logical implication.  $\cup$ is used for logical or.  Note
particularly that O2 implies that $n \cdot 1$ is always positive, never
zero, so our field characteristic is always zero.  No finite field can
be ordered.  This implies that our prime subfield is always ${\bf Q}$.
O3 implies that squares must be positive.

We further classify an ordered field as {\it Archemedian} if it
satisfies:

\begin{center}
\begin{supertabular}{l l @{} l r}
   Archemedian property	& $\forall a,$ & $\exists b \in {\bf Q}, b>a$ &(A1)\cr
\end{supertabular}
\end{center}

Non-Archemdian fields have ``infinitely large'' and ``infinitely
small'' numbers, larger and smaller than any rational number, and
${\bf Q}$ is confined to a narrow region of the field.  We will not
comsider such fields.

Now we define a {\it real field} as a field where -1 is not
expressible as a sum of squares.  A {\it maximal real field} is real
but has no proper algebraic extension that is real.  Any attempt to
algebraically extend a maximum real field would result in a non-real
field.  The prototypical example of a maximal real field is the
intersection of the algebraic closure of ${\bf Q}$ with the real line;
i.e, all purely real algebraic numbers.

We establish some key properties of an {\it ordered maximal real
field}, starting with the fact that all positive numbers are squares.
% Intuitively, this is because all positive real numbers have real
% roots, but we do not specialize to the real line, instead proofing the
% result for real fields in general.

Considering a positive element $p$ that is not a square, we construct
an algebraic extension $x^2-p$ which must produce an extension field
that where -1 can be expressed as a sum of squares, because the
original field was maximal real.  Furthermore, each element in the
extension field has the form $a + b \gamma$, where $\gamma$ solves
$x^2-p$ and $a$ and $b$ are in the original field.  So, we can write:

$$-1 = \sum_1^n (a_i + b_i \gamma)^2 = \sum_1^n (a_i^2 + 2 a_i b_i \gamma + b_i^2 p)$$

The $\gamma$ term must vanish, since -1 is in the original field, so

$$-1 = \sum_1^n a_i^2 + \sum_1^n b_i^2 p$$

This immediately implies that $p$ can not be expressed as a sum of
squares in the original field, because otherwise $-1$ could be.
Solving for $p$ we find that

$$p = \frac{-1 - \sum a_i^2}{\sum b_i^2}$$

Since squares are always positive, the expression on the right is
strictly negative, contradicting the assumption that $p$ is positive,
and proving the claim.

Every element is either a square, or the negative of a square, and
these cases are exclusive.  Thus, a maximal real field can be ordered
in only one way.  This also establishes that any sum of squares, being
positive, is a square and that adjoining $i$ creates a field where
square roots exist and thus every quadratic equation is solvable using
the quadratic formula.

Next, we want to establish that, in a maximal real field, all
polynomials of odd degree have at least one root.  We can proceed by
induction, as this is obviously true for linear polynomials, so
assuming that the theorem is satisfied for all polynomials of lower
degree, we can see that the polynomial must be irreducible, otherwise
we could factor it and apply induction to the factor of odd degree.

By adjoining a root $\alpha$ of the $n$ degree polynomial $f(x)$ we
get a non-real field:

$$-1 = \sum \phi_i(\alpha)^2 = \sum \phi_i(x)^2 + f(x) g(x)$$

All of the polynomials $\phi_i(x)^2$ have even degree, and the highest
degree terms can not cancel, since their coefficients are squares and
thus positive.  Thus, the sum has even degree of at most $2(n-1)$ and
$f(x)$ has odd degree $n$, so $g(x)$ must have odd degree of at most
$n-2$.  Induction implies that $g(x)$ must have a root $\beta$, so

$$\sum \phi_i(\beta)^2 = -1 - f(\beta) g(\beta) = -1$$

Yet $\beta$, and thus $\sum \phi_i(\beta)^2$, lies in the original
field, so this expression contradicts the claim that the original
field was real.

% $$(a_0 + a_1 \gamma + a_2 \gamma^2)^2 = a_0^2 + 2a_0 a_1 \gamma
% + (a_1^2 + 2a_0 a_2) \gamma^2 + 2a_1a_2\gamma^3 + a_2^2\gamma^4 $$
% $$= a_0^2 + 2a_0 a_1 \gamma
% + (a_1^2 + 2a_0 a_2) \gamma^2
% + 2a_1a_2(b_0 + b_1 \gamma + b_2\gamma^2)
% + a_2^2[b_0b_2 + (b_0 + b_1 b_2)\gamma + (b_1 + b_2^2)\gamma^2]
% $$
% $$= (a_0^2 + 2a_1a_2b_0 + a_2^2b_0b_2)
% + (2a_0 a_1 +2a_1a_2b_1 + a_2^2b_0 + a_2^2b_1b_2) \gamma
% + (a_1^2 + 2a_0 a_2
% + 2a_1a_2b_2
% + a_2^2b_1 + a_2^2b_2^2)\gamma^2
% $$

\theorem
Any field ${\cal F}$ where quadratic equations can always be solved
and polynomials of odd degree have at least one root is algebraically
closed.

\proof
Our main concern is polynomials of even degree.  Consider a splitting
field where all of the polynomial's roots $\alpha_i$ exist, construct
the $n(n-1)/2$ values $r_{ij} = \alpha_i + \alpha_j +
c \alpha_i \alpha_j$, where $c$ has been selected to make all of these
values distinct, and then form the polynomial $\Lambda(x) = \prod (x-r_{ij})$,
which exists in ${\cal F}[x]$ since it's symmetric.  Note that this
polynomial's degree, while larger, has one less power of two than the
original.  We repeat this procedure until we get a polynomial of odd
degree in ${\cal F}[x]$, which has at least one root in ${\cal F}$,
say $r_{ij}$.  If neither $\alpha_i + \alpha_j$ nor
$\alpha_i \alpha_j$ existed in the field, then by the Primitive
Element Theorem we could extend by $r_{ij}$, so at least one of these
two terms must exist in the field, and then the other can be
constructed.  Finally, we use the quadratic equation to construct
$\alpha_i$ and $\alpha_j$, and repeat this until we've got roots of
the original polynomial.

NEED SYMMETRIC POLYNOMIALS AND THEOREM OF PRIMITIVE ELEMENT

\endtheorem

There is only one way to extend an ordered integral domain into an
ordered quotient field.

Next, we introduce real numbers individually as Cauchy sequences and
collectively as an extension field, ordered by showing that the
trailing terms in such a sequence must either converge to zero or
become uniformly positive or negative, establishing O1.  A little more
work is then needed to show that O2 and O3 hold, and that the
Archemedian property carries through.

Next, we show that square roots always exist, as Cauchy sequences, for
all positive numbers.  This requires constructing a sequence of larger
and larger numbers, each of which we square, invert, multiply by 2,
and use A1 to demand an even larger element of {\bf Q}.  Fill in the
details.  Might need Archmedian property for this.

Next, construct ${\bf C}$ by extending algebraically to adjoin $i$,
and use the prior result to demonstrate that square roots exist for
all numbers in ${\bf C}$.  Then introduce the modulus of a complex
number and establish that $|1+\gamma|\le 1+|\gamma|$.

\end{comment}

\mysection{Polynomial factorization (optional)}
\qquad [van der Waerden], \S5.6
\hbox{}\qquad [Geddes], Chs. 5, 6, 8

Let's conclude this chapter by taking a least a brief look at fully
factoring a polynomial into its irreducible factors.  There are
several reasons to do this.

First of all, it's easy to declare that ``the Fundamental Theorem of
Algebra tells us that any polynomial in {\bf C}[x] can be factored
into linear factors'', and maybe even prove it.  That's true, but when it comes
time to actually do a computation, how do we proceed?  How do we
actually factor a polynomial?  Call it the price of success.
Differential algebra is solid enough to actually compute integrals, so
existance theorems don't cut it.  We need constructive algorithms.

Second, it's a surprisingly difficult problem.  An appreciation of its
difficulty now will motivate the discussion later when I show various
techniques that have been developed to avoid full factorization
whenever possible.  Yet the fact remains that it is at times
unavoidable.

Finally, both techniques that I will discuss here work according to a
basic principle that we'll use again later in a more advanced context,
so it makes sense to present it now in a simpler form.  Specifically,
we'll solve a difficult problem in an algebraic system by using a
homomorphism to map into a different algebraic system where we can
solve the problem more easily, then find some way of ``lifting'' this
answer back into the original system.  This is one of the most
powerful solution methods in algebra, and has been used to solve
problems once thought impossible.

Let's start simple.  We want to factor a polynomial in ${\bf Z}[x]$,
the ring of polynomials with integer coefficients.  If such a
polynomial has a factorization in ${\bf Z}[x]$, for example
$x^2-1=(x+1)(x-1)$, we want to find it.  If it has no such
factorization, for example $x^2+1$ (which would require at least ${\bf
Z}[i,x]; i^2=-1$ to factor), we want to prove this.

Now consider what happens when we set $x$ equal to some specific
integer value, say $a$.  Any polynomial in ${\bf Z}[x]$ will be
transformed into an integer.  Thus, we have a mapping $\phi_{x-a}:
{\bf Z}[x] \rightarrow {\bf Z}$ from polynomials to integers.  Not
only is this a mapping, but it is a {\it homomorphism}, a mapping that
preserves the operations, so $\phi_{x-a} (m+n) = \phi_{x-a}m \,\hat+\,
\phi_{x-a}n$ and $\phi_{x-a} (m\cdot n) = \phi_{x-a}m \,\hat\cdot\,
\phi_{x-a}n$, where I have used $\hat+$ and $\hat\cdot$ to emphasize
that these operations are operations in ${\bf Z}$, and distinct from
$+$ and $\cdot$, which are operations in ${\bf Z}[x]$.\footnote{The
symbols $\cdot$ and $\hat\cdot$ represent multiplication, which we normally
omit entirely, but I have written explicitly here to make this point.}
I leave it an exercise to actually prove this is a homomorphism.

Since $\phi_{x-a}$ is a homomorphism, any factorization of a
polynomial in ${\bf Z}[x]$ must map into a factorization of its image
integer in {\bf Z}.  In other words, if a polynomial factors into
smaller polynomials (all with integer coefficients), then setting the
variable equal to some specific integer causes all the polynomials to
evaluate into integers, which must themselves factor.  Consider
$x^2-1=(x+1)(x-1)$.  If we set $x=2$ (the evaluation homomorphism
$\phi_{x-2}$), then the equation becomes $3=3\cdot1$.  This happens
irregardless of our choice of integer.  Choosing $x=3$ ($\phi_{x-3}$)
transforms $x^2-1=(x+1)(x-1)$ into $8=4\cdot2$.

Thus, we have our homomorphism, which maps our problem from ${\bf
Z}[x]$ into ${\bf Z}$ and transforms the factorization of polynomials
into a factorization of integers.  Although factoring integers is
certainly not trivial (the security of the RSA cryptosystem depends on
its near impossibility for large numbers), it is much easier than
factoring polynomials.  Not only easier, but {\it finite}.  There are
only a certain number of ways any given integer can factor, and for
relatively small integers, we can enumerate them by computing a prime
factorization and then listing the finite number of ways that the
primes can be combined into factors.  The number $3$, for example, can
be split into two integer factors in only one of four ways: $3\cdot1$,
$1\cdot3$, $-3\cdot-1$, and $-1\cdot-3$.

\begin{comment}

\mysection{Axiom Soup}

At this point, the reader might begin to suspect that we build up a
theory from our axioms, and whenever we get stuck, we introduce a new
axiom so that we can move forward!  In a sense, this is true, but
don't miss an important point.  While the axioms are axioms in the
sense that you can't prove U1 or I1 for an arbitrary ring (i.e, just
given the R axioms), they are also theorems in the sense that we can
prove them for the particular systems of interest to us.  We prove the
axioms both for our base system ${\bf Z}$ (the integers form a unique
factorization domain), and for any constructed system ${\cal F}[x]$ (a
polynomial ring in a single {\it transcendental} variable over a field
is also a unique factorization domain).

Let me close this chapter by proving this in a series of theorems.

\end{comment}

\vfill\eject
\mysection{Exercises}

Factor the following polynomials in ${\bf Z}[x]$, ${\bf Q}[x]$, ${\bf R}[x]$, and ${\bf C}[x]$:

\begin{enumerate}
\item $x^2-2$  (factors in ${\bf R}[x]$ and ${\bf C}[x]$, but not in ${\bf Z}[x]$ or ${\bf Q}[x]$)
\item $x^2+1$  (factors in ${\bf C}[x]$, but not in any other ring)
\end{enumerate}

Write a computer program to factor the following polynomials:

\begin{enumerate}[resume]
\item $x^5+2\,x^4+2\,x^3-x-1$

\lstinline!expand((x^2+x+1)*(x^3+x^2-1));!

\item $34\,x^5+51\,x^4+60\,x^3+25\,x^2+8\,x-1$

\lstinline!factor(34*x^5+51*x^4+60*x^3+25*x^2+8*x-1);!

Ans: $\left(x^2+x+1\right)\,\left(34\,x^3+17\,x^2+9\,x-1\right)$


\end{enumerate}

Write a computer program to factor the following polynomials:\hfil\break
\-\hspace{1cm} (Hint: You'll need a 2 dimensional grid)

\begin{enumerate}[resume]

\begin{sympycode}
var('x, y')

def factorization_problem(p):
    print ('$')
    print (latex(p.expand()))
    print ('$ \n')
    print (p.expand())
    print ('\n')
    print ('Ans: $')
    print (latex(p.factor()))
    print ('$\n')

\end{sympycode}

\item \sympyc{factorization_problem((3*y+2*x-7)*(7*y+x**2+11))}

\item \sympyc{factorization_problem((y**3+2*x**2-3)*(y+x**2+1))}

\begin{maximacode}
x/(x+1);
\end{maximacode}

\end{enumerate}
