
\mychapter{Ring, Fields, and Polynomials}

In this chapter I will outline the basic algebraic structures
necessary to carry out the program sketched out in the previous
chapter.  This material is included mainly to provide a starting point
for the rest of the book. The pace of this chapter is
deliberately quick; I omit a lot of the more basic proofs and doubt
that this will substitute for a good introductory text on higher
algebra.\footnote{Need a good reference to such a text here.
I want to footnote all the missing proofs with references
back to two or three texts; maybe Lang, van der Waerden,
and an introductory undergraduate text of some kind.}

\mysection{Rings and Fields}
\qquad [van der Waerden], \S3.1

We begin with two key definitions that we will use throughout: the
{\it ring} and the {\it field}.  As I explained in the previous
chapter, both are associated primarily with sets of axioms.  Any
algebraic system that obeys the ring axioms is called a ring; any
algebraic system that obeys the field axioms is a field.

Both rings and fields are defined over sets with two binary operators,
conventionally called addition and multiplication.  It will appease
the nervous reader to know that for our purposes,
addition is addition and multiplication is multiplication --- the same
addition and multiplication we learned in grade school.  Of
course, in the general case, any pair of operations that obey the
axioms will suffice to form a ring or a field, but we won't need to
concern ourselves with this.

A {\it ring} ${\cal R}$ obeys the following axioms:\footnote{I freely
use the set-theoretic symbols $\forall$ (read: for all) and $\exists$
(read: there exists) to abbreviate this table.  Their ordering
is significant.  $\exists a, \forall b$ means ``there exists an
$a$ (independent of $b$) so that for all $b$\ldots'', while
$\forall a, \exists b$ means ``for all $a$, there exists a $b$
(possibly different for each $a$) so that\ldots''}

\begin{center}
\begin{supertabular}{l l l r}
   associative law of addition	& $\forall a,b,c \in {\cal R},$ & $(a+b)+c = a+(b+c)$ &(R1)\cr
   associative law of multiplication & $\forall a,b,c \in {\cal R},$ & $(ab)c = a(bc)$ &(R2)\cr
   commutative law of addition	& $\forall a,b \in {\cal R},$ & $a+b = b+a$ &(R3)\cr
   commutative law of multiplication & $\forall a,b \in {\cal R},$ & $ab = ba$ &(R4)\cr
   distributive law   & $\forall a,b,c \in {\cal R},$ & $a(b+c) = ab + bc$ &(R5)\cr
   existence of an additive identity (zero) & $\exists 0 \in {\cal R}, \forall a \in {\cal R}$, & $0 + a = a$ &(R6)\cr
   existence of a multiplicative identity (unity) & $\exists 1 \in {\cal R}, \forall a \in {\cal R},$ & $1a = a$ &(R7)\cr
   invertibility of addition & $\forall a \in {\cal R}, \exists b \in {\cal R}$, & $a + b = 0$ &(R8)\cr
\end{supertabular}
\end{center}

Some comments are in order.  Notice in particular that I listed the
commutative law of multiplication as (R4).  Most authors don't do
this, because a substantial theory has been developed around {\it
non-commutative} rings, probably because matrix multiplication (a
critically important example) is non-commutative.  A commutative ring
is said to be {\it abelian}, and most authors require a field to be
commutative; reserving the term {\it skew field} to refer to a
(possibly) non-commutative field.  Also, I required the existence of a
multiplicative identity as (R7).  Again, most authors don't require
rings to have a unity element, but since all of our rings will be
commutative, and all will have a unity element, I will omit any
additional terminology and adopt the R4 and R7 axioms to obtain a {\it
abelian (commutative) ring with unity} (and call it a ``ring'').

A {\it field} ${\cal F}$ obeys all the ring axioms (thus all fields are also
rings), as well as the following axiom:

\begin{center}
\begin{tabular}{l l l r}
   invertibility of multiplication & $\forall a \ne 0 \in {\cal F}, \exists b \in {\cal F},$ & $ab = 1$ &(F1)\cr
\end{tabular}
\end{center}

In plain English, rings are mathematical systems in which addition,
multiplication and subtraction are cleanly defined.  Subtraction is
defined due to (R8), the invertibility of addition.  Division,
however, is not, since it requires (F1).  Since the ring axioms do not
require multiplication to be invertible, there is no guarantee that we
can carry out division in a ring.  A field, on the other hand, is a
mathematical system in which all four elementary operations ---
addition, subtraction, multiplication, and division --- are defined.

THEOREM (Zero Theorem)

$\forall x\in{\cal R}, 0x=0$

PROOF

From the equation $0x=0x+0x+(-0x)=(0+0)x+(-0x)=0x+(-0x)=0$.

I used
first R8 (applied to $0x$) to obtain $0x+(-0x)=0$, then applied the
principle of equality to substitute this equation into $0x=0x+0$
(obtained from R6 and R3), then used R5 to factor out $(0+0)$ and R6
to replace this with $0$, then R8 again to conclude that $0x+(-0x)=0$.

Thus, multiplication by zero is a property of all rings.

END THEOREM

The simplest example of a ring is the set of integers, which I shall
denote as {\bf Z} (after the German word for number, {\it zahl}).  A pair
of integers can be added, subtracted or multiplied to form another
integer.  Note, however, that a pair of integers can not necessarily
be divided to form another integer.  $3 \over 2$ is not an integer,
because multiplication (in {\bf Z}) is not necessarily invertible ---
there is no integer that when multiplied by 2 forms 1.
(F1) is not satisfied.
Thus {\bf Z} forms a ring but not a field.

\mysection{Quotient fields}
\qquad [van der Waerden], \S3.3

We can remedy this by moving on the {\it rational numbers},
traditionally denoted {\bf Q} (probably for {\it quotient}).  This is the
simplest example of a {\it quotient field}, in this case formed over
the integers, {\bf Z}.  It is also our first example of a theme
we'll use repeatedly in this book, that of using a simple
algebraic system to construct a more complex one.

To form a quotient field from a ring, we take pairs of elements from
the ring (conventionally arranged into fractions) and establish an
{\it equivalence relationship} between them.  We also require that the
second element in the pair (the denominator) can not be zero.  Two
pairs $(a,b)$ and $(c,d)$ are equivalent if $ad=bc$, and we write them
$a\over b$ and $c\over d$.  We group equivalent pairs together into
{\it equivalence classes} and define our basic field operations as
follows:

\begin{center}
\begin{eqnarray*}
{a\over b} + {c\over d} &=& {{ad+bc}\over{bd}} \cr
{a\over b} - {c\over d} &=& {{ad-bc}\over{bd}} \cr
{a\over b} {c\over d} &=& {{ac}\over{bd}} \vbox to20pt{}\cr
{\,{a\over b}\, \over {c \over d}} &=& {{ad}\over{bc}} \vbox to24pt{}\cr
\end{eqnarray*}
\end{center}

The additive identity element is $0\over1$ and the multiplicative
identity is $1\over1$, using the original identities 0 and 1 from the
base ring.\footnote{Actually, a ring doesn't have to have a
multiplicative identity (check the axioms).  But all of
our rings will have 1, and it's not that difficult to extend
an arbitrary ring into a larger one that does.}
Note that the division by zero is not defined, nor do
our field axioms require it to be.

Notice that although we define all four field operations, we only
use the three ring operations to do it!  I.e, when we divide
$a\over b$ by $c\over d$, we need only to form $ad$ and $bc$
in order to form the $(ad,bc)$ pair, which we write as ${ad}\over{bc}$.
We thus divide $1\over2$ by $2\over3$ to obtain $3\over4$
without ever having to divide the {\it integers} ---
only multiplying them ($1\cdot3=3$ and $2\cdot2=4$).

In general, there is no guarantee that this kind of construction will
work.  We can't just pair numbers up however we want and call it a
field.  Several other conditions have to be met.  First of all, we
have to ensure that the equivalence relationship is well-defined.  If
$x=y$ (in the sense of equivalence) and $y=z$, then we must have
$x=z$, otherwise we can't even cleanly establish the critical notion
of an {\it equivalence class} (which says that $1\over2$ and $2\over4$
are basically the same thing).  I emphasize here that the new field,
and its new operations, are defined using the equivalence classes,
although we muddle this distinction by using the smallest fraction in
a class to represent it.  Strictly speaking, the multiplicative
identity is not ${1\over1}$ but
$\{{1\over1},{2\over2},{3\over3},\ldots\}$, the additive identify is
not ${0\over1}$ but $\{{0\over1},{0\over2},{0\over3},\ldots\}$, and my
example in the last paragraph should have read ``we thus divide
$\{{1\over2},{2\over4},{3\over6},\ldots\}$ by
$\{{2\over3},{4\over6},{6\over9},\ldots\}$ to obtain
$\{{3\over4},{6\over8},{9\over12},\ldots\}$...''

Which leads to the next point.  Having cleanly established equivalence
classes, we have to make sure that our operations actually work
consistently on them, since they are defined in terms of fractions
within the classes.  We need to verify that taking any fraction from
one equivalence class and any fraction from other, then applying one
of four operations to them, we always get an answer in a third
equivalence class.  The actual answer can (and will) vary depending on
the choice of representative fractions, but it has to always be in the
same class.  In this way, we confirm that the operations are cleanly
defined not just on the fractions, but on the classes.  I'm not going
to actually make this verification, but leave it as an exercise.

Which is why I excluded zero as a possible denominator.  We do this
because otherwise our operations aren't cleanly defined on these
equivalence classes. ${1\over0}$ is not equivalent to ${0\over1}$
(since $1\cdot1\ne0\cdot0$), so ${1\over0}$ must have a multiplicative
inverse (by axiom F1); i.e, some fraction ${a\over b}$ must exist
which when multiplied by ${1\over0}$ produces ${1\over1}$, yet by the
zero theorem, no such element $b$ can exist in the base ring so that
$1b=0$.  Excluding zero as a possible denominator ensures that our
field axioms are satisfied.

Yet in the quotient
field operations, where we multiply two denominators together to get
the result's denominator, what would happen if two non-zero elements can be
multiplied to form zero, producing a zero denominator?  Nothing
in the ring axioms prevents this from happening, so we add an
additional axiom.

An {\it integral domain} ${\cal I}$ obeys all the ring axioms,
as well as:

\begin{center}
\begin{tabular}{l l l r}
   non-existence of zero divisors & $\forall a\ne 0,b\ne 0 \in {\cal I},$ & $ab\ne 0$ &(I1)\cr
\end{tabular}
\end{center}

The quotient field construction is only defined on integral domains,
and I'll leave it as an exercise to show that ${\bf Z}$ is an integral
domain.  The main point of this section is to recognize that the
quotient field construction can be performed not only on the integers
${\bf Z}$ to obtain the rationals ${\bf Q}$, but on any integral
domain to obtain its quotient field.


\mysection{Polynomial rings and rational function fields}
\qquad [van der Waerden], \S3.4

Having built a field from a ring, can we build a ring from a field?
The answer is yes, and the most important such construction is a {\it
polynomial ring}, whose elements are polynomials in some variable with
coefficients in the underlying field, all but a finite number of which
must be zero.\footnote{If we relax the finiteness requirement and
allow infinite polynomials, we obtain a ring of {\it power series}
over the field, typically written ${\cal F}[[x]]$.  We will have
little use for power series in this book.}  We write this ring using
the underlying field, brackets and the variable, so ${\cal F}[x]$ is
the ring of polynomials in $x$ with coefficients from the field ${\cal
F}$.

${\cal F}[x]$ is a ring but not a field.  It is, however, an
integral domain (left as an exercise), so we can form a quotient field
from it, which we write using parenthesis instead of brackets: ${\cal
F}(x)$.  Elements in ${\cal F}(x)$ are fractions, both the numerator
and denominator of which are polynomials in $x$.  So, for example,
${x}\over{x-1}$ is a element of ${\bf Q}(x)$.  Fractions of
polynomials are called {\it rational functions}, so ${\bf Q}(x)$ is
the {\it field of rational functions in $x$ over the rational numbers}.
Got it?

Now, you might ask, ``Can't $(x-1)$ be zero?  Say, if $x$ is 1?''.
The answer is {\it no}.  $x$ is not 1 or any other number.  $x$ is
$x$, and in ${\bf Q}[x]$, $(x-1)$ is as different from zero as $3
\over 2$ is in {\bf Q}.  $(x-1)$, and things like it, are {\it
completely distinct elements} in the algebraic systems in which they
are defined.

Now, obviously, we can set $x$ to be 1.  But now we are no longer
working in ${\bf Q}[x]$ --- for starters, there is no longer a
distinct element $x$!  Now we are working in ${\bf Q}$.  Setting $x$
equal to 1 mapped everything from ${\bf Q}[x]$ into ${\bf Q}$.  This
is a simple example of an {\it evaluation homomorphism} --- a
homomorphism (a mapping which preserves operations) from one system to
another created by setting an independent variable equal to some
constant value.

So, you ask, ``what about $1\over2$ and $2\over4$?  Are they distinct
elements as well?''  {\it No}.  This time we are dealing with elements
that are basically the same. This is where the technical details of
the quotient field construction become significant.  Strictly
speaking, we are not working with elements like $1\over2$ at all.  We
are working with the {\it equivalence classes} defined above.
$1\over2$ is a {\it representative} of an equivalance class that
includes $2\over4$.

If all this seems a bit arbitrary, well, it is.  I could easily pick
two numbers from {\bf Z} and pair them into equivalence classes in
some other way than for {\bf Q}; if my basic axioms were satisified I
would even have a field!  Whether it would be useful for something
other than puzzling the few readers who would bother is a different
story.  Let me briefly cite a few other examples of {\it useful}
equivalence classes.  Take pairs of elements from a field $f$ and $g$
in the expression $f\,dg$ and form equivalence classes based on
whether the expression can be transformed to $f'\,dg'$; this is one
way to introduce differentials into an algebraic context.  Take points
in a Cartesian geometry $(x_1, x_2,\ldots x_n)$ and group them
together if they are related by a simple constant multiple $(\lambda
x_1, \lambda x_2,\ldots \lambda x_n)$; you now have lines through the
origin and the basis for projective geometry.  Take infinite
convergent sequences of rational numbers (from {\bf Q}) and group them
together if the differences between them converge to zero; the
equivalence classes are the real numbers.  I could keep going.  These
constructions are easy to form; their utility lies in our ability to
relate them to real world problems.

NEED TO INTRODUCE {\bf R} and {\bf C}.

\mysection{Long Division}
\qquad [van der Waerden], \S3.4

As we all learned in grade school, polynomials can be divided
using long division.  To generalize this in our more abstract
context, let's consider a very simple calculation of this type:

\input{02-ALGEBRA-EXAMPLE2.inc}

Each step starts by dividing the leading terms, i.e, $x^2$ is divided
by $2x$ to form $x\over 2$.  Actually, we can be a bit more precise.
Each step starts by dividing the leading {\it coefficients},
since the variables are divided just by subtracting their
powers. $x^2$ divided by $x$ is just $x$.  We divide $1$ by $2$
to form $1\over2$ and in this manner obtain $x\cdot{1\over2}={x\over2}$.

Next, we multiply this value by the divisor to obtain a polynomial
that we will subtract from the dividend (or what remains of it after
prior steps).  Again, let's be more precise.  We multiply the
polynomial variable just by adding its powers.  What we really have to
{\it multiply} are the {\it coefficients}.  To multiple $x\over2$ by
$2x+1$ we multiply $1\over2$ by $2$ to obtain $1$, add the powers of
$x$ and $x$ to obtain $x^2$, and arrive at the first term $1\cdot
x^2=x^2$.  Next, we multiply $1\over2$ by $1$, get $1\over2$, add the
powers of $1$ and $x$ to obtain $x$, and have the second term
${1\over2}\cdot x={1\over2}x$.  Adding these terms we get
$x^2+{1\over2}x$ --- the first of the intermediate polynomials.

To perform the third step, we don't have to do anything with
the variables.  We just subtract the coefficients.  These
three steps are repeated until we are left with a remainder
of lower degree than the divisor.

So, to summarize, working with the polynomial variable is easy --- we
just add or subtract its integer powers.  We perform polynomial long
division by dividing, multiplying, and subtracting the {\it
coefficients}.  Now, these are three of the basic four operations
provided by a field.  It follows, therefore, that we can perform
polynomial long division on polynomials whose coefficients lie in any
field whatsoever.  Given ${\cal F}[x]$, a polynomial ring over a
field, we can use the field operations provided by ${\cal F}$ to
divide any two elements from ${\cal F}[x]$ using polynomial long
division and obtain a remainder and a quotient.

We can even say a bit more.  Just like with grade school long
division, we know that the degree of the quotient will be the
difference in degrees of the dividend and the divisor, and that the
degree of the remainder will be less than the degree of the divisor.
We just need to keep in mind that these degrees are measured relative
to the polynomial ring variable, not any other variable that might
appear as part of the underlying field.

\mysection{GCD Computation}
\qquad [van der Waerden], \S3.7, \S3.8, \S5.4 (multivariate rings)\hfil\break
\hbox{}\qquad [Geddes], Ch. 7

One of the most important uses of polynomial long division is to
compute greatest common divisors (GCDs), at least in theory.  In
practice, there are other, more efficient algorithms.\footnote{See
[Geddes], for example} However, because long division is a simple and
straightforward way to compute GCDs, because it provides a theoretical
underpinning for other methods, and because it leads us directly to
solving polynomial diophantine equations, I'll present it here in this
section.

The first thing to observe is that the long division equation, $D = qd
+ r$ (dividend equals quotient times divisor plus remainder), can be
rearranged to read $r = D - qd$, which shows that any common divisor
of the dividend and the divisor can be divided out from the right hand
side of the equation, so must divide the left hand side also.  Thus,
common divisors of the dividend and divisor are preserved in the
remainder.

Furthermore, since the remainder is always of lower degree than the
divisor, we can repeat the long division with the divisor as the new
dividend and the remainder as the new divisor.  The new remainder will
also preserve common divisors of the original dividend and divisor,
and will be of lower degree than the original remainder.  This process
can repeated, lowering the degree of the remainder at each step, until
we are left with a zero remainder, i.e. $D' = q' d'$, where I've used
primes to emphasize that we are no longer dealing with the original
dividend and divisor.  Since common divisors have been preserved
throughout by $D'$ and $d'$, it follows that $d'$, the divisor of the
last step, must be a common divisor.  It is, in fact, a greatest
common divisor, if GCDs exist in this ring.  This has been known since
the time of Euclid, at least in the case of integers.

I did say ``if'' GCDs exist, because nothing in our axioms guarantee
their existance.  The problem is that there might be a lattice of
divisors for a given element, instead of a strict ordering of them.
We'll remedy this, again, by introducing a new axiom.

A {\it unique factorization domain} ${\cal U}$ obeys all the ring axioms,
as well as:

\begin{center}
\begin{tabular}{l l l r}
   unique factorization & $\forall a,b,c,d \in {\cal U},ab=cd,$ & $\exists x\in {\cal U}, ax=c$ or $ax=d$ or $bx=c$ or $bx=d$ &(U1)\cr
\end{tabular}
\end{center}

U1 implies I1.  Take a unique factorization domain, and pick two
elements $c$ and $d$ which are multiples of zero, $cd=0$.  Obviously,
we can pick $a=0$ and $b=0$ and have $ab=0=cd$.  So, by U1, $x$ exists
so that $0x=c$ or $0x=d$, which by the zero theorem implies that
either $c=0$ or $d=0$.  This proves I1.  Thus, a unique factorization
domain is also an integral domain.  Also, ${\cal F}[x]$ is a unique
factorization domain (proof omitted).

U1 also implies the existence of GCDs.  Consider an element $x$ with
two different factors, say $a$ and $c$, so $x=ab$ and $x=cd$.  U1
immediately implies that either $a$ is a factor of $c$, if $aa'=c$,
or that $c$ is a factor of $a$, if $aa'=d$ and $x=caa'=ab$ and $ca'=b$.
FIX THIS.

Not all rings satisfy U1.  Consider, for example, ${\bf Z}[i];
i^2=-1$, the Gaussian integers.  This ring differs from the polynomial
ring ${\bf Z}[x]$ because polynomials of degree two and higher don't
exist since the square of $i$ is -1; $i$ is thus {\it algebraic} (see
below) and this makes all the difference.  The number 9 can be
factored two different ways in this ring: $9=3\cdot3=(4-i)(4+i)$.
It's not too hard to see that 3 can't be multiplied by any Gaussian
integer to form either $(4-i)$ or $(4+i)$, so U1 is not satisfied.
The Gaussian integers form an integral domain, but not a unique
factorization domain.

I did say a greatest common divisor, not the greatest common divisor,
because there can be more than one.\footnote{Actually, I haven't even
proved that GCDs exist at all, and in some algebraic systems, they
don't!}  A {\it unit} is an invertable element.  Put another way, $u$
is a unit if there exists $u'$ such that $uu'=1$.  Now, any divisor
can be transformed into another divisor by multiplying it by a unit,
since if $uu'=1$, then $ab=(ua)(u'b)$ for any $a$ and $b$ whatsoever.
In particular, a greatest common divisor can be transformed into
another greatest common divisor by multiplying it by a unit.  I leave
without proof the claims that in ${\cal F}[x]$, the units are all
elements in ${\cal F}$, and that all GCDs differ from each other by a
unit multiple.

A few words are in order here about GCDs in systems of the form ${\cal
U}[x]$, i.e, where the coefficients come from a unique factorization
domain that is not a field.  A factorization in ${\bf Q}(x)[y]$ or
${\bf Q}(y)[x]$ (both of the form ${\cal F}[x]$) is superficially so
similar to a factorization in ${\bf Q}[x,y]$ ({\it not} of the form
${\cal F}[x]$), that the distinction should be noted.  In both of the
first two cases, we form a quotient field with respect to one of the
two variables and thus obtain a polynomial ring (in the other
variable) over the quotient field.  In the case of ${\bf Q}[x,y]$ we
do not form a quotient field with respect to either variable; thus we
have a polynomial ring over not a field, but over another polynomial ring.

Now a polynomial ring over a unique factorization domain ${\cal U}[x]$
itself satisfies U1 (proof omitted), so by induction any finite series
of such polynomial rings over a unique factorization domain (like
${\bf Q}[x,y]$) is also a unique factorization domain.  This implies
that GCDs exist in ${\cal U}[x]$-type systems.  The problem is finding
them, since long division only works cleanly in an ${\cal F}[x]$-type
system.

The solution, invented by Gauss\footnote{check this}, is to first
factor out of each polynomial the GCD of the coefficients (calculated
in ${\cal U}$) which we call the {\it content} of the polynomial,
leaving a {\it primitive polynomial}.  It can be shown\footnote{van
der Waerden} that if a primitive polynomial factors at all, then it
factors into primitive polynomials.  We thus can compute a primitive
GCD of the primitive parts and multiply this by the GCD of the
contents to obtain a GCD in ${\cal U}[x]$.  A LITTLE UNCLEAR.

We will have little use for ${\cal U}[x]$ factorizations in this book,
since invariably we will calculate GCDs with respect to one variable,
and form quotient fields from any others, and thus always be working
in ${\cal F}[x]$ systems.  I mention this mainly to avoid confusion
between factoring in ${\cal F}[x,y]$ and ${\cal F}(x)[y]$, and have
thus omitted the proofs of Gauss' method; see the references for
details.

\vfill\eject

EXAMPLE

Compute a GCD of $4x^4+13x^3+15x^2+7x+1$ and $2x^3+x^2-4x-3$ in ${\bf Q}[x]$.

\bigskip
\input{02-ALGEBRA-EXAMPLE1.inc}

The divisor of the last step, in this case ${35\over2}x^2+35x+{35\over2}$,
is the GCD, or I should say a GCD, since multiplying by any unit
will produce a different GCD.  In the case of a polynomial ring over
a field, the units are the elements of the underlying field,
so we can multiply by anything in {\bf Q} (i.e, any rational number)
and get another GCD.  For this example, the obvious thing to multiply
by is $2\over35$, which both clears the denominators and divides out
the common factor in the numerators to produce $x^2+2x+1$.  Both
answers are acceptable.

\vfill\eject

EXAMPLE

Compute the GCD of $5xy-5y^2-7x+7y$ and $2x^2-yx-y^2$ in ${\bf Q}[x,y]$.

This is a ${\cal U}[x]$-type system, so we'll need to work in a
${\cal F}[x]$-type system to perform the computation.  Our choices
are ${\bf Q}(x)[y]$ and ${\bf Q}(y)[x]$.

Let's start with ${\bf Q}(x)[y]$, and rearrange the polynomials
so that $y$ is the polynomial variable:

$$-5y^2+(5x+7)y-7x {\rm\qquad and\qquad} -y^2-xy+2x^2$$

The first step is to compute the content (GCD of the coefficients) of
each polynomial.  Clearly, the GCD of $-5$, $(5x+7)$, and $-7x$ is 1
and the GCD of $-1$, $-x$, and $2x^2$ is also 1, so both polynomials
are already primitive and we can just proceed with the GCD calculation
in ${\bf Q}(x)[y]$:

\input{02-ALGEBRA-EXAMPLE5a.inc}

This leads us to conclude that the last divisor,
$-(2x+{7\over5})y+(2x^2+{7\over5}x)$ is a GCD in ${\bf Q}(x)[y]$.  Now
we need to remove its content, which is the GCD of $-(2x+{7\over5})$
and $(2x^2+{7\over5}x)$, or $(2x+{7\over5})$.  Dividing through by
this polynomial (a polynomial in ${\bf Q}[x]$, and thus a unit in
${\bf Q}(x)[y]$) we obtain $-y+x$.  We now multiply by the GCD of our
original contents, but they were just 1, so we conclude that $x-y$
is our GCD in ${\bf Q}[x,y]$.

Now let's do all that again in ${\bf Q}(y)[x]$.  Our polynomials become:

$$(5y-7)x-(5y^2-7y) {\rm\qquad and\qquad} 2x^2-yx-y^2$$

The second one has unit content (the GCD of $2$, $-y$, and $-y^2$),
but the first one's content is $\gcd_y(5y-7,5y^2-7y)=5y-7$.
Dividing this out, we obtain:

$$x-y {\rm\qquad and\qquad} 2x^2-yx-y^2$$

and compute:

\input{02-ALGEBRA-EXAMPLE5.inc}

Thus, $x-y$ is the GCD of the primitive polynomials, and it has unit
content $\gcd_y(1,-y)$.  The GCD of the original contents
($1$ and $5y-7$) is 1, so the final result is again $x-y$.

\vfill\eject

\mysection{Polynomial Diophantine Equations}

The same long division procedure used for GCD computations can also be
used solve a certain class of {\it polynomial Diophantine equations}.
A Diophantine equation is one whose variables are restricted to be
integers.  The most famous example is Fermat's equation,
$x^n+y^n=z^n$; Fermat's theorem states that this equation has no
solutions $x,y,z,n\in{\bf Z}$ for $n>2$.  A generalized Diophantine
equation is one whose variables are restricted to some algebraic
system, not necessarily ${\bf Z}$.  A polynomial Diophantine equation
is one whose variables are restricted to be polynomials of some form,
and the one we will consider here is this:

\begin{displaymath}
sa+tb=c; \qquad a,b,c\in{\cal F}[x] {\rm\, given}; \qquad
s,t\in{\cal F}[x] {\rm\, unknown}
\end{displaymath}

Thus, $xxx$ is an equation of this form.

Let's begin by noting that any common divisor of $a$ and $b$, and in
particular $\gcd(a,b)$, can be divided out from the left hand side of
the equation, and thus must also divide the right hand side, so $c$
must be be a multiple of $\gcd(a,b)$, or the equation has no solution.

This necessary condition is also sufficient, and the simplest way to
demonstrate this is to use the GCD computation in a constructive
proof.  Note that first step in computing $\gcd(a,b)$ is to solve
$a=qb+r$.  Rearranging this as $r=a-qb$ we see how the remainder can
be expressed in the Diophantine form $sa+tb$.  More generally, at each
step of the calculation, we solve $D=qd+r$, where $D$ and $d$ are each
either $a$, $b$, or a remainder from a previous step, so using
$r=D-qd$ we can write each remainder in the form $sa+tb$.  At the end
of the calculation, we will have expressed $\gcd(a,b)$ in the form
$sa+tb$.

We now use long division to divide $c$ by $\gcd(a,b)$.  Because of the
necessity demonstrated above, the division must be exact (i.e, zero
remainder) or the equation has no solution.  Having computed both
$\gcd(a,b)=sa+tb$ and $c=q\gcd(a,b)$ we can now combine these
expression to form $c=(qs)a+(qt)b$, which solves the original
equation.

This solution is not unique.  Given a solution to $c=sa+tb$, we can
form any multiple of $ab$, say $mab$, and write another solution
$c=(s-mb)a+(t+ma)b$.  Note however, that $(s-mb)$ has the form of a
remainder after dividing $s$ by $b$ ($m$ is the quotient).  Since the
degree of a remainder is always less than the degree of the divisor,
it follows that if $sa+tb=c$ can be solved, then we can always compute
an $s$ of lower degree than $b$, or a $t$ of lower degree than $a$.

If $\deg(c)<\deg(a)+\deg(b)$, then these conditions are not exclusive;
finding an $s$ of lower degree than $b$ implies a $t$ of lower degree
than $a$.  To see this, simply note that if $\deg(s)<\deg(b)$, then
$\deg(sa)=\deg(s)+\deg(a)<\deg(b)+\deg(a)$.  Since $tb=c-sa$, if
$\deg(c)<\deg(a)+\deg(b)$ and $\deg(sa)<\deg(a)+\deg(b)$, then
$\deg(tb)<\deg(a)+\deg(b)$, which implies $\deg(t)<\deg(a)$.

We will make repeated use of this polynomial Diophantine equation
throughout the book.

\vfill\eject

%EXAMPLE

Solve:

$$s(4x^4+13x^3+15x^2+7x+1) + t(2x^3+x^2-4x-3) = x^3 + 5x^2 + 7x +3$$

\quad for $s,t \in {\bf Q}[x]$ satisfying minimal degree bounds.

\bigskip
\input{02-ALGEBRA-EXAMPLE1.inc}

%These are the same polynomials used for the first GCD example.
%Using the notation

\begin{eqnarray*}
a &=& 4x^4+13x^3+15x^2+7x+1, \cr
b &=& 2x^3+x^2-4x-3, {\rm\,and} \cr
c &=& x^3 + 5x^2 + 7x +3 \cr
\end{eqnarray*}

%we are trying to solve $sa+tb=c$.  The first step in the GCD
%calculation yielded:

\begin{eqnarray*}
a &=& (2x+{11\over2})b + ({35\over2}x^2+35x+{35\over2}){\rm,\,or} \cr\cr
x^2+2x+1 &=& {2\over35}a - {1\over35}(4x+11)b \cr
\end{eqnarray*}

%Having concluded at the last step in the GCD calculation that
%$x^2+2x+1$ is a GCD of $a$ and $b$, we now divide it into $c$:

\input{02-ALGEBRA-EXAMPLE3.inc}

%The remainder is zero, so the problem has a solution.
%We substitute our expansion for $x^2+2x+1$ above into $c=(x+3)(x^2+2x+1)$
%and obtain:

\begin{eqnarray*}
c &=& {2\over35}(x+3)a - {1\over35}(x+3)(4x+11)b \cr\cr
  &=& {2\over35}(x+3)a - {1\over35}(4x^2+23x+33)b \cr
\end{eqnarray*}

$\deg({2\over35}(x+3)) = 1 < \deg(b) = 3$ and $\deg({1\over35}(4x^2+23x+33))
= 2 < \deg(a) = 4$, so the degree bounds are already met.

Now verify this solution using Maxima, a computer algebra system:

{\small\begin{verbatim}
(%i24) a: 4*x^4+13*x^3+15*x^2+7*x+1;

                           4       3       2
(%o24)                  4 x  + 13 x  + 15 x  + 7 x + 1
(%i25) b: 2*x^3+x^2-4*x-3;

                                 3    2
(%o25)                        2 x  + x  - 4 x - 3
(%i26) gcdex(a,b);

                                          2
                            4 x + 11  35 x  + 70 x + 35
(%o26)/R/             [1, - --------, -----------------]
                               2              2
(%i27) c: x^3+5*x^2+7*x+3;

                               3      2
(%o27)                        x  + 5 x  + 7 x + 3
(%i28) d: c/gcdex(a,b)[3];

                                    2 x + 6
(%o28)/R/                           -------
                                      35
(%i29) gcdex(a,b)[1]*d;

                                    2 x + 6
(%o29)/R/                           -------
                                      35
(%i30) gcdex(a,b)[2]*d;

                                   2
                                4 x  + 23 x + 33
(%o30)/R/                     - ----------------
                                       35
(%i31) gcdex(a,b)[1]*d*a + gcdex(a,b)[2]*d*b;

                               3      2
(%o31)/R/                     x  + 5 x  + 7 x + 3
\end{verbatim}}


\vfill\eject


\mysection{Square-free factorization}

\hbox{}\qquad [Geddes], \S 8.2

A {\it square-free polynomial} is one with no repeated factors.
$x^2-1$ is square-free because it factors as $(x-1)(x+1)$.  $x^2+2x+1$
is not square-free because it factors as $(x+1)^2$.

Whether or not a polynomial is square-free is independent of the field
in which the factorization occurs.

A {\it square-free factorization} of a polynomial is a factorization
into square-free factors, each of which appears at a different power.
It is much easy to compute than a full factorization into irreducible
factors and for this reason will be quite useful to us.

Surprisingly, a polynomial's square-free factorization is independent
of its algebraic system!  For example, $x^2+1$ is irreducible in ${\bf
R}[x]$, so its square-free factorization is simply $x^2+1$.  On the
other hand, in ${\bf C}[x]$, $x^2+1$ factors as $(x+i)(x-i)$.  Yet
both of these factors combine together in the square-free
factorization (since they both appear to the first power), so
$x^2+1$'s square-free factorization in ${\bf C}[x]$ is\ldots $x^2+1$.

To compute square-free factorizations, we'll use an operation that,
for lack of a better word, I'll call ``differentiation.''  We
``differentiate'' a polynomial by multiplying each term by its power
and then lowering the power by one.

This ``differentiation'' not to be confused with the field operation
that I will define in the next chapter.  ``Differentiation'' is simply
a mechanical procedure of lowering powers and multiplying by
constants.  In particular, no attempt is made to ``differentiate''
the coefficients {\it even if they are not constants}.

To compute a square-free factorization, first we ``differentiate'' the
polynomial.  The result is a second polynomial with the degree of all
factors reduced by one.  Note in particular that any factors of unit
degree (and only those factors) disappear completely.  Dividing this
into the original polynomial, we obtain a polynomial with no square
factors --- all factors now appear with unit degree.  Computing the
GCD of this polynomial with the original one also produces a
polynomial with only factors of unit degree, except that the original
unit degree factors are missing.  We can divide this last two
polynomials into each other to determine the original unit degree
factors.  Going back to the ``differentiation'' step, we can keep
repeating the process until we have obtained all the square-free
factors.

\vfill\eject

\mysection{Partial Fractions Expansion}
\qquad [van der Waerden], \S5.10

As a first application of polynomial Diophantine equations, we use
them to construct partial fractions expansions.  Consider an element
$a$ from a polynomial quotient field ${\cal F}(x)$.  We can write
$a={n\over d}$ where $n,d\in{\cal F}[x]$.  If we are now given a
factorization of $d=d_1^{e_1} d_2^{e_2} \cdots d_k^{e_k}$, where
$d_i\in {\cal F}[x]$ and $\gcd_{{\cal F}[x]}(d_i,d_j)=1$ if $i\ne j$,
and assuming that $a$ is a proper fraction
($\deg_{{\cal F}[x]}n < \deg_{{\cal F}[x]}d$),
then we can construct a {\it partial fractions expansion} of $a$:

\begin{displaymath}
a={n\over d}=\sum_{i=1}^{n}\sum_{j=1}^{e_i} {n_{ij}\over {d_i}^j}
\qquad \deg_{{\cal F}[x]}(n_{ij}) < \deg_{{\cal F}[x]}(d_i)
\end{displaymath}

We begin by computing an expansion in the form:

\begin{displaymath}
a={n\over d}=\sum_{i=1}^{n} {n_i\over {d_i}^{e_i}}
\qquad \deg_{{\cal F}[x]}(n_i) < e_i\deg_{{\cal F}[x]}(d_i)
\end{displaymath}

$n_1$ is found by solving the following polynomial Diophantine
equation for $n_1$ and $r_1$:

\begin{eqnarray*}
n &=& n_1 \Big( \prod_{j\ne 1} {d_j}^{e_j} \Big) + r_1 (d_1^{\,e_1}) \cr
\end{eqnarray*}

Our degree bounds guarantee that
$\deg(n_1) < e_1\deg(d_1)$, and dividing through by $d$ shows:

\begin{eqnarray*}
{n\over d} &=& {{n_1}\over{d_1^{\,e_1}}} + {r_1\over{\prod_{j\ne 1} {d_j}^{e_j}}} \cr
\end{eqnarray*}

The second term on the right is a fraction in the original form,
but with one less factor in the denominator, so we can recurse
and separate out all the ${d_i}^{e_i}$ into seperate fractions.
Simple long division now suffices to seperate these fractions:

\begin{eqnarray*}
n_i &=& q_{ij} d_i + r_{ij} \cr\cr
{{n_i}\over{d_i^j}} &=& {{q_{ij}}\over{d_i^{\,j-1}}} + {r_{ij}\over{d_i^{\,j}}} \cr
\end{eqnarray*}

The $r_{ij}$ are our original $n_{ij}$, and the degree bounds on long
division ensure that $\deg_{{\cal F}[x]}(n_{ij}) < \deg_{{\cal F}[x]}(d_i)$.
We recurse on the first term until we have completed the desired construction.

\vfill\eject

EXAMPLE

Use a square-free factorization of the denominator to compute the partial fractions expansion of $${x^2 + 3x + 2}\over{x^3-3x^2+4}$$

We begin by ``differentiating'' the denominator to obtain $3x^2-6x$.
Computing the GCD of $x^3-3x^2+4$ and $3x^2-6x$:

\input{02-ALGEBRA-EXAMPLE4c.inc}

Thus, $-2x+4$ is a GCD, which we normalize by dividing through by -2
to obtain $x-2$.  We could now proceed by dividing $x^3-3x^2+4$ by
$x-2$ to obtain $x^2-x-2$ (all factors at unit power), compute the GCD
of $x-2$ and $x^2-x-2$ to obtain $x-2$ (all higher factors at unit
power), divide $x^2-x-2$ by $x-2$ to obtain $x+1$ (the unit
square-free factor), and repeat the process (trivially) with $x-2$ to
decide that $x-2$ is the second square-free factor.  Or, we could
shortcut the entire process by noting that since $x-2$ is linear,
it can only be the second square-free factor.  In any event, we conclude that:


$${{x^2 + 3x + 2}\over{x^3-3x^2+4}} = {{x^2 + 3x + 2}\over{(x-2)^2(x+1)}}$$

Next, we solve the polynomial Diophantine equation:

$$x^2 + 3x + 2 = s(x-2)^2 + t(x+1) = sa+tb$$

Compute the GCD of $a=(x-2)^2=x^2-4x+4$ and $b=x+1$:

\input{02-ALGEBRA-EXAMPLE4.inc}

Since the remainder, 9, is a unit, $a$ and $b$ have no common
factors and their GCD is 1.  Of course, this result is hardly
surprising since the square-free factorization was guaranteed
to produce factors with no common factor between them.

$$a=(x-5)b+9$$
$$9=a-(x-5)b$$
$$1={1\over9}[a-(x-5)b]$$
$$x^2 + 3x + 2 = {1\over9}(x^2+3x+2)a-{1\over9}(x^2+3x+2)(x-5)b$$
$$x^2 + 3x + 2 = {1\over9}(x^2+3x+2)a-{1\over9}(x^3-2x^2-13x-10)b$$

Our degree bounds aren't met yet, so we divide $b=x+1$ into $x^2+3x+2$:

\input{02-ALGEBRA-EXAMPLE4a.inc}

The zero remainder means that the $a$ term drops away completely,
and after subtracting $(x+2)a=x^3-2x^2-4x+8$ from the $b$ coefficient,
we conclude that:

$$x^2 + 3x + 2 = -{1\over9}(-9x-18)b = (x+2)b$$

In other words (remember that $b=x+1$),

$${{x^2 + 3x + 2}\over{(x-2)^2(x+1)}} = {{(x+2)(x+1)}\over{(x-2)^2(x+1)}} = {{(x+2)}\over{(x-2)^2}}$$

We need only divide $(x+2)$ by $(x-2)$:

\input{02-ALGEBRA-EXAMPLE4b.inc}

% $$(x+2)=(x-2)+4$$

so,

$${{x^2 + 3x + 2}\over{x^3-3x^2+4}} = {4\over{(x-2)^2}} + {1\over{(x-2)}}$$

Again, verify this result using a computer algebra system.

\vfill\eject


\mysection{Resultants}
\qquad [van der Waerden], \S5.8

At times, we will want a simple way of testing two polynomials over a
field to see if they have a GCD, without actually computing it.  This
is more than just a computational convenience.  The presence of the
polynomial's variable in the GCD often encumbers us.  On the other
hand, the {\it resultant} yields a simple element from the underlying
field that is zero if the polynomials have a non-trivial GCD and
non-zero otherwise.

The resultant is defined as the determinant of the Sylvester matrix
$S_x(P,Q)$, which is the $m+n \times m+n$ matrix constructed from two
polynomials (in ${\cal F}[x]$) $P$ and $Q$ of degrees $m$ and $n$ (all
the blanks are zeros):

$$ P = \sum_{i=0}^m p_i \, x^i $$

$$ Q = \sum_{i=0}^n q_i \, x^i $$

$$ S_x(P,Q) = \pmatrix{
  p_m & p_{m-1} & \ldots & p_0 & & & \cr
  & p_m & p_{m-1} & \ldots & p_0 & & \cr
  & & \ldots & & \ldots & & \cr
  & & & p_m & p_{m-1} & \ldots & p_0 \cr
  \vdots & & & \vdots & & & \vdots \cr
  q_n & q_{n-1} & \ldots & q_0 & & & \cr
  & q_n & q_{n-1} & \ldots & q_0 & & \cr
  & & \ldots & & \ldots & & \cr
  & & & q_n & q_{n-1} & \ldots & q_0 \cr
  } $$

In plain English, the matrix is constructed by forming the first row
from the first polynomial coefficients, adding $n-1$ trailing zeros at
the end of the row.  The second row is formed by shifting the first
row one position to the right.  This shifting is repeated a total of
$m-1$ times to obtain the first $m$ rows.  The last $n$ rows are
constructed in the same way from the second polynomial.

Now consider the following straightforward matrix identity:

$$ S_x(P,Q) \pmatrix{x^{n+m-1}\cr x^{n+m-2}\cr \vdots \cr x \cr 1}
 = \pmatrix{P x^{m-1}\cr P x^{m-2}\cr \vdots \cr Q x \cr Q} $$

If $\det S_x(P,Q)$ is non-zero, then the Sylvester matrix is
invertible, and we can form the following equation:

$$ \pmatrix{x^{n+m-1}\cr x^{n+m-2}\cr \vdots \cr x \cr 1}
 = S_x(P,Q)^{-1} \pmatrix{P x^{m-1}\cr P x^{m-2}\cr \vdots \cr Q x \cr Q} $$

Since the matrix is formed exclusively from the polynomials'
underlying field ${\cal F}$, its inverse must also be formed from
${\cal F}$.  Now consider the bottom element in the last equation.  It
must have the following form:

$$ 1 = f_0 P x^{n+m-1} + f_1 P x^{n+m-2} + \ldots + f_{n+m-1} Q x + f_{n+m} Q $$
$$ 1 = A P + B Q \qquad A,B \in {\cal F}[x] $$

The only way this statement can be true is if $P$ and $Q$ have a
trivial GCD, so a non-zero determinant of $S_x(P,Q)$ imply that $P$ and
$Q$ have only a trivial GCD.

Conversely, assume that $\gcd(P,Q) = 1$.  Then we can solve a series
of polynomial Diophantine equations to express $1, x, \ldots,
x^{n+m-1}$ as $AP+BQ$, where $\deg A < \deg Q = n$ and $\deg B <
\deg P = m$, which suffices to construct an inverse of the Sylvester
matrix.

We have thus proved that the resultant is zero iff the two polynomials
have a non-trivial GCD.


\vfill\eject

\mysection{Algebraic Extensions}

Both our quotient field and polynomial ring constructions are examples
of {\it extensions}.  Simply put, when we use an algebraic system to
construct a new algebraic system that includes the original system as
a subset, then the new system is an {\it extension} of the
original.\footnote{The key property is that the one system is a subset
of the other, not the exact method of construction.}  So, ${\bf Z}[x]$
is an extension of ${\bf Z}$ because we can identify ${\bf Z}$ as a
subset of ${\bf Z}[x]$ and, in fact, even homomorphicly map ${\bf Z}$
into ${\bf Z}[x]$.  Such an {\it inclusion homomorphism} should not be
confused with an evaluation homomorphism, which would map the other
way (from ${\bf Z}[x]$ into ${\bf Z}$).

The only remaining type of extension that will be important to us is
the {\it algebraic extension}.  It is another equivalence class
construction that we build starting with a polynomial ring over a
field, say ${\cal F}[x]$.  Our equivalence classes are all elements in
${\cal F}[x]$ whose differences are multiples of some distinguished
irreducible polynomial in ${\cal F}[x]$, called the {\it minimum
polynomial} of the field.  And I do say {\it field}, because we don't
need to use the quotient field construction with an algebraic
extension; the ring (as we will see) is already a field.

MORE HERE

Again, like with the quotient field, I tend to be a bit loose with the
notation.  Something like the Gaussian integers, which I wrote as
${\bf Z}[i]; i^2=-1$, really should be expressed as equivalence
classes modulo the polynomial $i^2+1$, i.e.  ${\bf Z}[i]/(i^2+1)$.

\mysection{Simple Field Extensions}

A {\it simple} extension of a field ${\cal F}$ is formed by adjoining
a single new element $\theta$ to the field, and then forming all
possible sums, differences, multiples, and quotients to form a new
field.  Any such simple extension will be isomorphic to either the
quotient field ${\cal F}(\theta)$ (if $\theta$ is transcendental) or
an algebraic extension ${\cal F}[\theta]$ (if $\theta$ is algebraic).

\mysection{Finite Fields}

By using the same reasoning to consider Diophantine equations over the
integers, it's not that hard to see that $2s+6t=1$ has no solution for
$s,t\in{\bf Z}$, because 2, the GCD of 2 and 6, does not divide the
right hand side of this equation.  On the other hand, if $p$ is a
prime number, then $xs+pt=1$ can always be solved for $s,t\in{\bf Z}$,
so long as $0<x<p$, since $\gcd(x,p)=1$.

This leads directly to the observation that ${\bf Z}_n$, the ring of
integers modulo $n$, is a field if and only if $n$ is in fact a prime
(and we write it ${\bf Z}_p$).  In order to invert a number $x$ in
${\bf Z}_n$, we need a solution $x'\in{\bf Z}_n$ to $xx'\equiv\, 1\,
(\pmod n)$, or $xx' = 1 + nt$, or $xx' - nt = 1$, i.e, we need
to solve the integer Diophantine equation considered above.  If $n$ is
prime, we can always solve the equation; if $n$ is composite, then the
equation can't be solved if $x$ is a factor of $n$.  Thus, everything
in ${\bf Z}_n$ is invertible if and only if $n$ is prime.

In fact, we can say more.  If ${\cal F}$ is a {\it finite field} (a
field with a finite number of elements), then its {\it prime subfield}
(its smallest subfield) must be isomorphic to ${\bf Z}_p$,
for some prime $p$.

A field can have a finite prime subfield, even if the field itself is
infinite.  Consider ${\bf Z}_5(x)$, the field of rational functions in
$x$ with coefficients in ${\bf Z}_5$.  Clearly, we can build
polynomials in ${\bf Z}_5(x)$ with as a high a degree as we want, and
they are all unique, so ${\bf Z}_5(x)$ is infinite.  Yet it
should also be clear that ${\bf Z}_5(x)$'s prime subfield
is ${\bf Z}_5$, which is finite.

Such fields, which share some properties of both purely infinite and
finite fields, are called {\it fields of characteristic $p$}, where
$p$ is the order of the prime subfield.  Fields with infinite prime
subfields (isomorphic to {\bf Z}) are called {\it fields of
characteristic zero}).  WHAT ABOUT RINGS OF INFINITE CHARACTERISTIC?

DEFINE ISOMORPHISM.

\mysection{Symmetric Polynomials}
\qquad [van der Waerden], \S5.7

\mysection{Linear Algebra}
\qquad [van der Waerden], Ch. 4, \S6.11 (trace of an field extension)

Determinants; trace.

\mysection{Polynomial factorization (optional)}
\qquad [van der Waerden], \S5.6
\hbox{}\qquad [Geddes], Chs. 5, 6, 8

Let's conclude this chapter by taking a least a brief look at fully
factoring a polynomial into its irreducible factors.  There are
several reasons to do this.

First of all, it's fine for a theory text like this one to declare,
``the Fundamental Theorem of Algebra tells us that any polynomial in
{\bf C}[x] can be factored into linear factors, so let's assume we
have such a factorization...''  That's a true statement, but when it
comes time to actually do a computation, how do we proceed?  Call it
the price of success.  Differential algebra is solid enough to
actually compute integrals, so existance theorems don't cut it.  We
need constructive algorithms.

Second, it's a surprisingly difficult problem.  An appreciation of its
difficulty now will motivate the discussion later when I show various
techniques that have been developed to avoid full factorization
whenever possible.  Yet the fact remains that it is at times
unavoidable.

Finally, both techniques that I will discuss here work according to a
basic principle that we'll use again later in a more advanced context,
so it makes sense to present it now in a simpler form.  Specifically,
we'll solve a difficult problem in an algebraic system by using a
homomorphism to map into a different algebraic system where we can
solve the problem more easily, then find some way of ``lifting'' this
answer back into the original system.  This is one of the most
powerful solution methods in algebra, and has been used to solve
problems once thought impossible.

Let's start simple.  We want to factor a polynomial in ${\bf Z}[x]$,
the ring of polynomials with integer coefficients.  If such a
polynomial has a factorization in ${\bf Z}[x]$, for example
$x^2-1=(x+1)(x-1)$, we want to find it.  If it has no such
factorization, for example $x^2+1$ (which would require at least ${\bf
Z}[i,x]; i^2=-1$ to factor), we want to prove this.

Now consider what happens when we set $x$ equal to some specific
integer value, say $a$.  Any polynomial in ${\bf Z}[x]$ will be
transformed into an integer.  Thus, we have a mapping $\phi_{x-a}:
{\bf Z}[x] \rightarrow {\bf Z}$ from polynomials to integers.  Not
only is this a mapping, but it is a {\it homomorphism}, a mapping that
preserves the operations, so $\phi_{x-a} (m+n) = \phi_{x-a}m \,\hat+\,
\phi_{x-a}n$ and $\phi_{x-a} (m\cdot n) = \phi_{x-a}m \,\hat\cdot\,
\phi_{x-a}n$, where I have used $\hat+$ and $\hat\cdot$ to emphasize
that these operations are operations in ${\bf Z}$, and distinct from
$+$ and $\cdot$, which are operations in ${\bf Z}[x]$.\footnote{The
symbols $\cdot$ and $\hat\cdot$ represent multiplication, which we normally
omit entirely, but I have written explicitly here to make this point.}
I leave it an exercise to actually prove this is a homomorphism.

Since $\phi_{x-a}$ is a homomorphism, any factorization of a
polynomial in ${\bf Z}[x]$ must map into a factorization of its image
integer in {\bf Z}.  In other words, if a polynomial factors into
smaller polynomials (all with integer coefficients), then setting the
variable equal to some specific integer causes all the polynomials to
evaluate into integers, which must themselves factor.  Consider
$x^2-1=(x+1)(x-1)$.  If we set $x=2$ (the evaluation homomorphism
$\phi_{x-2}$), then the equation becomes $3=3\cdot1$.  This happens
irregardless of our choice of integer.  Choosing $x=3$ ($\phi_{x-3}$)
transforms $x^2-1=(x+1)(x-1)$ into $8=4\cdot2$.

Thus, we have our homomorphism, which maps our problem from ${\bf
Z}[x]$ into ${\bf Z}$ and transforms the factorization of polynomials
into a factorization of integers.  Although factoring integers is
certainly not trivial (the security of the RSA cryptosystem depends on
its near impossibility for large numbers), it is much easier than
factoring polynomials.  Not only easier, but {\it finite}.  There are
only a certain number of ways any given integer can factor, and for
relatively small integers, we can enumerate them by computing a prime
factorization and then listing the finite number of ways that the
primes can be combined into factors.  The number $3$, for example, can
be split into two integer factors in only one of four ways: $3\cdot1$,
$1\cdot3$, $-3\cdot-1$, and $-1\cdot-3$.

\mysection{Axiom Soup}

At this point, the reader might begin to suspect that we build up a
theory from our axioms, and whenever we get stuck, we introduce a new
axiom so that we can move forward!  In a sense, this is true, but
don't miss an important point.  While the axioms are axioms in the
sense that you can't prove U1 or I1 for an arbitrary ring (i.e, just
given the R axioms), they are also theorems in the sense that we can
prove them for the particular systems of interest to us.  We prove the
axioms both for our base system ${\bf Z}$ (the integers form a unique
factorization domain), and for any constructed system ${\cal F}[x]$ (a
polynomial ring in a single {\it transcendental} variable over a field
is also a unique factorization domain).

Let me close this chapter by proving this in a series of theorems.
