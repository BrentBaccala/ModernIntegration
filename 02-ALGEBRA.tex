
\mychapter{Commutative Algebra}

\begin{comment}
The author of a mathematics text, really any text in a technical
subject, is faced with a difficult choice --- how much to leave out?
How often to refer readers to references?  How much knowledge
to presuppose?

Now, in the early twenty-first century, we are faced with
technological changes that affect the author's approach.  First, we
have the ability to distribute very large books, file size being a
slight or absent impediment.  Also, we have a political and economic
structure that relies heavily on using copyright to restrict and
control information flow.  These factor have influenced me to decide
in favor of writing a larger and more comprehensive text, that aims to
introduce abstract algebra as much it tries to teach integration
theory.
\end{comment}

In this chapter I will outline the basic algebraic structures
necessary to carry out the program sketched out in the previous
chapter.  This material is included mainly to provide a starting point
for the rest of the book.  Where I have omitted proofs, I have tried
to provide references to [Go14], which is a good introductory algebra
textbook that is freely available on-line.

\mysection{Rings and Fields}

\qquad [van der Waerden], \S3.1
\\
\qquad [Go14], \S1.11

We begin with two key definitions that we will use throughout: the
{\it ring} and the {\it field}.

\begin{comment}
Both rings and fields are defined over sets with two binary operators,
conventionally called addition and multiplication.  It will appease
the nervous reader to know that for our purposes,
addition is addition and multiplication is multiplication --- the same
addition and multiplication we learned in grade school.  Of
course, in the general case, any pair of operations that obey the
axioms will suffice to form a ring or a field, but we won't need to
concern ourselves with this.
\end{comment}

\begin{key point}
A {\it ring} is a mathematical system where addition,
subtraction, and multiplication are defined.
\end{key point}

\begin{key point}
A {\it field} is a mathematical system where addition,
subtraction, multiplication, and division are defined.
\end{key point}

Both concepts are associated with sets of axioms.  Any algebraic
system that obeys the ring axioms is called a ring; any algebraic
system that obeys the field axioms is called a field.

\begin{comment}
I'm going to use basic set-theoretic notation to define the axioms.
Read $\forall$ as ``for all'' and $\exists$ as ``there exists''.  Each
symbol is immediately followed by the new variable or a list of new
variables that it qualifies.  Their ordering is significant.  The rule
is that each variable is assigned in left-to-right order.  So,
``$\exists a, \forall b$'' means ``there exists an $a$ (independent of
$b$, because $b$ hasn't appeared yet) so that for all $b$\ldots'',
while ``$\forall b, \exists a$'' means ``for all $b$, there exists an
$a$ (possibly different for each $b$) so that\ldots''.  In the later
case, $a$ can be a function of $b$, but not in the first case.
Sometimes I'll add the set inclusion symbol $\in$, read ``in'', such
as ``$\forall a,b,c \in {\cal R}$...'', reading ``for all a, b, and c
that are members of ${\cal R}$...'', but I'll omit this from these
tables for simplicity, since everything is a member of ${\cal R}$.
\end{comment}

\begin{figure}
\label{ring axioms}
\begin{mdframed}[backgroundcolor=cyan!20]
\begin{center}
\begin{supertabular}{l l @{ } l r}
   associative law of addition	& $\forall a,b,c,$ & $(a+b)+c = a+(b+c)$ &(R1)\cr
   associative law of multiplication & $\forall a,b,c,$ & $(ab)c = a(bc)$ &(R2)\cr
   commutative law of addition	& $\forall a,b,$ & $a+b = b+a$ &(R3)\cr
   distributive law   & $\forall a,b,c,$ & $a(b+c) = ab + bc$ &(R4)\cr
   existence of an additive identity (zero) & $\exists 0, \forall a$, & $0 + a = a$ &(R5)\cr
   invertibility of addition & $\forall a, \exists b$, & $a + b = 0$ &(R6)\cr
   & & &\cr
   commutative law of multiplication & $\forall a,b,$ & $ab = ba$ &(CR1)\cr
   existence of a multiplicative identity (unity) & $\exists 1, \forall a,$ & $1a = a$ &(RwU1)\cr
\end{supertabular}
\end{center}
\end{mdframed}
%\caption{Commutative ring with unity axioms}
\caption{Ring axioms}
\end{figure}

A {\it ring} ${\cal R}$ obeys the axioms
in figure \ref{ring axioms}.

Notice the commutative law of multiplication (CR1), along with the
existence of a multiplicative identity (RwU1).  A substantial theory
has been developed around {\it non-commutative} rings, probably
because matrix multiplication (a critically important example) is
non-commutative.  Most of our rings are commutative ring, or {\it
abelian} (the terms are synonymous).  Also, I require the existence of
a multiplicative identity.  Much of ring theory can be developed
without this axiom, but some theorems require it, and I don't want to
belabor the point, since all of our rings will have a unity element.
Therefore, I will omit any additional terminology, adopt the (CR1) and
(RwU1) axioms along with ring axioms (R1) through (R6) to obtain a
{\it commutative ring with unity}, and call it a ring for the rest of
the book.

\begin{figure}
\label{field axioms}
\begin{mdframed}[backgroundcolor=cyan!20]
\begin{center}
All ring axioms, plus:

\begin{justify}
\begin{tabular}{l l r}
   invertibility of multiplication & $\forall a \ne 0, \exists b, ab = 1$ &(F1)\cr
\end{tabular}
\end{justify}
\end{center}
\end{mdframed}
\caption{Field axioms}
\end{figure}

A {\it field} ${\cal F}$ obeys all the ring axioms (thus all fields are also
rings), as well as one additional axiom (Figure \ref{field axioms}).

Informally, rings are mathematical systems in which addition and
multiplication are cleanly defined.  Subtraction is also defined due
to (R6), the invertibility of addition, which allows a subtraction
problem to be turned into an addition problem.  Division, however, is
not, since it requires (F1).  Since the ring axioms do not require
multiplication to be invertible, there is no guarantee that we can
carry out division in a ring.  A field, on the other hand, is a
mathematical system in which all four elementary operations ---
addition, subtraction, multiplication, and division --- are defined.

\begin{comment}

\theorem (The Zero Theorem)

Given a ring ${\cal R}$, $\forall x\in{\cal R}, 0x=0$

\proof

From the equation $0x=0x+0x+(-0x)=(0+0)x+(-0x)=0x+(-0x)=0$.

\begin{tabular}{r c l l @{\vbox to20pt{}}}

$0x+(-0x)$ &=& $0$ & R6 applied to $0x$ \cr
$0+0x$ &=& $0x$ & R5 applied to $0x$ \cr
$0+0x + (-0x)$ &=& $0$ & principle of equality \cr

   & & &\cr

$(0+0x)+(-0x)$ &=& $0$ & principle of equality\cr
$(0\cdot 1+0x)+(-0x)$ &=& $0$ & RwU1\cr
$0(\cdot 1+x)+(-0x)$ &=& $0$ & R4\cr

\end{tabular}

$=(0+0)x+(-0x)=0x+(-0x)=0$.

applied
into , obtained from R5 applied to $0x$, then used R5 to
factor out $(0+0)$ and R6 to replace this with $0$, then R6 again to
conclude that $0x+(-0x)=0$.

Thus, multiplication by zero is a property of all rings.

\endtheorem

\end{comment}

The simplest example of a ring is the set of integers, which I shall
denote as {\bf Z} (after the German word for number, {\it zahl}).  A pair
of integers can be added, subtracted or multiplied to form another
integer.  Note, however, that a pair of integers can not necessarily
be divided to form another integer.  $3 \over 2$ is not an integer,
because multiplication (in {\bf Z}) is not necessarily invertible ---
there is no integer that when multiplied by 2 forms 1.
(F1) is not satisfied.
Thus {\bf Z} forms a ring but not a field.

\mysection{Fraction fields}
\subsection*{\qquad The field ${\bf Q}$}
\qquad [van der Waerden], \S3.3
\\
\qquad [Go14], \S6.4

We can remedy our inability to divide using only integers
by moving on the {\it rational numbers},
traditionally denoted {\bf Q} (probably for {\it quotient}).  This is the
simplest example of a {\it fraction field}, in this case formed over
the integers, {\bf Z}.  It is also our first example of a theme
we'll use repeatedly in this book, that of using a simple
algebraic system to construct a more complex one.

To form a fraction field from a ring, we take pairs of elements from
the ring (conventionally arranged into fractions) and establish an
{\it equivalence relationship} between them.  We also require that the
second element in the pair (the denominator) can not be zero.  Two
pairs $(a,b)$ and $(c,d)$ are equivalent if $ad=bc$, and we write them
$a\over b$ and $c\over d$.  We group equivalent pairs together into
{\it equivalence classes} and define our basic field operations as
follows:

\begin{figure}[h]
\label{fraction field operations}
\begin{mdframed}[backgroundcolor=cyan!20]
\begin{center}
\begin{eqnarray*}
{a\over b} + {c\over d} &=& {{ad+bc}\over{bd}} \cr
{a\over b} - {c\over d} &=& {{ad-bc}\over{bd}} \cr
{a\over b} {c\over d} &=& {{ac}\over{bd}} \vbox to20pt{}\cr
{\,{a\over b}\, \over {c \over d}} &=& {{ad}\over{bc}} \vbox to24pt{}\cr
\end{eqnarray*}
\end{center}
\end{mdframed}
\caption{Fraction field operations}
\end{figure}

The additive identity element is $0\over1$ and the multiplicative
identity is $1\over1$, using the original identities 0 and 1 from the
base ring.  Note that the division by zero is not defined, nor do
our field axioms require it to be.

Notice that although we define all four field operations, we only
use the three ring operations to do it!  I.e, when we divide
$a\over b$ by $c\over d$, we need only to form $ad$ and $bc$
in order to form the $(ad,bc)$ pair, which we write as ${ad}\over{bc}$.
We thus divide $1\over2$ by $2\over3$ to obtain $3\over4$
without ever having to divide the {\it integers} ---
only multiplying them ($1\cdot3=3$ and $2\cdot2=4$).

In general, there is no guarantee that this kind of construction will
work.  We can't just pair numbers up however we want and call it a
field.  Several other conditions have to be met.  First of all, we
have to ensure that the equivalence relationship is well-defined.  If
$x=y$ (in the sense of equivalence) and $y=z$, then we must have
$x=z$, otherwise we can't even cleanly establish the critical notion
of an {\it equivalence class} (which says that $1\over2$ and $2\over4$
are basically the same thing).  I emphasize here that the new field,
and its new operations, are defined using the equivalence classes,
although we muddle this distinction by using the smallest fraction in
a class to represent it.  Strictly speaking, the multiplicative
identity is not ${1\over1}$ but
$\{{1\over1},{2\over2},{3\over3},\ldots\}$, the additive identify is
not ${0\over1}$ but $\{{0\over1},{0\over2},{0\over3},\ldots\}$, and my
example in the last paragraph should have read ``we thus divide
$\{{1\over2},{2\over4},{3\over6},\ldots\}$ by
$\{{2\over3},{4\over6},{6\over9},\ldots\}$ to obtain
$\{{3\over4},{6\over8},{9\over12},\ldots\}$...''

Having cleanly established equivalence
classes, we have to make sure that our operations actually work
consistently on them, since they are defined in terms of fractions
within the classes.  We need to verify that taking any fraction from
one equivalence class and any fraction from other, then applying one
of four operations to them, we always get an answer in a third
equivalence class.  The actual answer can (and will) vary depending on
the choice of representative fractions, but it has to always be in the
same class.  In this way, we confirm that the operations are cleanly
defined not just on the fractions, but on the classes.  I'm not going
to actually make this verification, but leave it as an exercise.

Which is why I excluded zero as a possible denominator.  We do this
because otherwise our operations aren't cleanly defined on these
equivalence classes. ${1\over0}$ is not equivalent to ${0\over1}$
(since $1\cdot1\ne0\cdot0$), so ${1\over0}$ must have a multiplicative
inverse (by axiom F1); i.e, some fraction ${a\over b}$ must exist
which when multiplied by ${1\over0}$ produces ${1\over1}$, yet by the
zero theorem, no such element $b$ can exist in the base ring so that
$1b=0$.  Excluding zero as a possible denominator ensures that our
field axioms are satisfied.

Yet in the fraction
field operations, where we multiply two denominators together to get
the result's denominator, what would happen if two non-zero elements can be
multiplied to form zero, producing a zero denominator?  Nothing
in the ring axioms prevents this from happening, so we add an
additional axiom.

An {\it integral domain} ${\cal I}$ obeys all the ring axioms, plus
one more (figure \ref{integral domain axiom}) that guarantees the
non-existence of zero divisors.  All of the rings in this book
are integral domains.

\begin{figure}
\label{integral domain axiom}
\begin{mdframed}[backgroundcolor=cyan!20]
\begin{center}
All ring axioms, plus:

\begin{justify}
\begin{tabular}{l l l r}
   non-existence of zero divisors & $\forall a\ne 0,b\ne 0 \in {\cal I},$ & $ab\ne 0$ &(I1)\cr
\end{tabular}
\end{justify}
\end{center}
\end{mdframed}
\caption{Integral domain axioms}
\end{figure}

The fraction field construction is only defined on integral domains,
and I'll leave it as an exercise to show that ${\bf Z}$ is an integral
domain.  The main point of this section is to recognize that the
fraction field construction can be performed not only on the integers
${\bf Z}$ to obtain the rationals ${\bf Q}$, but on any integral
domain to obtain its fraction field.

\vfill\eject

\mysection{Polynomial rings and rational function fields}
\subsection*{\qquad The ring ${\cal F}[x]$ and the field ${\cal F}(x)$}
\qquad [van der Waerden], \S3.4

Having built a field from a ring, can we build a ring from a field?
The answer is yes, and the most important such construction is a {\it
polynomial ring}, whose elements are polynomials in some variable with
coefficients in the underlying field, all but a finite number of which
must be zero.\footnote{If we relax the finiteness requirement and
allow infinite ``polynomials'', we obtain a ring of {\it formal power series}
over the field, typically written ${\cal F}[[x]]$.  We will have
little use for power series in this book.}  We write this ring using
the underlying field, brackets and the variable, so ${\cal F}[x]$ is
the ring of polynomials in $x$ with coefficients from the field ${\cal
F}$.

${\cal F}[x]$ is a ring but not a field.  It is, however, an
integral domain (left as an exercise), so we can form a fraction field
from it, which we write using parenthesis instead of brackets: ${\cal
F}(x)$.  Elements in ${\cal F}(x)$ are fractions, both the numerator
and denominator of which are polynomials in $x$.  So, for example,
${x}\over{x-1}$ is a element of ${\bf Q}(x)$.  Fractions of
polynomials are called {\it rational functions}, so ${\bf Q}(x)$ is
the {\it field of rational functions in $x$ over the rational numbers}.

Now, you might ask, ``Can't $(x-1)$ be zero?  Say, if $x$ is 1?''.
The answer is {\it no}.  $x$ is not 1 or any other number.  $x$ is
$x$, and in ${\bf Q}[x]$, $(x-1)$ is as different from zero as $3
\over 2$ is.  $(x-1)$, and things like it, are {\it
completely distinct elements} in the algebraic systems in which they
are defined.

Now, obviously, we can set $x$ to be $1$.  But now we are no longer
working in ${\bf Q}[x]$ --- for starters, there is no longer a
distinct element $x$, since it's equal to $1$!  Now we are working in
${\bf Q}$.  Setting $x$ equal to $1$ mapped everything from ${\bf
Q}[x]$ into ${\bf Q}$.  This is a simple example of an {\it evaluation
homomorphism} --- a homomorphism (a mapping which preserves
operations) from one system to another created by setting an
independent variable equal to some constant value.

So, you ask, ``what about $1\over2$ and $2\over4$?  Are they distinct
elements as well?''  {\it No}.  This time we are dealing with elements
that are basically the same. This is where the technical details of
the fraction field construction become significant.  Strictly
speaking, we are not working with elements like $1\over2$ at all.  We
are working with the {\it equivalence classes} defined above.
$1\over2$ is a {\it representative} of an equivalence class that
includes $2\over4$.

It's convenient to select one unique representative of each
equivalence class.  In the case of fraction fields, we'll use the
fraction with no common factors between the numerator and the
denominator, i.e, $\frac{1}{2}$ instead of $\frac{2}{4}$ and
$\frac{1}{x}$ instead of $\frac{x}{x^2}$.  To reduce any given
fraction to its canonical form, we need to compute the greatest common
divisor of the numerator and denominator, and divide it out.
The simplest way to do this involves long division.

\begin{comment}

If all this seems a bit arbitrary, well, it is.  I could easily pick
two numbers from {\bf Z} and pair them into equivalence classes in
some other way than for {\bf Q}; if my basic axioms were satisified I
would even have a field!  Whether it would be useful for something
other than puzzling the few readers who would bother is a different
story.  Let me briefly cite a few other examples of {\it useful}
equivalence classes.  Take pairs of elements from a field $f$ and $g$
in the expression $f\,dg$ and form equivalence classes based on
whether the expression can be transformed to $f'\,dg'$; this is one
way to introduce differentials into an algebraic context.  Take points
in a Cartesian geometry $(x_1, x_2,\ldots x_n)$ and group them
together if they are related by a simple constant multiple $(\lambda
x_1, \lambda x_2,\ldots \lambda x_n)$; you now have lines through the
origin and the basis for projective geometry.  Take infinite
convergent sequences of rational numbers (from {\bf Q}) and group them
together if the differences between them converge to zero; the
equivalence classes are the real numbers.  I could keep going.  These
constructions are easy to form; their utility lies in our ability to
relate them to real world problems.

\end{comment}

\vfill\eject

\mysection{Long Division}
\qquad [van der Waerden], \S3.4

\begin{sympycode}
'''
Python code to format a polynomial long division problem in TeX

Usage: long_division(dividend, divisor, var [, domain])

'domain' is domain of coefficients and defaults to QQ

For multivariate problems, you need domain to be something
like QQ(x) or QQ(y)

We use TeX's \halign, and use two columns for each term: one for the
plus or minus sign and one for the coefficient and monomial.

'''

from sympy.abc import x,y

from sympy import degree
from sympy import quo, rem
from sympy import Poly
from sympy import LT, LC

' This routine formats a single polynomial into a partial table row '
' For an n-th degree polynomial, we use 2n+1 columns                '

def format_poly_textableformat(poly):
   result = ""
   if poly == 0:
      return "&0"
   for i in poly.all_terms():
      power = i[0][0]
      coeff = i[1]

      if coeff == 0:
         result += "&&"
      else:

         if (power > 0) and (coeff == 1 or coeff == -1):
            coeff_str = ''
         elif coeff.could_extract_minus_sign():
            coeff_str = latex(-coeff)
         else:
            coeff_str = latex(coeff)

         ''' if the coefficient is complicated, put parens around it, unless it's a fraction '''

         if coeff_str.find('+') != -1 and not coeff_str.startswith('\\frac'):
            coeff_str = "(" + coeff_str + ")"

         if coeff.could_extract_minus_sign():
            result += "-&" + coeff_str
         elif power == poly.degree():
            result += "&" + coeff_str
         else:
            result += "+&" + coeff_str

         if power == 0:
            result += "&"
         elif power == 1:
            result += " " + str(poly.gen) + "&"
         else:
            result += " " + str(poly.gen) + "^{" + str(power) + "}&"
   return result[0:len(result)-1]

def long_division(dividend, divisor, var, domain='QQ'):
   use_parens = True
   dividend = Poly(dividend, var, domain=domain)
   divisor = Poly(divisor, var, domain=domain)
   quotient = dividend.div(divisor)[0]

   numcols = 2*(dividend.degree() + 1 + divisor.degree() + 1)
   print("\\vbox{\\offinterlineskip")
   print("\\tabskip=0pt plus1fil")
   print("\\halign to\\hsize{\\tabskip=0pt",end='')
   for i in range(0,numcols+1):
      print("\\hfil $#$",end='')
      if (i < numcols): print(" & ",end='')
   print("\\tabskip=0pt plus1fil\\cr")

   ' quotient, indented '
   ' remember, (twice the degree) plus two columns '

   print("\\multispan{", numcols - (2*quotient.degree() + 2), "}&", end='')
   print(format_poly_textableformat(quotient))
   print("\\vbox to16pt{}\\cr")

   ' line under quotient '

   print("\\multispan{", numcols - (2*dividend.degree()+2), "}&", end='')
   print("\\multispan{", 2*dividend.degree()+2, "}\\vbox to 5pt{}\\leaders\\hrule\\hfil\\cr")

   ' divisor, vertical bar, dividend '

   print(format_poly_textableformat(divisor), end='')
   print("&\\vrule\\," + format_poly_textableformat(dividend) + "\\vbox to16pt{}\\cr")

   ' series of divisions '

   remainder = dividend

   while (remainder != 0) and remainder.degree() >= divisor.degree():

      " subtraction term "

      sterm = remainder.LC()/divisor.LC() * (remainder.LM()/divisor.LM()).as_expr()
      sterm = expand(sterm * divisor.as_expr())
      sterm = Poly(sterm, divisor.gen, domain=domain)

      if use_parens:

	 ''' Insane code.  We want to span up to the first text of the     '''
	 ''' polynomial to put a leading "-(" in.  Need to span an extra   '''
	 ''' column if the first column of the poly isn't really occupied  '''
	 ''' by anything.  Also have to explicitly enter (by printing)     '''
	 ''' and leave (by modifying poly) math mode.                      '''

         multispan_cols = numcols - (2*sterm.degree() + 2) + 1
         polystr = format_poly_textableformat(sterm)
         if polystr.startswith('&'):
            polystr = polystr[1:len(polystr)]
            multispan_cols += 1
         polystr = polystr.replace('&', '$&', 1)
         print("\\multispan{", multispan_cols, "}", end='')
         print("\\hfil $-(" + polystr + "\\vbox to16pt{}&)\\cr")
      else:
         print("\\multispan{", numcols - (2*sterm.degree() + 2), "}&", end='')
         print(format_poly_textableformat(sterm))
         print("\\vbox to16pt{}\\cr", end='')

      " the line - there are two cases here depending on whether the leading "
      " term of the polynomial right above use is positive or negative,      "
      " which determines if we should extend the line left into the sign     "
      " field (if it's negative) or not (if it's positive)                   "

      if sterm.could_extract_minus_sign():
        extend = 1
      else:
        extend = 0

      print("\\multispan{", numcols - (2*sterm.degree()+2)+1-extend, "}&", end='');
      print("\\multispan{", (2*sterm.degree()+2)-1+extend, "}\\vbox to 5pt{}\\leaders\\hrule\\hfil\\cr")

      " compute and print the new remainder "

      remainder = remainder.sub(sterm)

      if remainder != 0:
         print("\\multispan{", numcols - (2*remainder.degree()+2), "}&", end='')
         print(format_poly_textableformat(remainder))
         print("&\\vbox to16pt{}\\cr")
      else:
         print("\\multispan{", numcols - 2, "}&", end='')
         print(format_poly_textableformat(remainder))
         print("&\\vbox to16pt{}\\cr")

   print("}}")
\end{sympycode}

As we all learned in grade school, polynomials can be divided
using long division.  To generalize this in our more abstract
context, let's consider a very simple calculation of this type:

\sympyc{long_division(x**2+2*x+1, 2*x+1, x)}

Each step starts by dividing the leading terms, i.e, $x^2$ is divided
by $2x$ to form $x\over 2$.  Actually, we can be a bit more precise.
Each step starts by dividing the leading {\it coefficients},
since the variables are divided just by subtracting their
powers. $x^2$ divided by $x$ is just $x$.  We divide $1$ by $2$
to form $1\over2$ and in this manner obtain $x\cdot{1\over2}={x\over2}$.

Next, we multiply this value by the divisor to obtain a polynomial
that we will subtract from the dividend (or what remains of it after
prior steps).  Again, let's be more precise.  We multiply the
polynomial variable just by adding its powers.  What we really have to
{\it multiply} are the {\it coefficients}.  To multiple $x\over2$ by
$2x+1$ we multiply $1\over2$ by $2$ to obtain $1$, add the powers of
$x$ and $x$ to obtain $x^2$, and arrive at the first term $1\cdot
x^2=x^2$.  Next, we multiply $1\over2$ by $1$, get $1\over2$, add the
powers of $1$ and $x$ to obtain $x$, and have the second term
${1\over2}\cdot x={1\over2}x$.  Adding these terms we get
$x^2+{1\over2}x$ --- the first of the intermediate polynomials.

To perform the third step, we don't have to do anything with
the variables.  We just subtract the coefficients.  These
three steps are repeated until we are left with a remainder
of lower degree than the divisor.

So, to summarize, working with the polynomial variable is easy --- we
just add or subtract its integer powers.  We perform polynomial long
division by dividing, multiplying, and subtracting the {\it
coefficients}.  Now, these are three of the basic four operations
provided by a field.  It follows, therefore, that we can perform
polynomial long division on polynomials whose coefficients lie in any
field whatsoever.  Given ${\cal F}[x]$, a polynomial ring over a
field, we can use the field operations provided by ${\cal F}$ to
divide any two elements from ${\cal F}[x]$ using polynomial long
division and obtain a remainder and a quotient.

We can even say a bit more.  Just like with grade school long
division, we know that the degree of the quotient will be the
difference in degrees of the dividend and the divisor, and that the
degree of the remainder will be less than the degree of the divisor.
We just need to keep in mind that these degrees are measured relative
to the polynomial ring variable, not any other variable that might
appear as part of the underlying field.

\mysection{Greatest Common Divisors}
\qquad [van der Waerden], \S3.7, \S3.8, \S5.4 (multivariate rings)\hfil\break
\hbox{}\qquad [Ge92], Ch. 7

One of the most important uses of polynomial long division is to
compute greatest common divisors (GCDs), at least in theory.  In
practice, there are other, more efficient algorithms.\footnote{See
[Ge92], for example} However, because long division is a simple and
straightforward way to compute GCDs, because it provides a theoretical
underpinning for other methods, and because it leads us directly to
solving polynomial diophantine equations, I'll present it here in this
section.

The first thing to observe is that the long division equation, $D = qd
+ r$ (dividend equals quotient times divisor plus remainder), can be
rearranged to read $r = D - qd$, which shows that any common divisor
of the dividend and the divisor can be divided out from the right hand
side of the equation, so must divide the left hand side also.  Thus,
common divisors of the dividend and divisor are preserved in the
remainder.

Furthermore, since the remainder is always of lower degree than the
divisor, we can repeat the long division with the divisor as the new
dividend and the remainder as the new divisor.  The new remainder will
also preserve common divisors of the original dividend and divisor,
and will be of lower degree than the original remainder.  This process
can repeated, lowering the degree of the remainder at each step, until
we are left with a zero remainder, i.e. $D' = q' d'$, where I've used
primes to emphasize that we are no longer dealing with the original
dividend and divisor.  Since common divisors have been preserved
throughout by $D'$ and $d'$, it follows that $d'$, the divisor of the
last step, must be a common divisor.  It is, in fact, a greatest
common divisor ([Go14] Theorem 1.8.16).  This has been known since the
time of Euclid, at least in the case of integers.

Nothing in our ring axioms guarantees the existence of GCDs.  The
problem is that there might be a lattice of divisors for a given
element, instead of a strict ordering of them.  However, GCDs exist in
any integral domain in which the procedure described in the last
paragraph can be carried out ([Go14] Theorem 6.5.8 and Lemma 6.6.2).

\begin{key point}
A {\it GCD domain} is an integral domain in which
any two elements have a greatest common divisor.
\end{key point}

\begin{key point}
A {\it unit} is an invertable element.
\end{key point}

There is usually more than one GCD.  Any divisor can be transformed into
another divisor by multiplying it by a unit, since if $uu'=1$, then
$ab=(ua)(u'b)$ for any $a$ and $b$ whatsoever.  In particular, a
greatest common divisor can be transformed into another greatest
common divisor by multiplying it by a unit.

\begin{comment}
I leave without proof the
claims that in ${\cal F}[x]$, the units are all elements in ${\cal
F}$, and that all GCDs differ from each other by a unit multiple.
\end{comment}

\vfill\eject

\example

Compute a GCD of $4x^4+13x^3+15x^2+7x+1$ and $2x^3+x^2-4x-3$ in ${\bf Q}[x]$.

\bigskip

\begin{sympycode}
a = 4*x**4 + 13*x**3 + 15*x**2 + 7*x + 1
b = 2*x**3 + x**2 - 4*x - 3

long_division(a,b,x)
print('\\bigskip')
long_division(b, rem(a,b), x)
\end{sympycode}

\bigskip

The divisor of the last step, in this case
${35\over2}x^2+35x+{35\over2}$, is a GCD, and multiplying it by any unit
will produce a different GCD.  In the case of a polynomial ring over a
field, the units are the elements of the underlying field, so we can
multiply by anything in {\bf Q} (i.e, any rational number) and get
another GCD.  For this example, the obvious thing to multiply by is
$2\over35$, which both clears the denominators and divides out the
common factor in the numerators to produce $x^2+2x+1$.  Both answers
are acceptable.

The Sage function {\tt gcd} compute GCDs.

\begin{sageblock}
# x=polygen(QQ,'x')
gcd(4*x^4 + 13*x^3 + 15*x^2 + 7*x + 1,
    2*x^3 + x^2 - 4*x - 3)
\end{sageblock}

\endexample

\vfill\eject

\subsection*{Multivariate GCDs}

A few words are in order here about GCDs of multivariate polynomials.
A factorization in ${\bf Q}(x)[y]$ or
${\bf Q}(y)[x]$ (both of the form ${\cal F}[x]$) is superficially so
similar to a factorization in ${\bf Q}[x,y]$ ({\it not} of the form
${\cal F}[x]$), that the distinction should be noted.  In both of the
first two cases, we form a fraction field with respect to one of the
two variables and thus obtain a polynomial ring (in the other
variable) over the fraction field.  In the case of ${\bf Q}[x,y]$ we
do not form a fraction field with respect to either variable; thus we
have a polynomial ring over not a field, but over another polynomial ring.

% in systems of the form ${\cal
% U}[x]$, i.e, where the coefficients come from a unique factorization
% domain that is not a field.

Now a polynomial ring over a GCD domain is itself a GCD domain ([Go14]
Lemma 6.6.2 and Theorem 6.6.7), so by induction any finite series of
polynomial rings (like ${\bf Q}[x,y]$) is also a GCD domain.  This
implies that GCDs exist in multivariate polynomial rings.  The problem
is finding them, since the procedure described above requires long
division, and this only works cleanly in an ${\cal F}[x]$-type system.

One solution is to pick one variable (say, $x$) to form coefficients
(in the ring ${\bf Q}[x]$), then use the second variable (say, $y$) to
form polynomials in ${\bf Q}[x][y]$.

Then we factor out of each ${\bf Q}[x][y]$ polynomial the GCD of the
coefficients (calculated in ${\bf Q}[x]$), which we call the {\it
content} of the polynomial, leaving a {\it primitive polynomial}.  It
can be shown ([Go14] Lemma 6.6.9) that if a primitive polynomial
factors at all, then it factors into primitive polynomials.  We thus
can compute a GCD of the primitive parts and multiply this by the GCD
of the contents to obtain a GCD in ${\bf Q}[x,y]$.

We will have little use for multivariate polynomial factorizations in
this book, since invariably we will calculate GCDs with respect to one
variable, and form fraction fields from any others, and thus always be
working in ${\cal F}[x]$ systems.

% I mention this mainly to avoid
% confusion between factoring in ${\cal F}[x,y]$ and ${\cal F}(x)[y]$,
% and have thus omitted the proofs of Gauss' method; see the references
% for details.

% \vfill\eject

\example

Compute the GCD of $5xy-5y^2-7x+7y$ and $2x^2-yx-y^2$ in ${\bf Q}[x,y]$.

${\bf Q}[x,y]$ is a multivariate polynomial ring, but we'll need to
work in a ${\cal F}[x]$-type system to perform the computation.  Our
choices are ${\bf Q}(x)[y]$ and ${\bf Q}(y)[x]$.

Let's start with ${\bf Q}(x)[y]$, and rearrange the polynomials
so that $y$ is the polynomial variable and our coefficients
are in ${\bf Q}[x]$:

$$-5y^2+(5x+7)y-7x {\rm\qquad and\qquad} -y^2-xy+2x^2$$

The first step is to compute the content (GCD of the coefficients) of
each polynomial.  Clearly, the GCD of $-5$, $(5x+7)$, and $-7x$ is 1
and the GCD of $-1$, $-x$, and $2x^2$ is also 1, so both polynomials
are already primitive and we can just proceed with the GCD calculation
in ${\bf Q}(x)[y]$:

\begin{sympycode}
long_division(-y**2-x*y+(2*x**2), -5*y**2+(5*x+7)*y-(7*x), y, domain='QQ[x]')
print("\\bigskip")
long_division(-5*y**2+(5*x+7)*y-(7*x), -(2*x+7/5)*y+(2*x**2+7/5*x), y, domain='QQ(x)')
\end{sympycode}


This leads us to conclude that the last divisor,
$-(2x+{7\over5})y+(2x^2+{7\over5}x)$ is a GCD in ${\bf Q}(x)[y]$.  Now
we need to remove its content, which is the GCD of $-(2x+{7\over5})$
and $(2x^2+{7\over5}x)$, or $(2x+{7\over5})$.  Dividing through by
this polynomial (a polynomial in ${\bf Q}[x]$, and thus a unit in
${\bf Q}(x)[y]$) we obtain $-y+x$.  We now multiply by the GCD of our
original contents, but they were just 1, so we conclude that $x-y$
is our GCD in ${\bf Q}[x,y]$.

Now let's do all that again in ${\bf Q}(y)[x]$.  Our polynomials become:

$$(5y-7)x-(5y^2-7y) {\rm\qquad and\qquad} 2x^2-yx-y^2$$

The second one has unit content (the GCD of $2$, $-y$, and $-y^2$),
but the first one's content is $\gcd(5y-7,5y^2-7y)=5y-7$.
Dividing this out, we obtain:

$$x-y {\rm\qquad and\qquad} 2x^2-yx-y^2$$

and compute the GCD of these polynomials:

\sympyc{long_division(2*x**2-y*x-y**2, x-y, x, 'QQ(y)')}

Thus, $x-y$ is the GCD of the primitive polynomials, and it has unit
content $\gcd(1,-y)$.  The GCD of the original contents
($1$ and $5y-7$) is 1, so the final result is again $x-y$.

\begin{maximablock}
gcd(5*x*y - 5*y^2 - 7*x + 7*y,
    2*x^2 - y*x - y^2);
\end{maximablock}

\endexample

\vfill\eject

\mysection{Polynomial Diophantine Equations}

The same long division procedure used for GCD computations can also be
used solve a certain class of {\it polynomial Diophantine equations}.
A Diophantine equation is one whose variables are restricted to be
integers.  The most famous example is Fermat's equation,
$x^n+y^n=z^n$; Fermat's theorem states that this equation has no
solutions $x,y,z,n\in{\bf Z}$ for $n>2$.  A generalized Diophantine
equation is one whose variables are restricted to some algebraic
system, not necessarily ${\bf Z}$.  A polynomial Diophantine equation
is one whose variables are restricted to be polynomials of some form,
and the one we will consider here is this:

\begin{displaymath}
sa+tb=c; \qquad a,b,c\in{\cal F}[x] {\rm\, given}; \qquad
s,t\in{\cal F}[x] {\rm\, unknown}
\end{displaymath}

Let's begin by noting that any common divisor of $a$ and $b$, and in
particular $\gcd(a,b)$, can be divided out from the left hand side of
the equation, and thus must also divide the right hand side, so $c$
must be be a multiple of $\gcd(a,b)$, or the equation has no solution.

This necessary condition is also sufficient, and the simplest way to
demonstrate this is to use the GCD computation in a constructive
proof.  Note that first step in computing $\gcd(a,b)$ is to solve
$a=qb+r$.  Rearranging this as $r=a-qb$ we see how the remainder can
be expressed in the Diophantine form $sa+tb$.  More generally, at each
step of the calculation, we solve $D=qd+r$, where $D$ and $d$ are each
either $a$, $b$, or a remainder from a previous step, so using
$r=D-qd$ we can write each remainder in the form $sa+tb$.  At the end
of the calculation, we will have expressed $\gcd(a,b)$ in the form
$sa+tb$.

We now use long division to divide $c$ by $\gcd(a,b)$.  Because of the
necessity demonstrated above, the division must be exact (i.e, zero
remainder) or the equation has no solution.  Having computed both
$\gcd(a,b)=sa+tb$ and $c=q\gcd(a,b)$ we can now combine these
expression to form $c=(qs)a+(qt)b$, which solves the original
equation.

This solution is not unique.  Given a solution to $c=sa+tb$, we can
form any multiple of $ab$, say $mab$, and write another solution
$c=(s-mb)a+(t+ma)b$.  Note however, that $(s-mb)$ has the form of a
remainder after dividing $s$ by $b$ ($m$ is the quotient).  Since the
degree of a remainder is always less than the degree of the divisor,
it follows that if $sa+tb=c$ can be solved, then we can always compute
an $s$ of lower degree than $b$, or a $t$ of lower degree than $a$.

If $\deg(c)<\deg(a)+\deg(b)$, then these conditions are not exclusive;
finding an $s$ of lower degree than $b$ implies a $t$ of lower degree
than $a$.  To see this, simply note that if $\deg(s)<\deg(b)$, then
$\deg(sa)=\deg(s)+\deg(a)<\deg(b)+\deg(a)$.  Since $tb=c-sa$, if
$\deg(c)<\deg(a)+\deg(b)$ and $\deg(sa)<\deg(a)+\deg(b)$, then
$\deg(tb)<\deg(a)+\deg(b)$, which implies that $\deg(t)<\deg(a)$.

We will make repeated use of this polynomial Diophantine equation
throughout the book.

\vfill\eject

\example

Solve:

$$s(4x^4+13x^3+15x^2+7x+1) + t(2x^3+x^2-4x-3) = x^3 + 5x^2 + 7x +3$$

\quad for $s,t \in {\bf Q}[x]$ satisfying minimal degree bounds.

Using the notation:

$$a = 4x^4+13x^3+15x^2+7x+1; \qquad
b = 2x^3+x^2-4x-3; \qquad
c = x^3 + 5x^2 + 7x +3$$

we see that we are trying to solve $sa+tb=c$.
$a$ and $b$ are the same polynomials used for the first GCD example.
Recalling that GCD calculation:

\bigskip
\begin{sympycode}
long_division(a,b,x)
print('\\bigskip')
long_division(b, rem(a,b), x)
\end{sympycode}

The first division states that:

\begin{eqnarray*}
a &=& (2x+{11\over2})b + ({35\over2}x^2+35x+{35\over2})
\end{eqnarray*}

Rearranging this equation we get:

\begin{eqnarray*}
x^2+2x+1 &=& {2\over35}a - {1\over35}(4x+11)b \cr
\end{eqnarray*}

The second division isn't used in solving the polynomial
Diophantine equation; it simply states that
${35\over2}x^2+35x+{35\over2}$ and $x^2+2x+1$ are GCDs.

Having concluded that
$x^2+2x+1$ is a GCD of $a$ and $b$, we divide it into $c$:

\sympyc{long_division(x**3+5*x**2+7*x+3, x**2+2*x+1, x)}


The remainder is zero, so the original problem has a solution
(remember that $c$ had to be a multiple of $\gcd(a,b)$).

We substitute our expansion for $x^2+2x+1$ above into $c=(x+3)(x^2+2x+1)$
and obtain:

%\begin{eqnarray*}
%c &=& {2\over35}(x+3)a - {1\over35}(x+3)(4x+11)b \cr\cr
%  &=& {2\over35}(x+3)a - {1\over35}(4x^2+23x+33)b \cr
%\end{eqnarray*}

$$c = (x+3)(x^2+2x+1)
%= {2\over35}(x+3)a - {1\over35}(x+3)(4x+11)b
 = {2\over35}(x+3)a - {1\over35}(4x^2+23x+33)b $$

$\deg({2\over35}(x+3)) = 1 < \deg(b) = 3$ and $\deg({1\over35}(4x^2+23x+33))
= 2 < \deg(a) = 4$, so the degree bounds are met and we have our solution:

\begin{eqnarray*}
s &=& {2\over35}(x+3) \cr\cr
t &=& - {1\over35}(4x^2+23x+33) \cr
\end{eqnarray*}

Now let's verify this solution using Maxima:

% \vfill\eject

\begin{maximablock}
a: 4*x^4+13*x^3+15*x^2+7*x+1 $
b: 2*x^3+x^2-4*x-3 $
c: x^3+5*x^2+7*x+3 $
\end{maximablock}

\begin{sageblock}
R.<x> = QQ['x']
a = 4*x^4+13*x^3+15*x^2+7*x+1;
b = 2*x^3+x^2-4*x-3;
c = x^3+5*x^2+7*x+3;
\end{sageblock}

Maxima's {\tt gcdex(a,b)} function returns a list {\tt [s,t,gcd]} satisfying

$$as + bt = \gcd(a,b)$$

\begin{maximablock}
gcdex(a,b);
\end{maximablock}

\begin{sageblock}
xgcd(a,b);
\end{sageblock}

\begin{sageblock}
def diophantine(a,b,c):
   (g,s,t) = xgcd(b,c)
   if not g.divides(a):
      raise ValueError("diophantine: a doesn't divide gcd(b,c)")
   s = s * (a//g)
   t = t * (a//g)
   # g = sb + ct
   # a = mg = (ms)b + (mc)t
   (q,r) = s.quo_rem(c)
   # s = r + qb, so r = s - qb
   return (r, t + q*b)
\end{sageblock}

\begin{sageblock}
(s,t) = diophantine(c,a,b)
c == a*s + b*t
\end{sageblock}

\vfill\eject

If the $c$ polynomial divides the GCD we just computed, then we can just multiply
through by the quotient:

\begin{maximablock}
divide(c, %[3]);
[s,t,_] : %th(2)*%[1];
s;
t;
a*s + b*t === c;
kill(a,b,c,s,t)$
\end{maximablock}

\endexample

\vfill\eject

\mysection{Partial Fractions Expansion}
\qquad [van der Waerden], \S5.10

As a first application of polynomial Diophantine equations, we use
them to construct partial fractions expansions, which will be
a major technical tool in the first half of the book.

Consider an element
$a$ from a polynomial fraction field ${\cal F}(x)$.  We can write
$a={n\over d}$ where $n,d\in{\cal F}[x]$.  If we are now given a
factorization of $d=d_1^{e_1} d_2^{e_2} \cdots d_k^{e_k}$, where
$d_i\in {\cal F}[x]$ and $\gcd_{{\cal F}[x]}(d_i,d_j)=1$ if $i\ne j$,
and assuming that $a$ is a proper fraction
($\deg_x n < \deg_x d$),
then we can construct a {\it partial fractions expansion} of $a$:

\begin{equation}
\label{partial fractions expansion}
a={n\over d}=\sum_{i=1}^{n}\sum_{j=1}^{e_i} {n_{i,j}\over {d_i}^j}
\qquad \deg_x (n_{i,j}) < \deg_x (d_i)
\end{equation}

We begin by computing an expansion in the form:

\begin{equation}
\label{partial fractions intermediate expansion}
a={n\over d}=\sum_{i=1}^{n} {n_i\over {d_i}^{e_i}}
\qquad \deg_x (n_i) < e_i\deg_x (d_i)
\end{equation}

i.e, each denominator factor is separated apart from the
others, but there is only a single term for each
denominator factor and the degree bounds are weaker.

$n_i$ is found by solving the following polynomial Diophantine
equation for $n_i$ and $r_i$:

\begin{eqnarray*}
n &=& n_i \Big( \prod_{j\ne i} {d_j}^{e_j} \Big) + r_i (d_i^{\,e_i}) \cr
\end{eqnarray*}

The degree bounds from the previous section guarantee that
$\deg_x (n_i) < e_i \deg_x (d_i)$, and dividing through by $d$ shows:

\begin{eqnarray*}
{n\over d} &=& {{n_i}\over{d_i^{\,e_i}}} + {r_i\over{\prod_{j\ne i} {d_j}^{e_j}}} \cr
\end{eqnarray*}

We can now either ignore $r_i$ and use this procedure to compute
all of the $n_i$'s, or we can note that
the second term on the right is a fraction in the original form,
but with one less factor in the denominator, so we can recurse
and separate out all the ${d_i}^{e_i}$ into seperate fractions.

Having computed an expansion of the form \eqref{partial fractions
intermediate expansion}, a series of long divisions now suffices to
seperate each of these fractions into a full partial fraction expansion in the
form \eqref{partial fractions expansion}:

% \begin{eqnarray*}
% q_{i,e_i} &=& n_i \cr
% q_{i,j} &=& q_{i,j-1} d_i + n_{i,j} \cr
% q_{i,1} &=& n_{i,1}
% \end{eqnarray*}

\begin{equation*}
q_{i,e_i} = n_i \qquad
q_{i,j} = q_{i,j-1} d_i + n_{i,j} \qquad
q_{i,1} = n_{i,1}
\end{equation*}

The degree bounds on long division ensure that

\begin{equation*}
\deg_x q_{i,j} < j \deg_x d_i  \qquad  {\rm and} \qquad \deg_x n_{i,j} < \deg_x d_i
\end{equation*}

Assembling the $n_{i,j}$'s together, we obtain the form
required by equation \eqref{partial fractions expansion}:

%\begin{eqnarray*}
%n_i &=& \sum_{j=1}^{e_i} n_{i,j}\, d_i^{\,e_i-j} \cr\cr
%{{n_i}\over{d_i^{\,e_i}}} &=& \sum_{j=1}^{e_i} {n_{i,j}\over {d_i}^j}
%\end{eqnarray*}

\begin{equation*}
n_i = \sum_{j=1}^{e_i} n_{i,j}\, d_i^{\,e_i-j} \qquad
{{n_i}\over{d_i^{\,e_i}}} = \sum_{j=1}^{e_i} {n_{i,j}\over {d_i}^j}
\end{equation*}

\theorem
Partial fractions expansions that meet the minimal degree bounds are unique.

\proof

If two different partial fractions expansions can be constructed for
the same proper fraction $a$, then subtracting them from each other
would yield a non-trivial partial fractions expansion for $0$:

\begin{equation}
0=\sum_{i=1}^{n}\sum_{j=1}^{e_i} {n_{i,j}\over {d_i}^j}
\qquad \deg_x (n_{i,j}) < \deg_x (d_i)
\end{equation}

We clear the denominators:

\begin{equation}
0 =\sum_{i=1}^{n}\sum_{j=1}^{e_i} n_{i,j} \left( d_i^{\,e_i-j} \prod_{k \ne i} d_k^{\,e_k} \right)
\end{equation}

Isolating the $n_{1,e_i}$ term we obtain:

\begin{equation}
-n_{1,e_1}\prod_{k = 2}^{n} d_{k}^{\,e_k}=\sum_{j=1}^{e_1-1} n_{1,j} \left( d_1^{\,e_1-j} \prod_{k =2}^n d_k^{\,e_k} \right)
+ \sum_{i=2}^{n}\sum_{j=1}^{e_i} n_{i,j} \left( d_i^{\,e_i-j} \prod_{k \ne i} d_k^{\,e_k} \right)
\end{equation}

% All of the terms except the $n_{1,e_i}$ term are multiplied by some power of $d_1$, which implies that
% the $n_{1,e_i}$ term must also be a multiple of $d_1$.  Since all of the $d_i$'s are relatively
% prime, this $d_1$ can only come from

All of the terms on the right hand side include a $d_1$ factor, which implies that
$d_1$ must also factor the left hand side.
Since all of the $d_i$'s are relatively
prime, this $d_1$ factor can only come from
$n_{1,e_1}$ itself.  Yet
$\deg_x (n_{1,e_1}) < \deg_x (d_1)$, so this is impossible,
and no such expansion can exist.

\endtheorem


% We recurse on the first term until we have completed the desired construction.

\vfill\eject

\example

\begin{maximacode}
pf: (3*x**2-4*x+2)/(x**3-3*x**2+4) $
/* pf: (x**2+3*x+2)/(x**3-3*x**2+4) $ */
\end{maximacode}

Compute the partial fractions expansion of $$\maximac{pf;}$$

We begin by factoring the denominator.  While factoring can be quite
complicated in practice, in this case we need only try some small
integers to discover that either $2$ or $-1$ solve the denominator,
leading directly to a factorization:

\begin{comment}
Differentiating to obtain $3x^2-6x$.
Computing the GCD of $x^3-3x^2+4$ and $3x^2-6x$:

\sympyc{long_division(x**3-3*x**2+4, 3*x**2-6*x, x)}
\bigskip
\sympyc{long-division(3*x**2-6*x, -2*x+4)}


Thus, $-2x+4$ is a GCD, which we normalize by dividing through by -2
to obtain $x-2$.  We could now proceed by dividing $x^3-3x^2+4$ by
$x-2$ to obtain $x^2-x-2$ (all factors at unit power), compute the GCD
of $x-2$ and $x^2-x-2$ to obtain $x-2$ (all higher factors at unit
power), divide $x^2-x-2$ by $x-2$ to obtain $x+1$ (the unit
square-free factor), and repeat the process (trivially) with $x-2$ to
decide that $x-2$ is the second square-free factor.  Or, we could
shortcut the entire process by noting that since $x-2$ is linear,
it can only be the second square-free factor.  In any event, we conclude that:

\end{comment}

$$\maximac{pf;} = \maximac{factor(pf);}$$

Next, we solve the polynomial Diophantine equation:

$$\maximac{num(pf);} = s(x-2)^2 + t(x+1) = sa+tb$$

First, we compute the GCD of $a=(x-2)^2=x^2-4x+4$ and $b=x+1$.
The actual result is obvious, but now we're interested
in both the remainder and the quotient.

\sympyc{long_division(x**2-4*x+4, x+1, x)}

Since the remainder, 9, is a unit, $a$ and $b$ have no common
factors and their GCD is 1.  Of course, this result is hardly
surprising since $(x-2)$ and $(x+1)$
have no common factor between them.

$$a=(x-5)b+9$$
$$9=a-(x-5)b$$
$$1={1\over9}[a-(x-5)b]$$
$$\maximac{num(pf);} = {1\over9}(\maximac{num(pf);})[a-(x-5)b]$$
$$\maximac{num(pf);} = {1\over9}(\maximac{num(pf);})a-{1\over9}(\maximac{expand(num(pf)*(x-5));})b$$

Our degree bounds aren't met yet, so we divide $b=x+1$ into $a$'s coefficient $\maximac{num(pf);}$:

\sympyc{long_division(3*x**2-4*x+2, x+1, x)}

The remainder becomes our new $a$ coefficient,
and after subtracting $(3x-7)a=\maximac{expand((3*x-7)*(x-2)**2);}$ from the $b$ coefficient,
we conclude that:

$$x^2 + 3x + 2 = a - (\maximac{expand((num(pf)*(x-5) - (3*x-7)*(x-2)**2)/9);})b$$

In other words (remember that $a=(x-2)^2$ and $b=x+1$),

$${{x^2 + 3x + 2}\over{(x-2)^2(x+1)}} = {{1}\over{x+1}} + {{2x-2}\over{(x-2)^2}}$$

Now we need only divide $(2x-2)$ by $(x-2)$:

\sympyc{long_division(2*x-2, x-2, x)}

so,

$${{x^2 + 3x + 2}\over{x^3-3x^2+4}} = {{1}\over{x+1}} + {2\over{(x-2)^2}} + {2\over{(x-2)}}$$

Again, let's verify this result using Maxima:

\begin{maximablock}
partfrac((3*x^2 - 4*x + 2)/(x^3 - 3*x^2 + 4), x);
\end{maximablock}

Here's Sage code to do a partial fractions expansion
and save the results in an array:

\begin{sagecommon}
def partfrac(num, den):
   b = {}
   factorization = factor(den)
   for f in factorization:
     (t,s) = diophantine(num, f[0]^f[1], den//(f[0]^f[1]))
     for i in range(f[1],1,-1):
       (s, b[f[0],i]) = s.quo_rem(f[0])
     b[f[0],1] = s
   return(b)
\end{sagecommon}

\begin{sageblock}
b = partfrac(3*x^2 - 4*x + 2, x^3 - 3*x^2 + 4);
displayarray(b);

\end{sageblock}

\endexample

\vfill\eject


\mysection{Resultants}
\qquad [van der Waerden], \S5.8; [Lang], \S IV.8

At times, we will want a simple way of testing two polynomials in ${\cal F}[x]$
to see if they have a GCD, without actually computing it.  This
is more than just a computational convenience.  The presence of the
polynomial's variable in the GCD often encumbers us.  On the other
hand, the {\it resultant} yields a simple element from the underlying
field that is zero if the polynomials have a non-trivial GCD and
non-zero otherwise.  The GCD exists in ${\cal F}[x]$, while
the resultant is in ${\cal F}$.

For example, the polynomials $t x+x+t+1$ and $ty + y$ share $t+1$
as a GCD, so their $t$-resultant is zero.  On the other hand,
their $x$-resultant is not zero, because in the ring ${\bf C}(y,t)[x]$,
$t+1$ is a unit, so the GCD is $1$.  The resultant is constructed in an
${\cal F}[x]$-type system, so for multivariate polynomials,
we always need to specify which variable is the ring variable;
any remaining variables are implicitly field variables.

The resultant is defined\footnote{There are other equivalent ways of
defining the resultant.} as the determinant of the Sylvester matrix
$S_x(P,Q)$, which is the $m+n \times m+n$ matrix constructed from two
polynomials (in ${\cal F}[x]$) $P$ and $Q$ of degrees $m$ and $n$ (all
the blanks are zeros):

$$ P = \sum_{i=0}^m p_i \, x^i \qquad Q = \sum_{i=0}^n q_i \, x^i $$

$$ S_x(P,Q) = \begin{pmatrix}
  p_m & p_{m-1} & \ldots & p_0 & & & \cr
  & p_m & p_{m-1} & \ldots & p_0 & & \cr
  & & \ldots & & \ldots & & \cr
  & & & p_m & p_{m-1} & \ldots & p_0 \cr
  \vdots & & & \vdots & & & \vdots \cr
  q_n & q_{n-1} & \ldots & q_0 & & & \cr
  & q_n & q_{n-1} & \ldots & q_0 & & \cr
  & & \ldots & & \ldots & & \cr
  & & & q_n & q_{n-1} & \ldots & q_0 \cr
  \end{pmatrix} $$

In plain English, the matrix is constructed by forming the first row
from the first polynomial coefficients, adding $n-1$ trailing zeros at
the end of the row.  The second row is formed by shifting the first
row one position to the right.  This shifting is repeated a total of
$m-1$ times to obtain the first $m$ rows.  The last $n$ rows are
constructed in the same way from the second polynomial.

Now consider the following straightforward matrix identity:

$$ S_x(P,Q) \begin{pmatrix}x^{n+m-1}\cr x^{n+m-2}\cr \vdots \cr x \cr 1\end{pmatrix}
 = \begin{pmatrix}P x^{m-1}\cr P x^{m-2}\cr \vdots \cr Q x \cr Q\end{pmatrix} $$

If $\det S_x(P,Q)$ is non-zero, then the Sylvester matrix is
invertible, and we can form the following equation:

$$ \begin{pmatrix}x^{n+m-1}\cr x^{n+m-2}\cr \vdots \cr x \cr 1\end{pmatrix}
 = S_x(P,Q)^{-1} \begin{pmatrix}P x^{m-1}\cr P x^{m-2}\cr \vdots \cr Q x \cr Q\end{pmatrix} $$

Since the matrix is formed exclusively from the polynomials'
underlying field ${\cal F}$, its inverse must also be formed from
${\cal F}$.  Now consider the bottom element in the last equation.  It
must have the following form (the $f_i$'s are the bottom row of
$S_x(P,Q)^{-1}$):

$$ 1 = f_0 P x^{m-1} + f_1 P x^{m-2} + \ldots + f_{n+m-1} Q x + f_{n+m} Q \qquad f_i \in {\cal F}$$
$$ 1 = A P + B Q \qquad A,B \in {\cal F}[x] $$

The only way this statement can be true is if $P$ and $Q$ (viewed
as polynomials in $x$) have a
trivial GCD, so a non-zero determinant of $S_x(P,Q)$ imply that $P$ and
$Q$ have only a trivial GCD.

Conversely, assume that $\gcd_x(P,Q) = 1$.  Then we can solve a series
of polynomial Diophantine equations to express $1, x, \ldots,
x^{n+m-1}$ as $AP+BQ$, where $\deg A < \deg Q = n$ and $\deg B <
\deg P = m$, which suffices to construct an inverse of the Sylvester
matrix.

We have thus proved:

\begin{theorem}\label{resultant theorem}
The resultant is zero iff the two polynomials have a non-trivial GCD.
\end{theorem}

Let me note two points.  First, though we used a field construction
for the proof, determinants are constructed using only ring
operations, so resultants can be computed in any ring and the result
will be a ring element.  The proof depends on the ring having a
well-defined fraction field, but since UFDs are integral domains, the
GCD concept doesn't make sense without a fraction field anyway.

Second, if the underlying ring involves multiple variables, the net
effect of the resultant is to eliminate one of them.  To see this,
imagine arbitrary values being assigned to the other variables.  The
resultant yields a condition on the remaining variables for the
original system to be solvable.


\vfill\eject

Here's a Maxima function that constructs a Sylvester matrix:

\begin{maximacommon}
syl_elem(a,b,t,i,j) :=
   if (i > hipow(b,t))
      then coeff(b,t,i-j)
      else coeff(a,t,i-j+hipow(a,t));
sylvester(a,b,t) := block(
   [size: hipow(a,t) + hipow(b,t)],
   genmatrix(lambda([i,j], syl_elem(a,b,t,i,j)),
             size, size)
);
\end{maximacommon}

\begin{comment}
...alternate Maxima function to construct Sylvester matrix
\begin{maximacommon}
syl1[a, x, rows, cols] :=
   genmatrix(lambda([i,j], coeff(a,x,hipow(a,x)-j+i)),
             rows, cols);
sylvester(a,b,x) :=
   addrow(syl1[expand(a), x,
               hipow(b,x),hipow(a,x)+hipow(b,x)],
          syl1[expand(b), x,
               hipow(a,x),hipow(a,x)+hipow(b,x)]);
\end{maximacommon}
\end{comment}

\example Compute the $t-$resultant of $t^2 - 1 -x$ and $t^3-t-y$.

\begin{maximablock}

sylvester(t**2 - 1 - x, t**3 - t - y, t);
ratsimp(determinant(%));
\end{maximablock}

More succinctly, we can use the built-in Maxima function {\tt resultant}:

\begin{maximablock}
resultant(t**2 - 1 - x, t**3 - t - y, t);
\end{maximablock}

This result implies that the polynomials $t^2-1-x$ and $t^3-t-y$
have a common factor when $y^2-x^3-x^2=0$.  For example,
this condition is satisfied when $x=3$ and $y=6$, and
the two polynomials become $t^2-4$ and $t^3-t-6$, which
have the common factor $t-2$:

\begin{maximablock}
factor(t**2-4);
factor(t**3-t-6);
\end{maximablock}

\endexample


\vfill\eject

\begin{comment}

\mysection{Simple Field Extensions}

A {\it simple} extension of a field ${\cal F}$ is formed by adjoining
a single new element $\theta$ to the field, and then forming all
possible sums, differences, multiples, and quotients to form a new
field.  Any such simple extension will be isomorphic to either the
fraction field ${\cal F}(\theta)$ (if $\theta$ is transcendental) or
an algebraic extension ${\cal F}[\theta]$ (if $\theta$ is algebraic).

\mysection{Finite Fields}

By using the same reasoning to consider Diophantine equations over the
integers, it's not that hard to see that $2s+6t=1$ has no solution for
$s,t\in{\bf Z}$, because 2, the GCD of 2 and 6, does not divide the
right hand side of this equation.  On the other hand, if $p$ is a
prime number, then $xs+pt=1$ can always be solved for $s,t\in{\bf Z}$,
so long as $0<x<p$, since $\gcd(x,p)=1$.

This leads directly to the observation that ${\bf Z}_n$, the ring of
integers modulo $n$, is a field if and only if $n$ is in fact a prime
(and we write it ${\bf Z}_p$).  In order to invert a number $x$ in
${\bf Z}_n$, we need a solution $x'\in{\bf Z}_n$ to $xx'\equiv\, 1\,
(\pmod n)$, or $xx' = 1 + nt$, or $xx' - nt = 1$, i.e, we need
to solve the integer Diophantine equation considered above.  If $n$ is
prime, we can always solve the equation; if $n$ is composite, then the
equation can't be solved if $x$ is a factor of $n$.  Thus, everything
in ${\bf Z}_n$ is invertible if and only if $n$ is prime.

In fact, we can say more.  If ${\cal F}$ is a {\it finite field} (a
field with a finite number of elements), then its {\it prime subfield}
(its smallest subfield) must be isomorphic to ${\bf Z}_p$,
for some prime $p$.

A field can have a finite prime subfield, even if the field itself is
infinite.  Consider ${\bf Z}_5(x)$, the field of rational functions in
$x$ with coefficients in ${\bf Z}_5$.  Clearly, we can build
polynomials in ${\bf Z}_5(x)$ with as a high a degree as we want, and
they are all unique, so ${\bf Z}_5(x)$ is infinite.  Yet it
should also be clear that ${\bf Z}_5(x)$'s prime subfield
is ${\bf Z}_5$, which is finite.

Such fields, which share some properties of both purely infinite and
finite fields, are called {\it fields of characteristic $p$}, where
$p$ is the order of the prime subfield.  Fields with infinite prime
subfields (isomorphic to {\bf Z}) are called {\it fields of
characteristic zero}).  WHAT ABOUT RINGS OF INFINITE CHARACTERISTIC?

DEFINE ISOMORPHISM.

\end{comment}

\begin{comment}

\mysection{Linear Algebra}
\qquad [van der Waerden], Ch. 4, \S6.11 (trace of an field extension)

Determinants; trace.

\end{comment}

\mysection{Algebraic Extensions}
\subsection*{\qquad The field ${\cal F}[x]$ mod $n(x)$}

Both our fraction field and polynomial ring constructions are examples
of {\it extensions}.  Simply put, when we use an algebraic system to
construct a new algebraic system that includes the original system as
a subset, then the new system is an {\it extension} of the
original.\footnote{The key property is that the one system is a subset
of the other, not the exact method of construction.}  So, ${\bf Z}[x]$
is an extension of ${\bf Z}$ because we can identify ${\bf Z}$ as a
subset of ${\bf Z}[x]$ and, in fact, even homomorphicly map ${\bf Z}$
into ${\bf Z}[x]$.  Such an {\it inclusion homomorphism} should not be
confused with an evaluation homomorphism, which would map the other
way (from ${\bf Z}[x]$ into ${\bf Z}$).

The only remaining type of extension that will be important to us is
the {\it algebraic extension}.  It is another equivalence class
construction that we build starting with a polynomial ring over a
field, say ${\cal F}[x]$.  Our equivalence classes are all elements in
${\cal F}[x]$ whose differences are multiples of some distinguished
irreducible polynomial in ${\cal F}[x]$, called the {\it minimal
polynomial} of the field.  And I do say {\it field}, because we don't
need to use the fraction field construction with an algebraic
extension; the ring is already a field.

Algebraic extensions are used to model fields where some algebraic
expression (the minimal polynomial) is zero.  Since adding any
multiple of zero to an expression doesn't affect its value, all
elements in an algebraic equivalence class are essentially the same
value.

This isn't exactly the same as the real number system.  Real numbers
are typically constructed using Cauchy sequences of rational numbers.
Thus, the square root of two is constructed as a real number using the
Cauchy sequence
$\{1, \frac{14}{10}, \frac{141}{100}, \frac{1414}{1000}, \frac{14142}{10000}, \cdots\}$,
which obeys the algebraic relationship $\gamma^2-2=0$.  The
distinction is that the negative square root of two obeys the same
algebraic relationship, but is constructed from a different Cauchy
sequence: $\{-1, -\frac{14}{10}, -\frac{141}{100}, -\frac{1414}{1000},
-\frac{14142}{10000}, \cdots\}$.  Thus $\gamma$, as a solution of
$\gamma-2=0$, could be either the positive or the negative square root
of two; we can't distinguish them using only the four field operations
(addition, subtraction, multiplication, and division).  To distinguish
between $+\sqrt{2}$ and $-\sqrt{2}$, we'd have to introduce an
additional relationship -- greater than or less than -- which would
produce an {\it ordered field}, and that's not part of our vocabulary.
An ordered field can be treated as an unordered field by ignoring its
ordering, which is fortunate for us, because it allows our integration
theory to be applied to standard real numbers.  For our purposes,
$\gamma$, as a root of $\gamma^2-2=0$, is simply a field element that
when squared equals two.  It could be either $+\sqrt{2}$ or
$-\sqrt{2}$; we can't tell the difference, but it isn't important
because our results will be the same in either case.

Polynomial long division (section \ref{sec:Long Division}) can be used
to divide any polynomial by the minimal polynomial, then we discard
the quotient and use the remainder as the unique representative of our
equivalence class.  Thus, we can easily perform addition, subtraction,
and multiplication in an algebraic extension field, by simply
performing ordinary polynomial operations and then modulo reducing to
a remainder, but how do we perform division?  The polynomial
Diophantine equation algorithm described in
section \ref{sec:Polynomial Diophantine Equations} can be used to
solve the following equation:

$$sx + tn = \gcd(x,n)$$

Now, if $n$ is irreducible, its GCD with any polynomial
of lower degree will be 1, so our equation becomes:

$$sx + tn = 1$$

Reducing mod $n$, we obtain

$$sx \equiv 1 \mod n$$

So $s$, when multiplied by $x$, yields 1, which means
that $s$ and $x$ are multipicative inverses.
If $n$ was not irreducible, this construction would
not work for all values of $x$, and we would have
a ring but not a field.

When calculating in ${\cal F}[x]$ mod $n(x)$,
I'll use the following Maxima function.
{\tt modulo} takes a rational function, an irreducible polynomial $n$,
and the variable $x$ used to construct the extension.  After splitting
the rational function into its numerator and denominator, it uses {\tt
gcdex} to invert the denominator modulo $n(x)$, then multiplies it by
the numerator and uses {\tt divide} to perform modulo reduction.

\begin{maximacommon}
modulo(r,n,x) := block(
   [d_inverse : gcdex(denom(r),n,x)[1]],
   divide(num(r) * d_inverse,n,x)[2]
);
\end{maximacommon}

We need to specify the variable in order to distinguish between
operations in ${\cal C}(x)[y]$ mod $n(y)$ and ${\cal C}(y)[x]$
mod $n(x)$.  For example, reducing mod $(y-x)$ effectively sets
$y$ equal to $x$, but it could also set $x$ equal to $y$:

\begin{maximablock}
modulo(x^3+y, y-x, y);
modulo(x^3+y, y-x, x);
\end{maximablock}


\begin{comment}
Again, like with the fraction field, I tend to be a bit loose with the
notation.  Something like the Gaussian integers, which I wrote as
${\bf Z}[i]; i^2=-1$, really should be expressed as equivalence
classes modulo the polynomial $i^2+1$, i.e.  ${\bf Z}[i]/(i^2+1)$.
\end{comment}

\vfill\eject
\mysection{Trace and Norm}
\qquad [van der Waerden], \S5.7

Given an algebraic extension $E$ of a field $F$, two of the
coefficients in the minimal polynomial have a special significance
that often makes them particularly useful.  Our only use of them will
come in Chapter 3 in the proof of Liouville's theorem.

First, let's note that while we used a minimal polynomial to
construct the extension field,
in fact, every element in the extension field
has a minimal polynomial associated with it,
which can be constructed by raising the element to successive
powers until one of the powers is a linear combination
of the lower powers.

\begin{key point}
\huge
\begin{equation*}
x^n + \underbrace{c_{n-1}}_{- \Tr(x)} x^{n-1} + \cdots + c_1 x + \underbrace{c_0}_{\mathclap{(-1)^n \N(x)}} = 0
\end{equation*}
\end{key point}

The special significance of these elements becomes more obvious if we
consider a {\it splitting field}, which is a further field extension
(an extension of the extension) in which the minimal polynomial
factors completely into linear factors.  The element $x$ is
itself one of the roots in these factors; the remaining
roots are called the {\it conjugates} of $x$.

Let's write $x$'s minimal polynomial in the form:

% $$\prod_{i=1}^n \left( t - x_i \right) = \sum_{k=0}^n (-1)^{k} a_k t^{n-k}$$

$$\prod_\sigma \left( t - \sigma x \right) = t^n - \sum_\sigma \sigma x t^{n-1} + \cdots + \prod_\sigma (-\sigma x) $$

where the sums and products are over all automorphisms that fix the base field.

Note that the $n-1^{\rm th}$ coefficient in the polynomial is the sum
of all the negatives of the conjugates, while the zeroth-order
coefficient coefficient in the polynomial, is the product of all the
negatives of the conjugates.

\definition
The {\it trace} of an element $x$ in $E$, written ${\rm Tr}(x)$, is
the sum of all the conjugates of $x$.
\enddefinition

\definition
The {\it norm} of an element $x$, written ${\rm N}(x)$, is the product
of all the conjugates of $x$.
\enddefinition

The significance of these two functions is first, that they map
elements from the extension field to the base field, and second, that
trace commutes with addition and norm commutes with multiplication.

\example Compute the norm and trace of $x+1$ in ${\bf Q}[x] \mod x^2-2$.

The field ${\bf Q}[x] \mod x^2-2$ consists of the rational numbers,
adjoined with $x=\sqrt{2}$.  Actually, $x$ is any square root of $2$.

$$(x+1)^2 = x^2+2x+1 \mod x^2-2 = 2x+3 = 2(x+1) + 1$$

$$(x+1)^2 - 2(x+1) - 1 = 0$$

$$\Tr (x+1) = 2 \qquad \N (x+1) = -1$$

$$\Tr \left(x + (x+1)\right) = \Tr x + \Tr (x+1) = 0 + 2 = 2 = \Tr 1 = 2$$

\endexample

\example Prove that $(1+2i)$ is irreducible in ${\bf Q}[i] \mod i^2+1$.

The field ${\bf Q}[i] \mod i^2+1$ consists of the rational numbers,
adjoined with $i$, the square root of $-1$.  These form the {\it
Gaussian integers}.

Since norm commutes with multiplication, any factorization of $(1+2i)$
must lead to a factorization of its norm, so let's compute the norm
of $(1+2i)$:

$$(1+2i)^2 = 1 +4i +4i^2 \mod i^2+1 = -3+4i = 2(1+2i) - 5$$

$$(1+2i)^2 - 2(1+2i) +5 =0 \mod i^2+1$$

So, $\N(1+2i) = 5$, which is prime.  Therefore, if $(1+2i)$ factors,
at least one of its factors must have norm $1$ or $-1$.  There
are no elements in this field with norm $-1$, and the only
elements with norm $1$ are $1$, $-1$, $i$, and $-i$, all
of which are units.

\endexample


\vfill\eject

\mysection{Factorization}

Some integers, as we know, are {\it prime} numbers, meaning that they
can not be factored into a product of smaller integers, and the primes
play a central role in number theory.  In a more abstract setting,
they generalize into the concepts of {\it irreducible elements} and
{\it prime elements}, which are similar, but not identical.
First, let me
first introduce the simpler concept of a {\it unit}:

\begin{key point}
A {\it unit} is an invertable element.
\end{key point}

Put another way, $u$ is a unit if there exists $v$ such that $uv=1$.
In a field, all elements except 0 are units, so units are only
meaningful in the context of a ring.

\begin{key point}
An {\it irreducible element} is a non-zero, non-unit element
that can not be written as the product of two non-units.
\end{key point}

Any divisor can be transformed into another divisor by multiplying it
by a unit, since if $uv=1$, then $ab=(ua)(vb)$ for any $a$ and $b$
whatsoever.  Therefore, factors are only unique up to units.  For
example, $x$ can be ``factored'' as $1 \cdot x$ or $(-1)\cdot(-1)\cdot x$,
but we don't regard these as factorizations for
determining irreducibility, since $1$ and $-1$ are units.

There exist rings in which indeterminates can be factored.

\example
In the ring ${\cal F}[x,y] \mod (y^2-x)$, the element $x$ is
not irreducible.

$x \equiv y^2 \mod (y^2-x)$, so $x$ can be factored as
$y\cdot y$.

Roughly speaking, $y$ is the square root of $x$.
\endexample

\begin{key point}
A {\it prime element} $p$ is a non-zero, non-unit element with
the property that when $p$ divides $ab$, $p$ divides either $a$ or $b$.
$$\forall a,b \quad p|ab \Rightarrow p|a \cup p|b$$
\end{key point}

There exist rings in which these two concepts do not coincide.

\example
In the ring ${\cal F}[x,y,z] \mod (z^2-xy)$, the element $z$ is
irreducible but not prime.  [Wikipedia]

First, note that $z$ is irreducible in this ring.

Then, consider the element $xy$.  $z$ divides $xy$ since
$xy \equiv z^2 \mod (z^2-xy)$, so $\frac{xy}{z} \equiv z \mod (z^2-xy)$.
However, $z$ divides neither $x$ nor $y$.
Therefore, $z$ is not prime.
\endexample



In particular, a greatest common divisor can be transformed into
another greatest common divisor by multiplying it by a unit.  I leave
without proof the claims that in ${\cal F}[x]$, the units are all
elements in ${\cal F}$, and that all GCDs differ from each other by a
unit multiple.

\subsection*{\qquad Unique Factorization}

\begin{key point}
A {\it unique factorization domain} ${\cal U}$
is an integral domain in which every non-zero element can be written as a product of a unit and prime elements.
[Wikipedia]
\end{key point}


The {\it Fundamental Theorem of Arithmetic} states that ${\mathbb Z}$
is a unique factorization domain.

\theorem

For any field $K$, the ring $K[x]$ is a unique factorization domain.

\proof

Assume the contrary, that we're working in a unique factorization
domain that is not an integral domain.  Pick two elements $c$ and $d$
which are divisors of zero, $cd=0$.  Obviously, we can pick $a=0$ and
$b=0$ and have $ab=0=cd$.  So, by U1, $x$ exists so that $0x=c$ or
$0x=d$, which by the zero theorem implies that either $c=0$ or $d=0$.
This proves I1.  Thus, a unique factorization domain is also an
integral domain.

\endtheorem

Also, ${\cal F}[x]$ is a unique
factorization domain (proof omitted).

Not all rings satisfy U1.  Consider, for example, ${\bf Z}[i];
i^2=-1$, the Gaussian integers.  This ring differs from the polynomial
ring ${\bf Z}[x]$ because polynomials of degree two and higher don't
exist since the square of $i$ is -1; $i$ is thus {\it algebraic} (see
below) and this makes all the difference.  The number 9 can be
factored two different ways in this ring: $9=3\cdot3=(4-i)(4+i)$.
It's not too hard to see that 3 can't be multiplied by any Gaussian
integer to form either $(4-i)$ or $(4+i)$, so U1 is not satisfied.
The Gaussian integers form an integral domain, but not a unique
factorization domain.

\subsection*{\qquad Algebraic Closure}
\qquad [van der Waerden], \S11

The most important way to characterize a field ${\cal F}$ is to extend
it to a polynomial ring ${\cal F}[x]$ and then study how polynomials
factor in ${\cal F}[x]$.

\begin{key point}
A field ${\cal F}$ is {\it algebraically closed} if all
polynomials in ${\cal F}[x]$ can be completely factored
into factors linear (i.e, first degree) in $x$.
\end{key point}

Put another way, a field ${\cal F}$ is algebraically closed if the
only irreducible polynomials in ${\cal F}[x]$ are linear in $x$.
These irreducible polynomials do not have to be linear in any other
indeterminate that might exist in ${\cal F}$.

A polynomial is {\it monic} if its leading coefficient is $1$.  If
${\cal F}$ is algebrically closed, then the monic irreducible
polynomials in ${\cal F}[x]$ are all of the form $(x-\lambda)$, where
$\lambda$ is some element of ${\cal F}$.

\begin{key point}
The {\bf Fundamental Theorem of
Algebra} states that the complex field ${\bf C}$ is algebraically
closed.
\end{key point}

There are several proof routes for this theorem.  The most
common involves complex analysis, and can be found in any standard
complex analysis text, usually near Liouville's Theorem on the
behavior of bounded entire functions.  I won't go into it here.

I do want to note the difference between ${\bf C}$ and ${\bf C}(x)$.
Both are fields.  ${\bf C}$ is algebraically closed because any
polynomial in ${\bf C}[x]$ can be completely factored into linear
factors, i.e, $x^3-3x^2+4=(x-2)^2(x+1)$.  ${\bf C}(x)$ is {\it not} algebraically closed.
If it were, then all polynomials in ${\bf C}(x)[y]$ could
be completely factored, but in fact, there exist polynomials
such as $y^2-x$ that can not be factored, so ${\bf C}(x)$
is not algebraically closed.

In this book, the complex field ${\bf C}$ is the most common field
that we'll use for our constants, precisely because it's algebraically
closed and the irreducible polynomials of ${\bf C}[x]$ can be so
easily characterized.  It's major disadvantage is that it isn't {\it
computable}, meaning that an arbitrary complex number can't be
expressed in a finite form.  This makes ${\bf C}$ fine for theoretical
proofs and for specific examples, but unsuitable for algorithms
designed to work with arbitrary inputs, for which smaller,
more complicated fields are required.

I use the algebraic closure of ${\bf Q}$ extensively.  This
option causes Sage to print elements of $\overline{\bf Q}$
using radical notation, if possible:

\begin{sageblock}
QQbar.options.display_format = 'radical';
\end{sageblock}

\begin{comment}

Here I will present another proof, Gauss's second method (cast into
modern terms by van der Waerden).  Not only is it more algebraic
in its flavor, but it is in fact a slightly stronger result.

Starting with unordered rings and fields, we introduce the concept of
{\it ordered} rings and fields field, where every element can be
compared to zero consistently with the axioms:

\begin{center}
\begin{supertabular}{l l l r}
   total order	& $\forall a,$ & one of $a<0, a=0, a>0$ holds &(O1)\cr
   consistency with negation & $\forall a,$ & one of $-a>0, a=0, a>0$ holds &(O1)\cr
   consistency with addition & $\forall a,b$ & $a>0 \cap b>0 \implies a+b>0$ &(O2)\cr
   consistency with multiplication & $\forall a,b$ & $a>0 \cap b>0 \implies ab>0$ &(O3)\cr
\end{supertabular}
\end{center}

I've introduced new logic symbols $\cap$ for logical and $\implies$
for logical implication.  $\cup$ is used for logical or.  Note
particularly that O2 implies that $n \cdot 1$ is always positive, never
zero, so our field characteristic is always zero.  No finite field can
be ordered.  This implies that our prime subfield is always ${\bf Q}$.
O3 implies that squares must be positive.

We further classify an ordered field as {\it Archemedian} if it
satisfies:

\begin{center}
\begin{supertabular}{l l @{} l r}
   Archemedian property	& $\forall a,$ & $\exists b \in {\bf Q}, b>a$ &(A1)\cr
\end{supertabular}
\end{center}

Non-Archemdian fields have ``infinitely large'' and ``infinitely
small'' numbers, larger and smaller than any rational number, and
${\bf Q}$ is confined to a narrow region of the field.  We will not
comsider such fields.

Now we define a {\it real field} as a field where -1 is not
expressible as a sum of squares.  A {\it maximal real field} is real
but has no proper algebraic extension that is real.  Any attempt to
algebraically extend a maximum real field would result in a non-real
field.  The prototypical example of a maximal real field is the
intersection of the algebraic closure of ${\bf Q}$ with the real line;
i.e, all purely real algebraic numbers.

We establish some key properties of an {\it ordered maximal real
field}, starting with the fact that all positive numbers are squares.
% Intuitively, this is because all positive real numbers have real
% roots, but we do not specialize to the real line, instead proofing the
% result for real fields in general.

Considering a positive element $p$ that is not a square, we construct
an algebraic extension $x^2-p$ which must produce an extension field
that where -1 can be expressed as a sum of squares, because the
original field was maximal real.  Furthermore, each element in the
extension field has the form $a + b \gamma$, where $\gamma$ solves
$x^2-p$ and $a$ and $b$ are in the original field.  So, we can write:

$$-1 = \sum_1^n (a_i + b_i \gamma)^2 = \sum_1^n (a_i^2 + 2 a_i b_i \gamma + b_i^2 p)$$

The $\gamma$ term must vanish, since -1 is in the original field, so

$$-1 = \sum_1^n a_i^2 + \sum_1^n b_i^2 p$$

This immediately implies that $p$ can not be expressed as a sum of
squares in the original field, because otherwise $-1$ could be.
Solving for $p$ we find that

$$p = \frac{-1 - \sum a_i^2}{\sum b_i^2}$$

Since squares are always positive, the expression on the right is
strictly negative, contradicting the assumption that $p$ is positive,
and proving the claim.

Every element is either a square, or the negative of a square, and
these cases are exclusive.  Thus, a maximal real field can be ordered
in only one way.  This also establishes that any sum of squares, being
positive, is a square and that adjoining $i$ creates a field where
square roots exist and thus every quadratic equation is solvable using
the quadratic formula.

Next, we want to establish that, in a maximal real field, all
polynomials of odd degree have at least one root.  We can proceed by
induction, as this is obviously true for linear polynomials, so
assuming that the theorem is satisfied for all polynomials of lower
degree, we can see that the polynomial must be irreducible, otherwise
we could factor it and apply induction to the factor of odd degree.

By adjoining a root $\alpha$ of the $n$ degree polynomial $f(x)$ we
get a non-real field:

$$-1 = \sum \phi_i(\alpha)^2 = \sum \phi_i(x)^2 + f(x) g(x)$$

All of the polynomials $\phi_i(x)^2$ have even degree, and the highest
degree terms can not cancel, since their coefficients are squares and
thus positive.  Thus, the sum has even degree of at most $2(n-1)$ and
$f(x)$ has odd degree $n$, so $g(x)$ must have odd degree of at most
$n-2$.  Induction implies that $g(x)$ must have a root $\beta$, so

$$\sum \phi_i(\beta)^2 = -1 - f(\beta) g(\beta) = -1$$

Yet $\beta$, and thus $\sum \phi_i(\beta)^2$, lies in the original
field, so this expression contradicts the claim that the original
field was real.

% $$(a_0 + a_1 \gamma + a_2 \gamma^2)^2 = a_0^2 + 2a_0 a_1 \gamma
% + (a_1^2 + 2a_0 a_2) \gamma^2 + 2a_1a_2\gamma^3 + a_2^2\gamma^4 $$
% $$= a_0^2 + 2a_0 a_1 \gamma
% + (a_1^2 + 2a_0 a_2) \gamma^2
% + 2a_1a_2(b_0 + b_1 \gamma + b_2\gamma^2)
% + a_2^2[b_0b_2 + (b_0 + b_1 b_2)\gamma + (b_1 + b_2^2)\gamma^2]
% $$
% $$= (a_0^2 + 2a_1a_2b_0 + a_2^2b_0b_2)
% + (2a_0 a_1 +2a_1a_2b_1 + a_2^2b_0 + a_2^2b_1b_2) \gamma
% + (a_1^2 + 2a_0 a_2
% + 2a_1a_2b_2
% + a_2^2b_1 + a_2^2b_2^2)\gamma^2
% $$

\theorem
Any field ${\cal F}$ where quadratic equations can always be solved
and polynomials of odd degree have at least one root is algebraically
closed.

\proof
Our main concern is polynomials of even degree.  Consider a splitting
field where all of the polynomial's roots $\alpha_i$ exist, construct
the $n(n-1)/2$ values $r_{ij} = \alpha_i + \alpha_j +
c \alpha_i \alpha_j$, where $c$ has been selected to make all of these
values distinct, and then form the polynomial $\Lambda(x) = \prod (x-r_{ij})$,
which exists in ${\cal F}[x]$ since it's symmetric.  Note that this
polynomial's degree, while larger, has one less power of two than the
original.  We repeat this procedure until we get a polynomial of odd
degree in ${\cal F}[x]$, which has at least one root in ${\cal F}$,
say $r_{ij}$.  If neither $\alpha_i + \alpha_j$ nor
$\alpha_i \alpha_j$ existed in the field, then by the Primitive
Element Theorem we could extend by $r_{ij}$, so at least one of these
two terms must exist in the field, and then the other can be
constructed.  Finally, we use the quadratic equation to construct
$\alpha_i$ and $\alpha_j$, and repeat this until we've got roots of
the original polynomial.

NEED SYMMETRIC POLYNOMIALS AND THEOREM OF PRIMITIVE ELEMENT

\endtheorem

There is only one way to extend an ordered integral domain into an
ordered quotient field.

Next, we introduce real numbers individually as Cauchy sequences and
collectively as an extension field, ordered by showing that the
trailing terms in such a sequence must either converge to zero or
become uniformly positive or negative, establishing O1.  A little more
work is then needed to show that O2 and O3 hold, and that the
Archemedian property carries through.

Next, we show that square roots always exist, as Cauchy sequences, for
all positive numbers.  This requires constructing a sequence of larger
and larger numbers, each of which we square, invert, multiply by 2,
and use A1 to demand an even larger element of {\bf Q}.  Fill in the
details.  Might need Archmedian property for this.

Next, construct ${\bf C}$ by extending algebraically to adjoin $i$,
and use the prior result to demonstrate that square roots exist for
all numbers in ${\bf C}$.  Then introduce the modulus of a complex
number and establish that $|1+\gamma|\le 1+|\gamma|$.

\end{comment}

\subsection*{\qquad Square-free factorization}

\hbox{}\qquad [Ge92], \S 8.2

A {\it square-free polynomial} is one with no repeated factors.
$x^2-1$ is square-free because it factors as $(x-1)(x+1)$.  $x^2+2x+1$
is not square-free because it factors as $(x+1)^2$.

Whether or not a polynomial is square-free is independent of the field
in which the factorization occurs.

A {\it square-free factorization} of a polynomial is a factorization
into square-free factors, each of which appears at a different power.
It is much easy to compute than a full factorization into irreducible
factors and for this reason will be quite useful to us.

Surprisingly, a polynomial's square-free factorization is independent
of its algebraic system!  For example, $x^2+1$ is irreducible in ${\bf
R}[x]$, so its square-free factorization is simply $x^2+1$.  On the
other hand, in ${\bf C}[x]$, $x^2+1$ factors as $(x+i)(x-i)$.  Yet
both of these factors combine together in the square-free
factorization (since they both appear to the first power), so
$x^2+1$'s square-free factorization in ${\bf C}[x]$ is\ldots $x^2+1$.

To compute square-free factorizations, we'll use an operation that,
for lack of a better word, I'll call ``differentiation.''  We
``differentiate'' a polynomial by multiplying each term by its power
and then lowering the power by one.

This ``differentiation'' not to be confused with the field operation
that I will define in the next chapter.  ``Differentiation'' is simply
a mechanical procedure of lowering powers and multiplying by
constants.  In particular, no attempt is made to ``differentiate''
the coefficients {\it even if they are not constants}.

To compute a square-free factorization, first we ``differentiate'' the
polynomial.  The result is a second polynomial with the degree of all
factors reduced by one.  Note in particular that any factors of unit
degree (and only those factors) disappear completely.  Dividing this
into the original polynomial, we obtain a polynomial with no square
factors --- all factors now appear with unit degree.  Computing the
GCD of this polynomial with the original one also produces a
polynomial with only factors of unit degree, except that the original
unit degree factors are missing.  We can divide this last two
polynomials into each other to determine the original unit degree
factors.  Going back to the ``differentiation'' step, we can keep
repeating the process until we have obtained all the square-free
factors.


\subsection*{Full polynomial factorization (optional)}

\qquad [van der Waerden], \S5.6
\hbox{}\qquad [Ge92], Chs. 5, 6, 8

Let's conclude this chapter by taking a least a brief look at fully
factoring a polynomial into its irreducible factors.  There are
several reasons to do this.

First of all, it's easy to declare that ``the Fundamental Theorem of
Algebra tells us that any polynomial in {\bf C}[x] can be factored
into linear factors'', and maybe even prove it.  That's true, but when it comes
time to actually do a computation, how do we proceed?  How do we
actually factor a polynomial?  Call it the price of success.
Differential algebra is solid enough to actually compute integrals, so
existence theorems don't cut it.  We need constructive algorithms.

Second, it's a surprisingly difficult problem.  An appreciation of its
difficulty now will motivate the discussion later when I show various
techniques that have been developed to avoid full factorization
whenever possible.  Yet the fact remains that it is at times
unavoidable.

Finally, both techniques that I will discuss here work according to a
basic principle that we'll use again later in a more advanced context,
so it makes sense to present it now in a simpler form.  Specifically,
we'll solve a difficult problem in an algebraic system by using a
homomorphism to map into a different algebraic system where we can
solve the problem more easily, then find some way of ``lifting'' this
answer back into the original system.  This is one of the most
powerful solution methods in algebra, and has been used to solve
problems once thought impossible.

Let's start simple.  We want to factor a polynomial in ${\bf Z}[x]$,
the ring of polynomials with integer coefficients.  If such a
polynomial has a factorization in ${\bf Z}[x]$, for example
$x^2-1=(x+1)(x-1)$, we want to find it.  If it has no such
factorization, for example $x^2+1$ (which would require at least ${\bf
Z}[i,x]; i^2=-1$ to factor), we want to prove this.

Now consider what happens when we set $x$ equal to some specific
integer value, say $a$.  Any polynomial in ${\bf Z}[x]$ will be
transformed into an integer.  Thus, we have a mapping $\phi_{x-a}:
{\bf Z}[x] \rightarrow {\bf Z}$ from polynomials to integers.  Not
only is this a mapping, but it is a {\it homomorphism}, a mapping that
preserves the operations, so $\phi_{x-a} (m+n) = \phi_{x-a}m \,\hat+\,
\phi_{x-a}n$ and $\phi_{x-a} (m\cdot n) = \phi_{x-a}m \,\hat\cdot\,
\phi_{x-a}n$, where I have used $\hat+$ and $\hat\cdot$ to emphasize
that these operations are operations in ${\bf Z}$, and distinct from
$+$ and $\cdot$, which are operations in ${\bf Z}[x]$.\footnote{The
symbols $\cdot$ and $\hat\cdot$ represent multiplication, which we normally
omit entirely, but I have written explicitly here to make this point.}
I leave it an exercise to actually prove this is a homomorphism.

Since $\phi_{x-a}$ is a homomorphism, any factorization of a
polynomial in ${\bf Z}[x]$ must map into a factorization of its image
integer in {\bf Z}.  In other words, if a polynomial factors into
smaller polynomials (all with integer coefficients), then setting the
variable equal to some specific integer causes all the polynomials to
evaluate into integers, which must themselves factor.  Consider
$x^2-1=(x+1)(x-1)$.  If we set $x=2$ (the evaluation homomorphism
$\phi_{x-2}$), then the equation becomes $3=3\cdot1$.  This happens
irregardless of our choice of integer.  Choosing $x=3$ ($\phi_{x-3}$)
transforms $x^2-1=(x+1)(x-1)$ into $8=4\cdot2$.

Thus, we have our homomorphism, which maps our problem from ${\bf
Z}[x]$ into ${\bf Z}$ and transforms the factorization of polynomials
into a factorization of integers.  Although factoring integers is
certainly not trivial (the security of the RSA cryptosystem depends on
its near impossibility for large numbers), it is much easier than
factoring polynomials.  Not only easier, but {\it finite}.  There are
only a certain number of ways any given integer can factor, and for
relatively small integers, we can enumerate them by computing a prime
factorization and then listing the finite number of ways that the
primes can be combined into factors.  The number $3$, for example, can
be split into two integer factors in only one of four ways: $3\cdot1$,
$1\cdot3$, $-3\cdot-1$, and $-1\cdot-3$.

\begin{comment}

\mysection{Axiom Soup}

At this point, the reader might begin to suspect that we build up a
theory from our axioms, and whenever we get stuck, we introduce a new
axiom so that we can move forward!  In a sense, this is true, but
don't miss an important point.  While the axioms are axioms in the
sense that you can't prove U1 or I1 for an arbitrary ring (i.e, just
given the R axioms), they are also theorems in the sense that we can
prove them for the particular systems of interest to us.  We prove the
axioms both for our base system ${\bf Z}$ (the integers form a unique
factorization domain), and for any constructed system ${\cal F}[x]$ (a
polynomial ring in a single {\it transcendental} variable over a field
is also a unique factorization domain).

Let me close this chapter by proving this in a series of theorems.

\end{comment}

\vfill\eject
\mysection{Exercises}

Factor the following polynomials in ${\bf Z}[x]$, ${\bf Q}[x]$, ${\bf R}[x]$, and ${\bf C}[x]$:

\begin{enumerate}
\item $x^2-1$  (factors in all four rings)
\item $4x^2-1$ (factors in all rings except ${\bf Z}[x]$)
\item $x^2-2$  (factors in ${\bf R}[x]$ and ${\bf C}[x]$, but not in ${\bf Z}[x]$ or ${\bf Q}[x]$)
\item $x^2+1$  (factors only in ${\bf C}[x]$)
\end{enumerate}

Write a computer program to factor the following polynomials:

\begin{maximacode}
declare(x,mainvar)$
factorization_problem(p) := block(
   ?princ(concat("$", tex1(expand(p)), "$")),
   ?format(true, "~%~%"),
   ?princ(concat("\\textcolor{blue}{\\tt factor(", string(expand(p)), ")}")),
   ?format(true, "~%~%"),
   ?princ(concat("{Ans: $", tex1(factor(p)), "$}"))
)$
\end{maximacode}

\begin{enumerate}[resume]

\item \maximac{factorization_problem((x^2+x+1)*(x^3+x^2-1))$}

\item \maximac{factorization_problem(34*x^5+51*x^4+60*x^3+25*x^2+8*x-1)$}

\item \maximac{factorization_problem((x^2+x+1)*(x^3+x^2-1))$}


\end{enumerate}

Write a computer program to factor the following polynomials:\hfil\break
\-\hspace{1cm} (Hint: You'll need a 2 dimensional grid)

\begin{enumerate}[resume]

\item \maximac{factorization_problem((3*y+2*x-7)*(7*y+x**2+11))$}

\item \maximac{factorization_problem((y**3+2*x**2-3)*(y+x**2+1))$}

\end{enumerate}




\begin{comment}

% This is some older code that uses tricks with labels[]

\begin{maximacode}
grind: true$
\end{maximacode}

\item $\maximac{expand((x^2+x+1)*(x^3+x^2-1));}$

% labels[1] is the current input label
% labels[2] is the previous output label
% labels[3] is the previous input label

% labels[3] is the previous input label (something like %i31).  We
% convert it to a string (something like '%i31'), evaluate it to get
% the previous input, and output it as a string.

% The net result is to typeset the previous Maxima input command in a
% verbatim block

\textcolor{blue}{\maximac{string(eval_string(string(labels[3])));}}

Ans: $\maximac{eval_string(\%);}$

\item $\maximac{expand((x^2+x+1)*(x^3+x^2-1));}$

% This prints the factored polynomial as TeX
% $\maximac{factor(eval_string(string(labels[2])));}$

% This typesets the previous OUTPUT as grind verbatim
\maximac{string(eval_string(string(labels[2])));}

\maximac{concat("factor(", eval_string(string(labels[2])), ");");}

$\maximac{eval_string(eval_string(string(labels[2])));}$

\end{comment}
