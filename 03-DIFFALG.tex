
\setcounter{chapter}{2}
\mychapter{Differential Algebra}

\section{Differential Fields}

The advent of the modern, axiomatized approach to mathematics at the
turn of the twentieth century led directly to the development of
abstract algebra, with its rings and fields, in the 1920s.  By 1940, a
Columbia University professor named J.F. Ritt had proposed the
concepts of {\it differential rings} and {\it differential fields}.
They are exactly analogous to ordinary rings and fields, except that
they are equipped with a third basic operator (addition and
multiplication are the first two), called {\it derivation}.  A
derivation is a unary operator (the other two are binary), which we
shall typically denote by $D$.  Since algebra is fundamentally
concerned with how operators commute with each other, the first
question we are lead to ask are, ``How does derivation commute with
addition and multiplication?''  The answer is to be found in two basic
axioms:

\begin{center}
\begin{supertabular}{l l l r}
   addition law of derivations	& $\forall a,b \in {\cal D},$ & $D(a+b) = Da+Db$ &(D1)\cr
   multiplication law of derivations	& $\forall a,b \in {\cal D},$ & $D(ab) = aDb+bDa$ &(D2)\cr
\end{supertabular}
\end{center}

Neither axiom should come as any great surprise.  After all, these are
just the basic addition and multiplication rules we learned in first
year calculus.  Yet note how they are being presented; not as results
derived from some theorem involving fractions and limits, but as
axioms that are assumed true from the start.  One of the great themes
of differential algebra is that we purge from the subject almost any
mention of limits; for us, derivation is just a mapping in a field
that carries an object $a$ to another object $b$.  Integration, then,
is little more than the inversion of derivation: given an object $b$,
can we find an object $a$ which maps into $b$?

Yet the connection to calculus should be made clear.  Since derivation
(in the calculus sense) obeys these two axioms for derivation (in the
algebra sense), the calculus derivation will always behave as an
algebra derivation, so any theory we develop for the algebra
derivation will apply immediately to the calculus version.

What can we determine from these two axioms?  A surprising lot,
in my opinion.

\vfill\eject

\theorem

$D(0) = 0$, $D(1) = 0$, and $\displaystyle{D({1\over a}) = - {1\over a^2}D(a)}$

\proof

$$D(0) = D(0) + D(0) - D(0) = D(0+0) - D(0) = D(0) - D(0) = 0$$

$$D(1) = D(1\cdot1) = D(1) + D(1)$$

$$D(1) = D(1) + D(1) - D(1) = D(1) - D(1) = 0$$

$$0 = D(1) = D(a\cdot{1\over a}) = {1\over a} D(a) + a D({1\over a})$$

$$ a D({1\over a}) = - {1\over a}D(a)$$

$$ D({1\over a}) = - {1\over a^2}D(a)$$

\endtheorem

\theorem

The derivation of an element algebraic over a subfield is completely
defined by the subfield's derivation.

\proof

The algebraic extension is governed by a primitive polynomial of the form:

$$\sum_i a_i \xi^i = 0$$

Differentiating this polynomial (using the D1 and D2 axioms), we obtain:

$$\sum_i (a_i' \xi^i + i a_i \xi^{i-1} \xi') = 0 $$

$$\sum_i i a_i \xi^{i-1} \xi' = - \sum_i a_i' \xi^i $$

$$\xi' = - {\sum_i a_i' \xi^i \over {\sum_i i a_i \xi^{i-1}}} $$


Now, by theorem ??, any element $r \in {\cal A}$ can be written:

$$r = b_n \xi^n + \cdots + b_1 \xi + b_0 = \sum_i b_i \xi^i$$

Differentiating $r$ (again using the D1 and D2 axioms), we obtain:

$$r' = \sum_i (b_i' \xi^i + i b_i \xi^{i-1} \xi') $$

Since we already have an expression for $\xi'$, this equation
gives the derivation of any element in ${\cal A}$.

\endtheorem

It follows immediately from these theorem that our entire prime
subfield, as well as any purely algebraic extension thereof, must map
to zero under derivation.

THEOREM

The set of all elements in a differential field which map to zero
under derivation forms a subfield.

END THEOREM

The subfield which maps to zero is called the {\it constant subfield}.
It necessarily includes the prime subfield and any elements algebraic
over the prime subfield, but may include other transcendental elements
as well.  For example, consider ${\bf R}$, the real numbers.  $2$ is
in the prime subfield, so $D(2)=0$; $\sqrt{2}$ is algebraic over the
prime subfield, so $D({\sqrt 2})=0$; $\pi$ is transcendental over the
prime subfield, so doesn't {\it have} to map to zero, but we will
(obviously) set $D(\pi)=0$.  All three elements --- $2$, ${\sqrt 2}$,
$\pi$ --- are in the constant subfield.

The upshot of all this is that our basic D1 and D2 axioms completely
define a derivation both for our prime subfield as well as any purely
algebraic extensions.  It therefore follows that we need only specify
the behavior of a derivation on transcendental elements and we will
have completely defined the derivation.

We will use four types of transcendental elements in our theory:

1. Constants.  $D(c)=0$

2. The distinguished variable of integration, $x$.  $D(x)=1$

Since this is an O.D.E. theory, and particularly an integration
theory, we are always integrating with respect to some variable of
integration.  There is no loss of generality in labeling it $x$.  By
setting $D(x)=1$ we establish that our derivation is in fact a
derivative and not a differential.

Incidently, Ritt had already conceived back in the 1940s of equipping
a differential field with multiple derivations, one for each of a set
of independent variables.  This corresponds nicely to what should be
needed for a P.D.E. theory.  Thus, given variables $x$, $y$ and $z$,
we could construct derivatives $D_x$, $D_y$ and $D_z$ so that
$D_x(x)=1$, $D_x(y)=0$, $D_x(z)=0$ and so on.  I definitely consider
this approach to be ripe ``for further study,'' but since our focus is
on integration, I'll have nothing more to say about fields with
multiple derivations.

3. Logarithmic extensions. $D(\theta)={D(\phi)\over{\phi}}$

4. Exponential extensions. $D(\theta)=\theta D(\phi)$

These two clearly correspond to $\theta = \ln\phi$ (in the logrithmic
case) and $\theta = \exp\phi$ (in the exponential case).  The key
point I want to made immediately is that these are {\it
transcendental} extensions\ldots and not all logarithms and
exponentials are transcendental!  Transcendental extensions are
defined by exclusion --- any extension that isn't algebraic is
transcendental.  If we're dealing with an algebraic extension, even if
defined using logarithms and exponentials, we have to use our
algebraic theory.

\example Represent $\displaystyle {{4^x+1}\over{2^x+1}}$ in a form suitable for Risch Integration.

There are three ways to do this --- the easy way, the hard way, and the wrong way.

Let me first note that $4^x = (2^2)^x = (2^x)^2$.  The existence of
this algebraic relationship between $4^x$ and $2^x$ means that we {\it
can not} use two seperate transcendental extensions.  So this:

$${{\theta+1}\over{\phi+1}}; \theta = \exp(x \ln 4); \phi = \exp(x \ln 2)$$

is the {\it wrong} way.

The {\it easy} way is to set up $2^x$ first and then construct $4^x$
as its square:

$${{\phi^2+1}\over{\phi+1}}; \phi = \exp(x \ln 2)$$

You can also do this the {\it hard} way, setting up $4^x$ first and
then using an additional algebraic extension to get its square root,
$2^x$:

$${{\theta+1}\over{\phi+1}}; \theta = \exp(x \ln 4); \phi^2 = \theta$$

\endexample

That's it!  The basic two differential axioms, algebraic extensions,
quotient fields, and these four types of transcendentals, round out
the entire base algebraic structure we'll need to construct our
theory.  We do need to be careful, though, as the last example
illustrated.  In these simple examples, figuring out which elements
are algebraic and which are transcendental is easy, but in more
complex expressions this may not be obvious.  We'll discuss in Chapter
?? how to test new elements for transcendence.

What about sines and cosines, all those arc-functions, raising things
to powers, and all that?  Turns out we can express all those
operations using just our basic extensions.  The key here is Euler's
famous identity $e^{i\theta}=i\sin\theta+\cos\theta$.

\example

Express $\sin x$ in a form suitable for Risch Integration.

Euler's identity immediately gives:

$$\sin x = {-i \,{{e^{ix} - e^{-ix}}\over 2}}$$

Therefore, starting from ${\bf C}(x)$,
we add the exponential extension $\theta = \exp(ix)$,
and conclude that $\sin x$ can be expressed as the rational function:

$${{\theta^2 - 1}\over 2i\theta}$$

in the field ${\bf C}(x,\theta); \theta=\exp(ix)$.

\endexample

If trigonometric functions can be represented using complex
exponentials, then it should come as no real surprise that inverse
trigonometric functions can be represented with complex logarithms.

\example Represent $\arcsin x$ in a form suitable for Risch Integration.

Let's start with Euler's identity and take its logarithm:

$$e^{i\theta}=i\sin\theta+\cos\theta$$

$$i\theta=\ln(i\sin\theta+\cos\theta)$$

Now, if $\theta = \arcsin x$, then $x = \sin \theta$, and we can use
the basic $\sin^2 \theta + \cos^2 \theta = 1$ identity to compute
$\cos \theta = \sqrt{1-\sin^2\theta} = \sqrt{1-x^2}$.  Substituting above:

$$i\theta=\ln(i x+\sqrt{1-x^2})$$

$$\theta=-i\ln(i x+\sqrt{1-x^2})$$

$$\arcsin x=-i\ln(i x+\sqrt{1-x^2})$$

Thus, we need first an algebraic extension to construct $\phi = \sqrt{1-x^2}$,
followed by a logarithm extension to construct $\arcsin x = -i\ln(ix+\phi)$.

\endexample

I think the details of further constructions along these lines are
straightforward enough that I will simply summarize them in a table.

\vfill\eject

\section{Root/Logarithm/Exponential Form}


\def\sech{{\rm sech}}
\def\csch{{\rm csch}}

\begin{center}
\begin{tabular}{c c c c @{\bigskip}}
Elementary expression & \multicolumn{1}{c}{Expansion} &
Elementary expression & \multicolumn{1}{c}{Expansion} \\
\hline
& \\
$f^g$ & $\displaystyle e^{\,g \ln f}$ &
 & \\
$\sin x$ & $\displaystyle {-i \,{{e^{ix} - e^{-ix}}\over 2}}$ \vbox to20pt{}&
 $\sinh x$ & $\displaystyle {{e^{x} - e^{-x}}\over 2}$ \vbox to20pt{} \\
$\cos x$ & $\displaystyle {{e^{ix} + e^{-ix}}\over 2}$ &
 $\cosh x$ & $\displaystyle {{e^{x} + e^{-x}}\over 2}$ \vbox to20pt{} \\
$\tan x$ & $\displaystyle {-i \,{{e^{ix}-e^{-ix}}\over {e^{ix}+e^{-ix}}}}$ &
 $\tanh x$ & $\displaystyle {{e^{x}-e^{-x}}\over {e^{x}+e^{-x}}}$ \\

$\sec x$ & $\displaystyle {2\over{e^{ix} + e^{-ix}}}$ &
 $\sech x$ & $\displaystyle {2\over{e^{x} + e^{-x}}}$ \vbox to20pt{} \\
$\csc x$ & $\displaystyle {{2i}\over{e^{ix} - e^{-ix}}}$ \vbox to20pt{}&
 $\csch x$ & $\displaystyle {2\over{e^{x} - e^{-x}}}$ \vbox to20pt{} \\
$\cot x$ & $\displaystyle {i \,{{e^{ix}+e^{-ix}}\over {e^{ix}-e^{-ix}}}}$ &
 $\coth x$ & $\displaystyle {{e^{x}+e^{-x}}\over {e^{x}-e^{-x}}}$ \\

$\arcsin x$ & $\displaystyle -i \,\ln (ix + \sqrt{1-x^2})$ &
 $\sinh^{-1} x$ & $\displaystyle \ln (x + \sqrt{x^2+1})$ \\
$\arccos x$ & $\displaystyle -i \,\ln (x + i\sqrt{1-x^2})$ &
 $\cosh^{-1} x$ & $\displaystyle \ln (x + \sqrt{x^2-1})$ \\
$\arctan x$ & $\displaystyle {1\over2}\,i\,\ln {{ix-1}\over{ix+1}}$ &
 & \\

$\sec^{-1} x$ & $\displaystyle -i \,\ln {{1 + i\sqrt{x^2-1}}\over{x}}$ &
 & \\
$\csc^{-1} x$ & $\displaystyle -i \,\ln {{i + \sqrt{x^2-1}}\over{x}}$ &
 & \\
$\cot^{-1} x$ & $\displaystyle {1\over2}\,i\,\ln {{i+x}\over{i-x}}$ &
 & \\

\end{tabular}
\end{center}

\vfill\eject

\section{Liouville's Theorem}

The next problem we must confront is to limit the number of possible
fields in which we can find solutions to our problem.  So far, we have
seen how to construct an algebraic system to express any elementary
function, but there are an infinity of such systems.  Searching them
exhaustively for the solution to a given integral is out of the
question.  Fortunately, it's been known for almost 200 years that
there are severe restrictions on what extensions can appear in an
integral above and beyond those used in the original integrand.

For example, consider the expression $e^x$.  Differentiating it
yields, well, $e^x$.  Now the key thing to note is that the
exponential does not disappear after differentiation.  This, in fact,
is a general property of exponentials --- differentiation never makes
them disappear.  They can change around, to be sure,
${d\over dx}e^{2x}=2e^{2x}$, but notice that the exponential is still
present in the result.  Therefore, since the solution to our integral
must differentiate into the original integrand, we conclude that no
new exponentials can appear in the integral beyond those in the
integrand.  If there were new exponentials in the result, then they
would have to appear in the integrand as well, since they can never
disappear under differentiation.

The same thing happens with roots.  Differentiate $\sqrt{x}$ and you
get ${1\over{2\sqrt{x}}}$.  This time the root moves from the
numerator to the denominator, but again, it doesn't completely
disappear.  This is a general property of roots, algebraic extensions
in general, in fact.

Logarithms are different, though.  Differentiate $\ln x$ to get
$1\over x$.  The logarithm is gone.  So new {\it logarithms} can
appear in integrals, because they can disappear under differentiation
to recover the original integrand.  Even here, though, there are
important restrictions.  The logarithms have to appear with constant
coefficients (because something like $x\ln x$ would differentiate into
$1 + \ln x$), can not appear in powers or in denominators (${d\over
dx} \ln^2x = 2{\ln x\over x}$), and can not be nested (${d\over dx}
\ln(\ln x) = {1\over{x\ln x}}$).

Of course, all of this is hand-waving so far, but I hope that it
provides context and concrete examples to understand what has become
known as {\it Liouville's Theorem} --- the only new extensions that
can appear are simple logarithms.  Let's nail this down, now.

\definition

A {\it trivial} element in an extension is one that only involves
terms from the underlying field.  For example, in the field ${\bf
C}(x)$, the only trivial elements are those in ${\bf C}$.
Anything involving $x$ is non-trivial.

\enddefinition

\theorem

Differentiating a non-trivial element in an algebraic extension always
produces a non-trivial element.

\proof

Consider an algebraic extension ${\cal A} = {\cal F}(\xi)$.
By theorem ??, any element $r \in {\cal A}$ can be written:

$$r = a_n \xi^n + \cdots + a_1 \xi + a_0 = \sum_i a_i \xi^i$$

where $a_n$ is non-zero and $n \ne 0$ (otherwise the element would be
trivial), and $r'$ is:

$$r' = \sum_i (a_i' \xi^i + i a_i \xi^{i-1} \xi') $$

$$r' = \sum_i a_i' \xi^i + (\sum_i i a_i \xi^{i-1}) \xi' $$

$$r' = \sum_i a_i' \xi^i - (\sum_i i a_i \xi^{i-1}) {\sum_i m_i' \xi^i \over {\sum_i i m_i \xi^{i-1}}} $$

\endtheorem

EXAMPLE

Explain the ``disappearance'' of the square root in:

$$\int{1\over\sqrt{1-x^2}} = \arcsin x$$

Finding $\arcsin x$ in the table, we see that:

$$\arcsin x = -i \,\ln (ix + \sqrt{1-x^2})$$

That's where it went!  It ``disappeared'' into the complex logarithm
that $\arcsin x$ is formed from.  New logarithms, of course, are
acceptable.  Notice that the new logarithm has a constant coefficient
($-i$), is not nested, and appears to the first power.
