
\setcounter{chapter}{2}
\mychapter{Differential Algebra}

\section{Differential Fields}

The advent of the modern, axiomatized approach to mathematics at the
turn of the twentieth century led directly to the development of
abstract algebra, with its rings and fields, in the 1920s.  By 1940, a
Columbia University professor named J.F. Ritt had proposed the
concepts of {\it differential rings} and {\it differential fields}.
They are exactly analogous to ordinary rings and fields, except that
they are equipped with a third basic operator (addition and
multiplication are the first two), called {\it derivation}.  A
derivation is a unary operator (the other two are binary), which we
shall typically denote by $D$.  Since algebra is fundamentally
concerned with how operators commute with each other, the first
question we are lead to ask are, ``How does derivation commute with
addition and multiplication?''  The answer is to be found in two basic
axioms:

\begin{center}
\begin{supertabular}{l l l r}
   addition law of derivations	& $\forall a,b \in {\cal D},$ & $D(a+b) = Da+Db$ &(D1)\cr
   multiplication law of derivations	& $\forall a,b \in {\cal D},$ & $D(ab) = aDb+bDa$ &(D2)\cr
\end{supertabular}
\end{center}

Neither axiom should come as any great surprise.  After all, these are
just the basic addition and multiplication rules we learned in first
year calculus.  Yet note how they are being presented; not as results
derived from some theorem involving fractions and limits, but as
axioms that are assumed true from the start.  One of the great themes
of differential algebra is that we purge from the subject almost any
mention of limits; for us, derivation is just a mapping in a field
that carries an object $a$ to another object $b$.  Integration, then,
is little more than the inversion of derivation: given an object $b$,
can we find an object $a$ which maps into $b$?

Yet the connection to calculus should be made clear.  Since derivation
(in the calculus sense) obeys these two axioms for derivation (in the
algebra sense), the calculus derivation will always behave as an
algebra derivation, so any theory we develop for the algebra
derivation will apply immediately to the calculus version.

What can we determine from these two axioms?  A surprising lot,
in my opinion.

% \vfill\eject

\theorem\label{basic difalg}

\begin{itemize}
\item $D(0) = 0$
\item $D(1) = 0$
\item $\displaystyle{D({1\over a}) = - {1\over a^2}D(a)}$
\item $D(cx) = c\, D(x)$ if $D(c)=0$
\end{itemize}

\proof

$$D(0) = D(0) + D(0) - D(0) = D(0+0) - D(0) = D(0) - D(0) = 0$$

$$D(1) = D(1\cdot1) = D(1) + D(1)$$

$$D(1) = D(1) + D(1) - D(1) = D(1) - D(1) = 0$$

$$0 = D(1) = D(a\cdot{1\over a}) = {1\over a} D(a) + a D({1\over a})$$

$$ a D({1\over a}) = - {1\over a}D(a)$$

$$ D({1\over a}) = - {1\over a^2}D(a)$$

\endtheorem

It follows immediately from this theorem that our entire prime
subfield, as well as any purely algebraic extension thereof, must map
to zero under derivation.

\theorem

The set of all elements in a differential field which map to zero
under derivation forms a subfield.

\endtheorem

The subfield which maps to zero is called the {\it constant subfield}.
It necessarily includes the prime subfield and any elements algebraic
over the prime subfield, but may include other transcendental elements
as well.  For example, consider ${\bf R}$, the real numbers.  $2$ is
in the prime subfield, so $D(2)=0$; $\sqrt{2}$ is algebraic over the
prime subfield, so $D({\sqrt 2})=0$; $\pi$ is transcendental over the
prime subfield, so doesn't {\it have} to map to zero, but we will
(obviously) set $D(\pi)=0$.  All three elements --- $2$, ${\sqrt 2}$,
$\pi$ --- are in the constant subfield.

\theorem

The derivation of an element algebraic over a subfield is completely
defined by the subfield's derivation and the element's minimal polynomial.

\proof

Given an element $\xi$, let its minimal polynomial be:

$$\sum_i a_i \xi^i = 0$$

Differentiating this polynomial (using the D1 and D2 axioms), we obtain:

$$\sum_i (a_i' \xi^i + i a_i \xi^{i-1} \xi') = 0 $$

$$\sum_i i a_i \xi^{i-1} \xi' = - \sum_i a_i' \xi^i $$

$$\xi' = - {\sum_i a_i' \xi^i \over {\sum_i i a_i \xi^{i-1}}} $$

\endtheorem

The upshot of all this is that our basic D1 and D2 axioms completely
define a derivation both for our prime subfield as well as any purely
algebraic extensions.  It therefore follows that we need only specify
the behavior of a derivation on transcendental elements and we will
have completely defined the derivation.

We will use four types of transcendental elements in our theory:

1. Constants.  $D(c)=0$

2. The distinguished variable of integration, $x$.  $D(x)=1$

Since this is an O.D.E. theory, and particularly an integration
theory, we are always integrating with respect to some variable of
integration.  There is no loss of generality in labeling it $x$.  By
setting $D(x)=1$ we establish that our derivation is in fact a
derivative and not a differential.

Incidently, Ritt had already conceived back in the 1940s of equipping
a differential field with multiple derivations, one for each of a set
of independent variables.  This corresponds nicely to what should be
needed for a P.D.E. theory.  Thus, given variables $x$, $y$ and $z$,
we could construct derivatives $D_x$, $D_y$ and $D_z$ so that
$D_x(x)=1$, $D_x(y)=0$, $D_x(z)=0$ and so on.  I definitely consider
this approach to be ripe ``for further study,'' but since our focus is
on integration, I'll have nothing more to say about fields with
multiple derivations.

3. Logarithmic extensions. $D(\theta)={D(\phi)\over{\phi}}$

4. Exponential extensions. $D(\theta)=\theta D(\phi)$

These two clearly correspond to $\theta = \ln\phi$ (in the logrithmic
case) and $\theta = \exp\phi$ (in the exponential case).  The key
point I want to made immediately is that these are {\it
transcendental} extensions\ldots and not all logarithms and
exponentials are transcendental!  Transcendental extensions are
defined by exclusion --- any extension that isn't algebraic is
transcendental.  If we're dealing with an algebraic extension, even if
defined using logarithms and exponentials, we have to use our
algebraic theory.

\example Represent $\displaystyle {{4^x+1}\over{2^x+1}}$ in a form suitable for Risch Integration.

There are three ways to do this --- the easy way, the hard way, and the wrong way.

Let me first note that $4^x = (2^2)^x = (2^x)^2$.  The existence of
this algebraic relationship between $4^x$ and $2^x$ means that we {\it
can not} use two seperate transcendental extensions.  So this:

$${{\theta+1}\over{\phi+1}}; \theta = \exp(x \ln 4); \phi = \exp(x \ln 2)$$

is the {\it wrong} way.

The {\it easy} way is to set up $2^x$ first and then construct $4^x$
as its square:

$${{\phi^2+1}\over{\phi+1}}; \phi = \exp(x \ln 2)$$

You can also do this the {\it hard} way, setting up $4^x$ first and
then using an additional algebraic extension to get its square root,
$2^x$:

$${{\theta+1}\over{\phi+1}}; \theta = \exp(x \ln 4); \phi^2 = \theta$$

\endexample

That's it!  The basic two differential axioms, algebraic extensions,
quotient fields, and these four types of transcendentals, round out
the entire base algebraic structure we'll need to construct our
theory.  We do need to be careful, though, as the last example
illustrated.  In these simple examples, figuring out which elements
are algebraic and which are transcendental is easy, but in more
complex expressions this may not be obvious.  We'll discuss in Chapter
?? how to test new elements for transcendence.

What about sines and cosines, all those arc-functions, raising things
to powers, and all that?  Turns out we can express all those
operations using just our basic extensions.  The key here is Euler's
famous identity $e^{i\theta}=i\sin\theta+\cos\theta$.

\example

Express $\sin x$ in a form suitable for Risch Integration.

Euler's identity immediately gives:

$$\sin x = {-i \,{{e^{ix} - e^{-ix}}\over 2}}$$

Therefore, starting from ${\bf C}(x)$,
we add the exponential extension $\theta = \exp(ix)$,
and conclude that $\sin x$ can be expressed as the rational function:

$${{\theta^2 - 1}\over 2i\theta}$$

in the field ${\bf C}(x,\theta); \theta=\exp(ix)$.

\endexample

If trigonometric functions can be represented using complex
exponentials, then it should come as no real surprise that inverse
trigonometric functions can be represented with complex logarithms.

\example Represent $\arcsin x$ in a form suitable for Risch Integration.

Let's start with Euler's identity and take its logarithm:

$$e^{i\theta}=i\sin\theta+\cos\theta$$

$$i\theta=\ln(i\sin\theta+\cos\theta)$$

Now, if $\theta = \arcsin x$, then $x = \sin \theta$, and we can use
the basic $\sin^2 \theta + \cos^2 \theta = 1$ identity to compute
$\cos \theta = \sqrt{1-\sin^2\theta} = \sqrt{1-x^2}$.  Substituting above:

$$i\theta=\ln(i x+\sqrt{1-x^2})$$

$$\theta=-i\ln(i x+\sqrt{1-x^2})$$

$$\arcsin x=-i\ln(i x+\sqrt{1-x^2})$$

Thus, we need first an algebraic extension to construct $\phi = \sqrt{1-x^2}$,
followed by a logarithm extension to construct $\arcsin x = -i\ln(ix+\phi)$.

\endexample

I think the details of further constructions along these lines are
straightforward enough that I will simply summarize them in a table.

\vfill\eject

\section{Root/Logarithm/Exponential Form}


\def\sech{{\rm sech}}
\def\csch{{\rm csch}}

\begin{center}
\begin{tabular}{c c c c @{\bigskip}}
Expression & \multicolumn{1}{c}{Expansion} &
Expression & \multicolumn{1}{c}{Expansion} \\
\hline
& \\
$f^g$ & $\displaystyle e^{\,g \ln f}$ &
 & \\
$\sin x$ & $\displaystyle {-i \,{{e^{ix} - e^{-ix}}\over 2}}$ \vbox to20pt{}&
 $\sinh x$ & $\displaystyle {{e^{x} - e^{-x}}\over 2}$ \vbox to20pt{} \\
$\cos x$ & $\displaystyle {{e^{ix} + e^{-ix}}\over 2}$ &
 $\cosh x$ & $\displaystyle {{e^{x} + e^{-x}}\over 2}$ \vbox to20pt{} \\
$\tan x$ & $\displaystyle {-i \,{{e^{ix}-e^{-ix}}\over {e^{ix}+e^{-ix}}}}$ &
 $\tanh x$ & $\displaystyle {{e^{x}-e^{-x}}\over {e^{x}+e^{-x}}}$ \\

$\sec x$ & $\displaystyle {2\over{e^{ix} + e^{-ix}}}$ &
 $\sech\, x$ & $\displaystyle {2\over{e^{x} + e^{-x}}}$ \vbox to20pt{} \\
$\csc x$ & $\displaystyle {{2i}\over{e^{ix} - e^{-ix}}}$ \vbox to20pt{}&
 $\csch\, x$ & $\displaystyle {2\over{e^{x} - e^{-x}}}$ \vbox to20pt{} \\
$\cot x$ & $\displaystyle {i \,{{e^{ix}+e^{-ix}}\over {e^{ix}-e^{-ix}}}}$ &
 $\coth x$ & $\displaystyle {{e^{x}+e^{-x}}\over {e^{x}-e^{-x}}}$ \\

$\arcsin x$ & $\displaystyle -i \,\ln (ix + \sqrt{1-x^2})$ &
 $\sinh^{-1} x$ & $\displaystyle \ln (x + \sqrt{x^2+1})$ \\
$\arccos x$ & $\displaystyle -i \,\ln (x + i\sqrt{1-x^2})$ &
 $\cosh^{-1} x$ & $\displaystyle \ln (x + \sqrt{x^2-1})$ \\
$\arctan x$ & $\displaystyle {1\over2}\,i\,\ln {{ix-1}\over{ix+1}}$ &
 $\tanh^{-1} x$ & $\displaystyle {1\over2} \ln {{1+x}\over{1-x}}$ \\

$\sec^{-1} x$ & $\displaystyle -i \,\ln {{1 + i\sqrt{x^2-1}}\over{x}}$ &
 $\sech^{-1} x$ & $\displaystyle {1\over2} \ln {{1+\sqrt{1-x^2}}\over{1-\sqrt{1-x^2}}}$ \\
$\csc^{-1} x$ & $\displaystyle -i \,\ln {{i + \sqrt{x^2-1}}\over{x}}$ &
 $\csch^{-1} x$ & $\displaystyle {1\over2} \ln {{\sqrt{1+x^2}+1}\over{\sqrt{1+x^2}-1}}$ \\
$\cot^{-1} x$ & $\displaystyle {1\over2}\,i\,\ln {{i+x}\over{i-x}}$ &
 $\coth^{-1} x$ & $\displaystyle {1\over2} \ln {{x+1}\over{x-1}}$ \\

\end{tabular}
\end{center}

\vfill\eject

\section{Liouville's Theorem}

The next problem we must confront is to limit the number of possible
fields in which we can find solutions to our problem.  So far, we have
seen how to construct an algebraic system to express any elementary
function, but there are an infinity of such systems.  Searching them
exhaustively for the solution to a given integral is out of the
question.  Fortunately, it's been known for almost 200 years that
there are severe restrictions on what extensions can appear in an
integral above and beyond those used in the original integrand.

For example, consider the expression $e^x$.  Differentiating it
yields, well, $e^x$.  Now the key thing to note is that the
exponential does not disappear after differentiation.  This, in fact,
is a general property of exponentials --- differentiation never makes
them disappear.  They can change around, to be sure,
${d\over dx}e^{2x}=2e^{2x}$, but notice that the exponential is still
present in the result.  Therefore, since the solution to our integral
must differentiate into the original integrand, we conclude that no
new exponentials can appear in the integral beyond those in the
integrand.  If there were new exponentials in the result, then they
would have to appear in the integrand as well, since they can never
disappear under differentiation.

The same thing happens with roots.  Differentiate $\sqrt{x}$ and you
get ${1\over{2\sqrt{x}}}$.  This time the root moves from the
numerator to the denominator, but again, it doesn't completely
disappear.  This is a general property of roots, algebraic extensions
in general, in fact.

Logarithms are different, though.  Differentiate $\ln x$ to get
$1\over x$.  The logarithm is gone.  So new {\it logarithms} can
appear in integrals, because they can disappear under differentiation
to recover the original integrand.  Even here, though, there are
important restrictions.  The logarithms have to appear with constant
coefficients (because something like $x\ln x$ would differentiate into
$1 + \ln x$), can not appear in powers or in denominators (${d\over
dx} \ln^2x = 2{\ln x\over x}$), and can not be nested (${d\over dx}
\ln(\ln x) = {1\over{x\ln x}}$).

% Of course, all of this is hand-waving so far, but I hope that it
% provides context and concrete examples to understand what has become
% known as {\it Liouville's Theorem} --- the only new extensions that
% can appear are simple logarithms.  Let's nail this down, now.

These examples are all special cases of {\it Liouville's Theorem} ---
the only new extensions that can appear in an integral are simple
logarithms with constant coefficients.

\theorem\label{basic logarithmic properties}
Let $E=K(\theta)$ be a simple transcendental logarithmic extension of
a differential field $K$ with the same constant subfield as $K$, let
$p=\sum p_i \theta^i$ be a polynomial in $K[\theta]$, and let $r =
a/b$ be a rational function in $K(\theta)$ ($a, b \in K[\theta]$).
Then:

\begin{enumerate}
\item If $p$'s leading coefficient is constant ($p_n' = 0$), then ${\rm Deg}_\theta\, p' = {\rm Deg}_\theta\, p - 1$
\item If $p$'s leading coefficient is not constant ($p_n' \ne 0$), then ${\rm Deg}_\theta\, p' = {\rm Deg}_\theta\, p$
\item If $p$ is monic and irreducible, then $p' \nmid p$
\item $r' \in K$ if and only if $r$ has the form $c\theta + k$, where $c$ is a constant
\end{enumerate}

\proof

The first two statements follow easily from considering $p'$:

$$p'=\sum_{i=0}^n (p_i' \theta^i + i p_i \theta' \theta^{i-1}) = \sum_{i=0}^n
\left(p_i' + \left(i+1\right)p_{i+1} \theta'\right) \theta^i$$

Note that since $K(\theta)$ is a logarithmic extension, $\theta'
\in K$, so for all $i$ the entire expression $(p_i' + (i+1)p_{i+1} \theta')$
is in $K$.  In particular, since $p_{n+1}$ is zero, the $n^{\rm th}$
coefficient of $p'$ is just $p_n'$ and the $\theta$-degree of $p'$
will be $n$ if $p_n'$ is non-zero.  On the other hand, if $p_n'$ is
zero, then the $n-1^{\rm th}$ coefficient of $p'$ is $(p_{n-1}' + n
p_n \theta')$ which would be zero only if $\theta' =
-\frac{p_{n-1}'}{n p_n} = (-\frac{p_{n-1}}{n p_n})'$ (by Theorem
\ref{basic difalg} since $p_n$ is constant), which implies
an algebraic relationship between $\theta$ and $-\frac{p_{n-1}}{n
p_n}$ (specifically, they differ only by a constant, which must be in
$K$), contradicting the transcendence of $E$ over $K$.

Next, if $p$ is monic and irreducible, then ${\rm Deg}_\theta\, p' =
{\rm Deg}_\theta\, p - 1$, and no lower degree polynomial can divide
an irreducible polynomial, establishing the third claim.

Finally, consider a rational function $r=a(\theta)/b(\theta)$ in its
normalized form, so $\gcd(a,b) = 1$ and $b$ is monic.  Now we can
factor $b$ into irreducible factors ($b=\prod b_i(\theta)^{m_i}$)
and expand $r$ using partial fractions:

$$r = a_0(\theta) + \sum_{i=1}^\mu \sum_{j=1}^{m_i} \frac{a_{ij}(\theta)}{b_i(\theta)^j}$$

where $a_0, a_{ij}, b_i \in K[\theta]$ and ${\rm Deg}_\theta\, a_{ij} < {\rm
Deg}_\theta\, b_i$.  Now let's differentiate:

$$r' = a_0'(\theta) + \sum_{i=1}^\mu \sum_{j=1}^{m_i} \left[
\frac{a'_{ij}(\theta)}{b_i(\theta)^j} - \frac{j\, a_{ij}(\theta)\,
b_i'(\theta)}{b_i(\theta)^{j+1}} \right]$$

$a_{ij}$ does not divide $b_i$ (since ${\rm Deg}_\theta\, a_{ij} <
{\rm Deg}_\theta\, b_i$, and we proved above that $b'_i$ does not
divide $b_i$ (since $b_i$ is monic and irreducible), so there is
exactly one term on the right hand side with $b_i(\theta)^{m_i + 1}$
in its denominator and no other terms with higher powers.  Therefore,
$r'$ must have a $b_i(\theta)^{m_i +1}$ in its denominator.  Since our
hypothesis states that $r$ is in $K$, it can not have any $\theta$
terms in its denominator (or anywhere else), so there can not be any
$b_i(\theta)$ factors, and $r$ must be a polynomial.  Futhermore, our
first two claims imply that if ${\rm Deg}_\theta\, r' = 0$ (since
$r'\in K$), then ${\rm Deg}_\theta\, r$ can be at most 1, and its
leading coefficient must be constant.

\endtheorem

\theorem\label{basic exponential properties}
Let $E=K(\theta)$ be a simple transcendental exponential extension of
a differential field $K$ with the same constant subfield as $K$,
let $p=\sum p_i \theta^i$ be a polynomial in $K[\theta]$,
and let $r=a/b$ be a rational function in $K(\theta)$
($a, b \in K[\theta]$).  Then:

\begin{enumerate}
\item ${\rm Deg}_\theta\, x' = {\rm Deg}_\theta\, x$
\item $p' \mid p$ if and only if $p$ is monomial.
\item $r' \in K$ if and only if $r \in K$
\end{enumerate}

\proof

Again,

$$p' = \sum_{i=0}^n (p_i' \theta^i + i p_i \theta' \theta^{i-1})$$

This time, however, $\theta' = k'\theta$, so

$$p' = \sum_{i=0}^n (p_i' + i p_i k') \theta^i$$

Assume that one of these coefficients, say $(p_i' + i p_i k')$, was
zero but $p_i$ was non-zero.  Then $D(p_i \theta^i) = (p_i' + i p_i
k') = 0$, so $p_i \theta^i$ would be a constant, which must be in $K$,
contradicting the transcendence of $E$.  Therefore, none of these
coefficients can be zero, establishing the first claim.

To establish the second claim, assume first that $p' \mid p$.  Since
$p'$ has the same degree as $p$ (by the first claim), it can only
divide $p$ if it has the form $mp$, where $m \in K$.  Equating
coefficients of $\theta$ in the above sums leads us to conclude that

$$m = (\frac{p_i'}{p_i} + i k')$$

If $p$ was not monomial, then all of its coefficients must
yield the same value for $m$, i.e,

$$m = (\frac{p_i'}{p_i} + i k') = (\frac{p_j'}{p_j} + j k')$$

$$p_i' p_j - p_i p_j' + (i - j) k' p_i p_j = 0$$

$$\frac{p_i' p_j - p_i p_j'}{p_j^2} + (i - j) k' \frac{p_i}{p_j} = 0$$
$$\left(\frac{p_i}{p_j}\right)' + (i - j) k' \frac{p_i}{p_j} = 0$$

Then $D(\frac{p_i}{p_j} \theta^{j-i}) = \left(\frac{p_i}{p_j}\right)'
+ (i - j) \frac{p_i}{p_j} k' = 0$, again contradicting the
transcendence of $E$ over $K$.  So $p$ must be monomial.

Conversely, if $p$ is monomial, say $a\theta^n$, then $p' = (a' +
n a k') \theta^n = \frac{a' + n a k'}{a} p$ and $p' \mid p$.

To prove the final claim, we proceed as before, expanding $r$ using
partial fractions:

$$r = a_0(\theta) + \sum_{i=1}^\mu \sum_{j=1}^{m_i} \frac{a_{ij}(\theta)}{b_i(\theta)^j}$$

and taking the derivative:

$$r' = a_0'(\theta) + \sum_{i=1}^\mu \sum_{j=1}^{m_i} \left[
\frac{a'_{ij}(\theta)}{b_i(\theta)^j} - \frac{j\, a_{ij}(\theta)\,
b_i'(\theta)}{b_i(\theta)^{j+1}} \right]$$

This time, however, $b_i'$ can divide $b_i$ if $b_i$ is monomial.  All
other possibilities are excluded as before, so $r$ can now have the
form:

$$r = \sum_{i=-m}^n r_i \theta^i$$

where $r_i \in K$.  We've already established that if $r_i$ is
non-zero, then the corresponding term in the derivative
is also non-zero, so the only way for $r'$ to be in $K$ is if
$r$ is in $K$.

\endtheorem

\example Let $p = x e^x$.  Then
$\frac{d}{dx} x e^x = e^x + x e^x = (x+1) e^x = \frac{x+1}{x} e^x$.

We start with the rational function field $K = {\mathbb C}(x)$, and
extend by the transcendental exponential $\theta = \exp(x)$ to form
the ring $K[\theta]$.  Both $p$ and $p'$ are in $K[\theta]$;
$p=x\theta$ is monomial (in $\theta$); note that $\frac{x+1}{x} \in
K$, so $p' \mid p$ in this ring.

\endexample

\theorem\label{basic algebraic properties}

Let $A$ be an algebraic extension of a differential field $K$ with the
same constant subfield as $K$.  Then for all $a\in A$, $a' \in K
\leftrightarrow a \in K$.

\proof

The right-to-left implication is obvious (since differential fields
are closed under derivation), so we need only to prove the
left-to-right implication.

Extend $A$ to its algebraic closure $\bar{A}$.  Since derivation of an
algebraic element is determined solely by its minimum polynomial,
$A$'s derivation extends uniquely to define $\bar{A}$'s.  Now
consider $({\rm Tr\,} a)'$ (the derivation of $a$'s trace):

$$({\rm Tr\,} a)' = (\sum a_i)'$$

where the $a_i$ are the conjugates of $a$ in $\bar{A}$.  Now apply the
differential addition axiom to $(\sum a_i)'$ and note that since
derivation of an algebraic element is determined solely by its minimum
polynomial, and all of the conjugate elements have the same minimum
polynomial, then

$$({\rm Tr\,} a)' = (\sum a_i)' = \sum a_i' = ({\rm Deg\,}a) a'$$

$$(\frac{1}{{\rm Deg\,} a}{\rm Tr\,} a)' = a'$$

Since the trace (a symmetric polynomial of conjugates) exists in the
underlying field $K$, so does $\frac{1}{{\rm Deg\,} a}{\rm Tr\,}$, so
we have identified an element in $K$ with the same derivation as $a$,
which therefore can differ from $a$ solely by a constant.  Since $A$
and $K$ have the same constant subfield, all of our constants are in
$K$, so $a$ is therefore also in $K$.

\endtheorem

\example

Explain the ``disappearance'' of the square root in:

$$\int{1\over\sqrt{1-x^2}} = \arcsin x$$

Finding $\arcsin x$ in the table, we see that:

$$\arcsin x = -i \,\ln (ix + \sqrt{1-x^2})$$

That's where it went!  It ``disappeared'' into the complex logarithm
that $\arcsin x$ is formed from.  New logarithms, of course, are
acceptable.  Notice that the new logarithm has a constant coefficient
($-i$), is not nested, and appears to the first power.

\endexample

Notice the extra condition on the algebraic extension, that the
extension has to preserve the constant subfield.  The theorem would
fail without this condition, as shown by numerous examples of roots
appearing in integrals where only rational numbers were needed in the
integrand.  The simplest way to handle this situation is to use an
algebraically closed constant subfield (like ${\mathbb C}$), but
this is not always practical.

\example

$$\int \frac{1}{x^2-2} dx = \int \frac{1}{2\sqrt{2}} \left[ \frac{1}{x-\sqrt{2}} - \frac{1}{x+\sqrt{2}} \right] dx$$
$$= \frac{1}{2\sqrt{2}} \left[ \ln(x-\sqrt{2}) - \ln(x+\sqrt{2}) \right]$$

This integrand can be expressed in ${\mathbb Q}(x)$, but the integral
requires ${\mathbb Q}(x,\xi,\theta,\psi)$; $\xi$ is algebraic
with minimal polynomial $\xi^2-2=0$; $\theta$ and $\psi$ are
logarithmic transcendental with $\theta' = 1/(x-\xi)$
and $\psi' = 1/(x+\xi)$.

\endexample

\theorem (Liouville) Let $L$ be a Liouvillian extension of
a differential field $K$ with the same constant subfield as $K$.
Then $\forall l \in L$, $l' \in K$ iff $l$ has the form:

$$k + \sum_{i=1}^n c_i \theta_i$$

where $k\in K$, $K(\theta_i)$ are simple logarithmic extensions of $K$ and $c_i$
are constants.

\proof

Since $L$ is formed from $K$ by a finite number of extensions,
consider the last extension and call it $\theta$, i.e, $L=M(\theta)$
where $K \subset M \subset L$.  Since $l' \in K$, $l' \in M$.  If
$\theta$ were either exponential or algebraic, then by Theorems
\ref{basic exponential properties} and \ref{basic algebraic properties}
$l$ would have to be in $M$.  On the other hand, if $\theta$ were
logarithmic, then by Theorem \ref{basic logarithmic properties} $l$
must have the form $c\theta + m$, where $m\in M$.  Applying this
argument inductively leads us to conclude that $l$ must have the
form...  (FIX AND FINISH THIS PROOF)

\endtheorem
