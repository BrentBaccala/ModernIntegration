
\section{Integral Elements}

\definition

A field's {\bf local ring} at a place $p$ is the set of
all functions in the field with no pole at $p$; that
is, $ \lim_{x\to p} f(x) \ne \infty $.

A function is {\bf locally integral} at a place $p$
if it belongs to its field's local ring at $p$.

\enddefinition

Remember the distinction I drew between a {\it pole} and a {\it
removable singularity} is Chapter ?.  That's the reason for the limit
in the definition; removable singularities are specifically included
in a local ring.  Thus, $x\over\sqrt{x}$ is locally integral at $x=0$,
despite the fact that it has a zero denominator, because
$\lim_{x\to0}{x\over\sqrt{x}}=\sqrt{x}=0$, a finite value.

It is often important, however, to specify which field we are
refering to when we discuss a local ring.

\example \quad

In the field ${\cal C}(x)$, the function $x$ is locally integral at $x=0$.

In the field $F = {{\cal C}(x,y); y^2=x+1}$, the function $x$ is locally integral
at both $(x=0, y=1)$ and $(x=0, y=-1)$.  There is no place $x=0$ in
this field; to speak of it so is ambiguous, as there are two places
in $F$ where $x=0$.

Likewise, $y$ is locally integral in $F$ at both $(x=0, y=1)$ and
$(x=0, y=-1)$.  It is not locally integral in ${\cal C}(x)$ at $x=0$ simply
because $y$ does not exist in the field ${\cal C}(x)$.

\endexample

Thus, a function like $x$ can be locally integral in both ${\cal C}(x)$ and
${\cal C}(x,y)$; it is important to specify which we are talking about.

Since the fields ${\cal C}(x)$ and ${\cal C}(x,y)$ are distinct, and have different
places, they also have different local rings.  There is an important
connection between them, however.

\theorem
\label{local integral polynomial}

A function $f$ is locally integral at a place $p$ in ${\cal C}(x,y)$ if it
satisfies a monic polynomial with coefficients in the local ring of
${\cal C}(x)$ at the corresponding place in that field.

\proof

Define $z = {1 \over f}$ and consider what would happen if $f$ had a
pole at $p$.  Then $\lim_{(x,y)\to p} f(x) = \infty$, so $z(x,y) = 0$
(we are justified in discarding the limit by the principle of isolated
singularities).  Yet $f$ satisfies a monic polynomial with coefficients
in ${\cal C}(x)$'s local ring under $p$:

	$$f^n + a_{n-1} f^{n-1} + \cdots + a_1 f + a_0 = 0$$

	$$z^{-n} + a_{n-1} z^{-n+1} + \cdots + a_1 z^{-1} + a_0 = 0$$

	$$1 + a_{n-1} z + \cdots + a_1 z^{n-1} + a_0 z^n = 0$$

Now, since $z$ is zero at a place over $p$, at least one of this
polynomial's roots must be zero.  Since all of the $a_i$ are finite at
$p$ (they are in ${\cal C}(x)$'s local ring there), multiplying
any of them by zero yields zero, so substituting in
$z=0$ yields $1=0$. We conclude that $z$ can not be zero, and thus
$f$ can not have a pole over $p$.

\endtheorem

The converse is not true; just because a function does not satisfy
such a monic polynomial does not mean that it is not locally integral.
A slightly weaker converse does hold, however:

\theorem

A function $f$ in ${\cal C}(x,y)$ which is locally integral at all
places in ${\cal C}(x,y)$ over a place $p$ in ${\cal C}(x)$ satisfies
a monic polynomial with coefficients in the local ring of ${\cal C}(x)$
under $p$.

\proof

Construct the polynomial

$$(z - f(x,y))(z - f(x,y_2)) \cdots (z - f(x,y_n)) = 0$$

$f$ clearly satisfies this polynomial at every place over $p$.
Furthermore, multiplying out the terms yields:

$$\matrix{z^n & - (f(x,y)+f(x,y_2)+\cdots+f(x,y_n)) z^{n-1} \hfill\cr
   & + \quad (f(x,y)f(x,y_2)+f(x,y)f(x,y_3)+\cdots+f(x,y_{n-1})f(x,y_n)) z^{n-2} \hfill\cr
   & + \cdots + f(x,y)f(x,y_2)\cdots f(x,y_n) = 0 \hfill\cr}$$

All of these coefficients are symmetric functions in $y_i$, and thus
exist in ${\cal C}(x)$.  Furthermore, since $f$ is finite (by
assumption) at all conjugate values over $p$, each of these
coefficients must also be finite at $p$ (since they are constructed by
addition and multiplication from the conjugate values of $f$), and are
thus in the local ring of ${\cal C}(x)$ at $p$.

\endtheorem

In short, a function in ${\cal C}(x,y)$ which satisfies a monic
polynomial whose coefficients are in a local ring of ${\cal C}(x)$ at
a point $p$ is locally integral in ${\cal C}(x,y)$ at all places over
$p$.  A function which does not satisfy such a polynomial has
a pole at at least one place over $p$.

A more advanced, more purely algebraic approach to this subject would
{\it define} local rings using Theorem \ref{local integral
polynomial}, then use valuations and completions to {\it prove} the
connection between local rings and poles (or the lack thereof).  See
[van der Waerden], Chapter 18.  I will forego this in favor of this
simpler, more analytic (note the use of the limit) approach.


Having established the existence and basic properties of {\it locally}
integral elements, we can now discuss {\it globally} integral
elements.  Obviously, elements in a field will be locally integral at
more than one place.  In fact, field elements will be locally integral
at almost all places (because they only have a finite number of
poles).  So, intuitively, a {\it globally} integral element would be
one that is integral at every place in a field.  However, Theorem ?
immediately tell us that the only such functions (those which have no
poles anywhere) are the constants, so this turns out to be too
restrictive to be useful.  Instead, we exclude infinity and define:

\definition

A function is {\bf globally integral} in a field if it is locally
integral at all {\it finite} places in the field.

\enddefinition

What's special about infinity?  Why not exclude some other place?
Well, nothing's all that special about infinity.  We've already seen
how a birational transformation can be used to swap infinity with any
finite point.  We use infinity because it's convenient.  For example,

\theorem

The globally integral functions in ${\cal C}(x)$ are the polynomials.

\proof

The elements of ${\cal C}(x)$ are fractions of polynomials with no
common factors between numerator and denominator.  Since {\cal C} is
algebraically closed, both numerator and denominator factor into
linear terms with no cancelation between them.  To avoid any finite
poles, the denominator must be a constant.  Since polynomials
have no finite poles, there is no restriction on the numerator.

\endtheorem

Pretty straightforward, huh?

While it's obvious from inspection which functions are globally
integral in ${\cal C}(x)$, the situation in ${\cal C}(x,y)$ is more
difficult.  Something like $y \over x$, which appears to have a pole
at $x=0$, is actually globally integral if, for example, $y^2=x^3$.
Then we can consider squaring $y \over x$ to obtain ${y^2 \over x^2} =
{x^3 \over x^2} = x$.  If the square is finite, then the original
function had to be finite (you can't square infinity and get a
finite value), so we conclude that $y \over x$ is, in fact,
globally integral in ${\cal C}(x,y); y^2=x^3$.

We could use Theorem \ref{local integral polynomial} and the fact
that, in ${\cal C}(x,y); y^2=x^3$,

	$$ \left({y \over x}\right)^2 - x = 0 $$

to conclude that, since this is a monic polynomial with coefficients
globally integral in ${\cal C}(x)$, $y \over x$ must be globally
integral in ${\cal C}(x,y); y^2=x^3$.  Theorem \ref{local integral
polynomial} can be easily generalized:

\theorem

A function $f$ is globally integral in ${\cal C}(x,y)$ if it
satisfies a monic polynomial with globally integral coefficients in
${\cal C}(x)$.

\proof

\endtheorem

Proof that globally integral elements form a finite extension:
[Eichler], p. 54

Proof that field elements are determined by their divisors, up to a
constant multiple: [Eicher], p. 84

[Eicher], p. 86: Prime divisors are isomorphic with special local rings.

The problem is that we have no straightforward means to construct
such a polynomial, or prove that one doesn't exist, for any
particular function $f$.  To test a function to determine
if it is integral, we'll use a different approach.

\section{Modules}

We'll resort now to {\it modules}, a fairly important algebra concept
backed by a substantial body of theory, upon which I shall only draw
as needed.  General references included [Atiyah+McDonald], [Lang], and
about a thousand others.

\definition

An {\it R-module} over a ring R is an additive group M acted on by R
(i.e, there is a mapping $R \times M \to M$) in a distributive manner:

$$(r_1 + r_2)m = r_1 m + r_2 m \qquad r_1, r_2 \in {\rm R};\, m \in {\rm M}$$

where we have adopted the usual convention of writing R's action on M
as a multiplication.

\enddefinition

\definition

A {\it free R-module} is an R-module spanned by a basis
$\{b_1, b_2, ... b_n\}$.  It consists of all elements formed as follows:
\footnote{I'll also note that a multiplication rule needs to be specified
between the basis elements and the elements of the ring, and an
addition rule between the elements of the module.  Also,
the expression has to be {\it unique} --- you can't be
able to write an element two different ways.  In our
case, these rules are obvious, but that's not always the case.}

	$$ a_1 b_1 + a_2 b_2 + ... + a_n b_n; \qquad a_i \in R $$


\enddefinition

Not all modules have a basis, but for those that do the two
definitions are basically identical.  Elements formed from a basis can
be added by using the module's distributive property (!!) to factor out the
coefficients from of each basis element and then performing the addition
in the ring R:

$$ (a_1 b_1 + a_2 b_2 + ... + a_n b_n) + (c_1 b_1 + c_2 b_2 + ... + c_n b_n) $$
$$  = (a_1 + c_1) b_1 + (a_2 + c_2) b_2 + ... + (a_n + c_n) b_n $$

So the elements generated from a basis clearly form a module.  R operates
on them by multiplication by every coefficient.

\example \quad

An ideal I in a ring R is a R-module, but a subring of R, in
general, is not, because multiplication by an element of R might
not produce a result in the subring.

\endexample


Note that it is vitally important to specify the ring used for the
coefficients.  For example, consider the basis $\{1, y\}$.  Treating
this as a ${\cal C}(x)$-module, I can form ${y \over x} = {1 \over x}
y$, since ${1 \over x} \in {\cal C}(x)$.  However, $y \over x$ does {\it
not} belong to the ${\cal C}[x]$-module generated by $\{1, y\}$.  I
would need to use polynomial coefficients to form a ${\cal
C}[x]$-module, not the rational functions coefficients allowed in a
${\cal C}(x)$-module.  We'll be primarily interested in ${\cal C}[x]$-modules,
${\cal C}(x)$-modules, and ${\cal I}$-modules, where ${\cal I}$ is the ring
of globally integral elements in ${\cal C}(x,y)$.


The first kind of basis we're interested in, a {\it basis for all
rational functions}, is one than spans the entire ${\cal C}(x,y)$ field
as a ${\cal C}(x)$-module.
In other words, we're looking for a basis $\{b_1, b_2,
... b_n\}$ so that everything in ${\cal C}(x,y)$ can be expressed
in the form:

	$$ a_1 b_1 + a_2 b_2 + ... + a_n b_n; a_i \in {\cal C}(x) $$

Such a basis will always have $n$ elements, where $n$ is the degree of
the ${\cal C}(x,y)$ extension over ${\cal C}(x)$, and can be most
conveniently characterized using its {\it conjugate matrix}:

\definition

The {\bf conjugates} of a rational function $\eta(x,y)$ in ${\bf
C}(x,y)$ are the functions formed by replacing $y$ with its conjugate
values.

The {\bf trace} of a rational function $\eta(x,y)$ is the sum of
its conjugates:

$${\rm T}(\eta(x,y)) = \sum_i \eta(x,y_i)$$

The {\bf norm} of a rational function $\eta(x,y)$ is the product of
its conjugates:

$${\rm N}(\eta(x,y)) = \prod_i \eta(x,y_i)$$

Both the trace and norm, as symmetric functions in $y_1,...,y_n$, are
functions in ${\bf C}(x)$.

The {\bf conjugate matrix} ${\bf M}_\omega$ of $n$ elements $\omega_i$
in ${\cal C}(x,y)$, where $n$ is the degree of ${\cal C}(x,y)$ over
${\cal C}(x)$, is the matrix whose each row consists of the $n$
conjugate values of a single element, and whose $n$ rows are formed in
this way from the $n$ elements.

\enddefinition

\definition

For any function $\eta \in {\bf C}(x,y)$ and any set of $n$ elements
$\omega_i$, the {\bf trace vector}
${\bf T}_{\eta/\omega} = \Big({\rm T}(\eta \omega_i)\Big)$ 
of $\eta$ relative to $\omega$
is formed from the
traces of the $n$ products of $\eta$ with the $n$ functions
$\omega_i$.

The {\bf conjugate vector} ${\bf C}_\eta = (\eta(x,y_i))$ is formed from the
$n$ conjugates of $\eta$.

\enddefinition

\theorem
\label{function is zero if trace vector is zero}

For any function $\eta \in {\bf C}(x,y)$ and any set of $n$ functions
$\omega_i \in {\bf C}(x,y)$, if ${\bf T}_{\eta/\omega}$ is the zero vector
and $|{\bf M}_\omega|$ is non-zero, then $\eta$ is zero.

\proof

${\bf T}_{\eta/\omega}$, ${\bf M}_\omega$ and ${\bf C}_\eta$
obey the matrix equation

$${\bf T}_{\eta/\omega} = {\bf M}_\omega {\bf C}_\eta$$

since each row of this matrix equation has the form

$$ {\rm T}(\eta \omega_i) = \sum_j \omega_i(x,y_j)\eta(x,y_j) $$

Since ${\bf M}_\omega$ is invertible (since its determinant is
non-zero), if ${\bf T}_{\eta/\omega}$ is identically zero, then so must be
${\bf C}_\eta$, and $\eta$ is the first element in ${\bf C}_\eta$.

\endtheorem

\theorem
\label{|M| != 0 implies C(x) basis}

A set of $n$ functions $\omega_i$ forms a ${\cal C}(x)$-module basis for
${\cal C}(x,y)$ if $|{\bf M}_\omega|$ is not zero. ([Bliss], Theorem 19.1)

\proof

Note that when we multiply ${\bf M}_\omega$ by its transpose ${\bf
M}_\omega^T$, the $ij^{\rm th}$ element of ${\bf M}_\omega{\bf
M}_\omega^T$ is:

$$ \sum_k \omega_i(x, y_k)\omega_j(x, y_k) = {\rm T}(\omega_i \omega_j)$$

Since $|{\bf M}_\omega|$ is non-zero, $|{\bf M}_\omega^T|$ is
non-zero, and $|{\bf M}_\omega{\bf M}_\omega^T|$ is non-zero, so given
any function $\eta \in {\bf C}(x,y)$, we can solve the following
equation for ${\bf R}$:

$${\bf T}_\eta = {\bf M}_\omega {\bf M}_\omega^T {\bf R}$$

each of row of which reads:

$$ {\rm T}(\eta \omega_i) = \sum_j {\rm T}(\omega_i \omega_j) r_j $$

Since both ${\bf T}_\eta$ and ${\bf M}_\omega{\bf M}_\omega^T$ are composed of
nothing but traces, they exist in ${\bf C}(x)$, so ${\bf R}$ must also
exist in ${\bf C}(x)$ and its elements therefore commute with the
trace:

$$ {\rm T}(\eta \omega_i) = \sum_j {\rm T}(r_j \omega_j \omega_i) $$

Since the trace of a sum is the sum of the traces:

$$ {\rm T}(\eta \omega_i) = {\rm T}(\sum_j r_j \omega_j \omega_i) $$
$$ {\rm T}((\eta - \sum_j r_j \omega_j) \omega_i) = 0 $$

which implies that $\eta = \sum_j r_j \omega_j$, by Theorem
\ref{function is zero if trace vector is zero}, and since we've
already shown that the $r_j$ are rational functions in ${\bf C}(x)$,
this proves the theorem.

\endtheorem

Let me illustrate with a simple example.

\example

Consider the basis $\{1, y\}$ over the field ${\cal C}(x,y); y^2=x$.
The conjugate value of $y$ is $-y$ (PROVE THIS), so the conjugate
matrix is:

$$C=\left(\matrix{1&1\cr y&-y\cr}\right)$$

and its determinant:

$$\det C=\left|\matrix{1&1\cr y&-y\cr}\right| = -2y$$

Since $-2y$ is not zero, we conclude that $\{1, y\}$ is a basis
for all rational functions over ${\cal C}(x,y); y^2=x$.

\endexample

Notice that I didn't ask whether $-2y$ was zero at some place in the
field.  The determinant of the conjugate matrix can be zero at certain
places; in fact, often is.  It just can't be {\it identically} zero;
i.e, it can't be zero {\it everywhere}.  If this isn't clear, reread
Theorems \ref{function is zero if trace vector is zero} and \ref{|M|
!= 0 implies C(x) basis}, noting that all the matrices are defined
over the {\it fields} ${\bf C}(x)$ and ${\bf C}(x,y)$, where the only
zero element is 0.

\definition

A basis is said to be a {\bf locally integral basis} at a place $p$ in
${\cal C}(x)$ if the basis elements are locally integral at all places
in ${\cal C}(x,y)$ over $p$ and the determinant of its conjugate
matrix assumes a finite, non-zero value at that place.

\enddefinition

\theorem

A function $f$ in ${\cal C}(x,y)$ has no poles over a place $p$ in
${\cal C}(x)$ iff $f$ can be formed from a basis locally integral at
$p$, using elements from ${\cal C}(x)$ locally integral at $p$.

\endtheorem

\definition

A basis is said to be a {\bf globally integral basis}, or more commonly
just an {\bf integral basis}, if it is a locally integral basis at
all finite places in ${\cal C}(x)$

\enddefinition

It is not immediately clear that an (globally) integral basis will
exist for any particular field, and in fact the calculation of an
integral basis ultimately becomes one of the biggest computational
barriers in this theory.  Therefore, I will defer a more detailed
discussion until a later chapter, and instead present a simple
construction for the special case of a simple radical extension.

With a local integral basis (no pole at a place), there are 1-to-1
correspondances:

	use coeffs in K(x) w/ no pole at place <=> elem in K(x,y) w/ no pole

	use coeff w/ pole at place <=> elem in K(x,y) w/ pole at place
