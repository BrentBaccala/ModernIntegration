
\section{Integral Elements}

\definition

A field's {\bf local ring} at a place $p$ is the set of
all functions in the field with no pole at $p$; that
is, $ \lim_{x\to p} f(x) \ne \infty $.

A function is {\bf locally integral} at a place $p$
if it belongs to its field's local ring at $p$.

\enddefinition

Remember the distinction I drew between a {\it pole} and a {\it
removable singularity} is Chapter ?.  That's the reason for the limit
in the definition; removable singularities are specifically included
in a local ring.  Thus, $x\over\sqrt{x}$ is locally integral at $x=0$,
despite the fact that it has a zero denominator, because
$\lim_{x\to0}{x\over\sqrt{x}}=\sqrt{x}=0$, a finite value.

It is often important, however, to specify which field we are
refering to when we discuss a local ring.

\example \quad

In the field ${\cal C}(x)$, the function $x$ is locally integral at $x=0$.

In the field $F = {{\cal C}(x,y); y^2=x+1}$, the function $x$ is locally integral
at both $(x=0, y=1)$ and $(x=0, y=-1)$.  There is no place $x=0$ in
this field; to speak of it so is ambiguous, as there are two places
in $F$ where $x=0$.

Likewise, $y$ is locally integral in $F$ at both $(x=0, y=1)$ and
$(x=0, y=-1)$.  It is not locally integral in ${\cal C}(x)$ at $x=0$ simply
because $y$ does not exist in the field ${\cal C}(x)$.

\endexample

Thus, a function like $x$ can be locally integral in both ${\cal C}(x)$ and
${\cal C}(x,y)$; it is important to specify which we are talking about.

Since the fields ${\cal C}(x)$ and ${\cal C}(x,y)$ are distinct, and have different
places, they also have different local rings.  There is an important
connection between them, however.

\theorem
\label{local integral polynomial}

A function $f$ is locally integral at a place $p$ in ${\cal C}(x,y)$ if it
satisfies a monic polynomial with coefficients in the local ring of
${\cal C}(x)$ at the corresponding place in that field.

\proof

Define $z = {1 \over f}$ and consider what would happen if $f$ had a
pole at $p$.  Then $\lim_{(x,y)\to p} f(x) = \infty$, so $z(x,y) = 0$
(we are justified in discarding the limit by the principle of isolated
singularities).  Yet $f$ satisfies a monic polynomial with coefficients
in ${\cal C}(x)$'s local ring under $p$:

	$$f^n + a_{n-1} f^{n-1} + \cdots + a_1 f + a_0 = 0$$

	$$z^{-n} + a_{n-1} z^{-n+1} + \cdots + a_1 z^{-1} + a_0 = 0$$

	$$1 + a_{n-1} z + \cdots + a_1 z^{n-1} + a_0 z^n = 0$$

Now, since $z$ is zero at a place over $p$, at least one of this
polynomial's roots must be zero.  Since all of the $a_i$ are finite at
$p$ (they are in ${\cal C}(x)$'s local ring there), multiplying
any of them by zero yields zero, so substituting in
$z=0$ yields $1=0$. We conclude that $z$ can not be zero, and thus
$f$ can not have a pole over $p$.

\endtheorem

The converse is not true; just because a function does not satisfy
such a monic polynomial does not mean that it is not locally integral.
A slightly weaker converse does hold, however:

\theorem

A function $f$ in ${\cal C}(x,y)$ which is locally integral at all
places in ${\cal C}(x,y)$ over a place $p$ in ${\cal C}(x)$ satisfies
a monic polynomial with coefficients in the local ring of ${\cal C}(x)$
under $p$.

\proof

Construct the polynomial

$$(z - f(x,y))(z - f(x,y_2)) \cdots (z - f(x,y_n)) = 0$$

$f$ clearly satisfies this polynomial at every place over $p$.
Furthermore, multiplying out the terms yields:

$$\matrix{z^n & - (f(x,y)+f(x,y_2)+\cdots+f(x,y_n)) z^{n-1} \hfill\cr
   & + \quad (f(x,y)f(x,y_2)+f(x,y)f(x,y_3)+\cdots+f(x,y_{n-1})f(x,y_n)) z^{n-2} \hfill\cr
   & + \cdots + f(x,y)f(x,y_2)\cdots f(x,y_n) = 0 \hfill\cr}$$

All of these coefficients are symmetric functions in $y_i$, and thus
exist in ${\cal C}(x)$.  Furthermore, since $f$ is finite (by
assumption) at all conjugate values over $p$, each of these
coefficients must also be finite at $p$ (since they are constructed by
addition and multiplication from the conjugate values of $f$), and are
thus in the local ring of ${\cal C}(x)$ at $p$.

\endtheorem

In short, a function in ${\cal C}(x,y)$ which satisfies a monic
polynomial whose coefficients are in a local ring of ${\cal C}(x)$ at
a point $p$ is locally integral in ${\cal C}(x,y)$ at all places over
$p$.  A function which does not satisfy such a polynomial has
a pole at at least one place over $p$.

A more advanced, more purely algebraic approach to this subject would
{\it define} local rings using Theorem \ref{local integral
polynomial}, then use valuations and completions to {\it prove} the
connection between local rings and poles (or the lack thereof).  See
[van der Waerden], Chapter 18.  I will forego this in favor of this
simpler, more analytic (note the use of the limit) approach.


Having established the existence and basic properties of {\it locally}
integral elements, we can now discuss {\it globally} integral
elements.  Obviously, elements in a field will be locally integral at
more than one place.  In fact, field elements will be locally integral
at almost all places (because they only have a finite number of
poles).  So, intuitively, a {\it globally} integral element would be
one that is integral at every place in a field.  However, Theorem ?
immediately tell us that the only such functions (those which have no
poles anywhere) are the constants, so this turns out to be too
restrictive to be useful.  Instead, we exclude infinity and define:

\definition

A function is {\bf globally integral} in a field if it is locally
integral at all {\it finite} places in the field.

\enddefinition

What's special about infinity?  Why not exclude some other place?
Well, nothing's all that special about infinity.  We've already seen
how a birational transformation can be used to swap infinity with any
finite point.  We use infinity because it's convenient.  For example,

\theorem

The globally integral functions in ${\cal C}(x)$ are the polynomials.

\proof

The elements of ${\cal C}(x)$ are fractions of polynomials with no
common factors between numerator and denominator.  Since {\cal C} is
algebraically closed, both numerator and denominator factor into
linear terms with no cancelation between them.  To avoid any finite
poles, the denominator must be a constant.  Since polynomials
have no finite poles, there is no restriction on the numerator.

\endtheorem

Pretty straightforward, huh?

While it's obvious from inspection which functions are globally
integral in ${\cal C}(x)$, the situation in ${\cal C}(x,y)$ is more
difficult.  Something like $y \over x$, which appears to have a pole
at $x=0$, is actually globally integral if, for example, $y^2=x^3$.
Then we can consider squaring $y \over x$ to obtain ${y^2 \over x^2} =
{x^3 \over x^2} = x$.  If the square is finite, then the original
function had to be finite (you can't square infinity and get a
finite value), so we conclude that $y \over x$ is, in fact,
globally integral in ${\cal C}(x,y); y^2=x^3$.

We could use Theorem \ref{local integral polynomial} and the fact
that, in ${\cal C}(x,y); y^2=x^3$,

	$$ \left({y \over x}\right)^2 - x = 0 $$

to conclude that, since this is a monic polynomial with coefficients
globally integral in ${\cal C}(x)$, $y \over x$ must be globally
integral in ${\cal C}(x,y); y^2=x^3$.  Theorem \ref{local integral
polynomial} can be easily generalized:

\theorem

A function $f$ is globally integral in ${\cal C}(x,y)$ if it
satisfies a monic polynomial with globally integral coefficients in
${\cal C}(x)$.

\proof

\endtheorem

Proof that globally integral elements form a finite extension:
[Eichler], p. 54

Proof that field elements are determined by their divisors, up to a
constant multiple: [Eicher], p. 84

[Eicher], p. 86: Prime divisors are isomorphic with special local rings.

The problem is that we have no straightforward means to construct
such a polynomial, or prove that one doesn't exist, for any
particular function $f$.  To test a function to determine
if it is integral, we'll use a different approach.

\section{Modules}

We'll resort now to {\it modules}, a fairly important algebra concept
backed by a substantial body of theory, upon which I shall only draw
as needed.  General references included [Atiyah+McDonald], [Lang], and
about a thousand others.

\definition

An {\it R-module} over a ring R is an additive group M acted on by R
(i.e, there is a mapping $R \times M \to M$) in a distributive manner:

$$(r_1 + r_2)m = r_1 m + r_2 m \qquad r_1, r_2 \in {\rm R};\, m \in {\rm M}$$

where we have adopted the usual convention of writing R's action on M
as a multiplication.

\enddefinition

\definition

A {\it free R-module} is an R-module spanned by a linearly independent basis
$\{b_1, b_2, ... b_n\}$.  It consists of all elements formed as follows:
\footnote{I'll also note that a multiplication rule needs to be specified
between the basis elements and the elements of the ring, and an
addition rule between the elements of the module.  Also,
the expression has to be {\it unique} --- you can't be
able to write an element two different ways.  In our
case, these rules are obvious, but that's not always the case.}

	$$ a_1 b_1 + a_2 b_2 + ... + a_n b_n; \qquad a_i \in R $$


\enddefinition

Not all modules have a finite set of generators, and not all those
have a linearly independent set of generators.  Elements formed from a
basis can be added by using the module's distributive property (!!) to
factor out the coefficients from of each basis element and then
performing the addition in the ring R:

$$ (a_1 b_1 + a_2 b_2 + ... + a_n b_n) + (c_1 b_1 + c_2 b_2 + ... + c_n b_n) $$
$$  = (a_1 + c_1) b_1 + (a_2 + c_2) b_2 + ... + (a_n + c_n) b_n $$

So the elements generated from a basis clearly form a module.  R operates
on them by multiplication by every coefficient.

\example \quad
\label{sample modules}

An ideal I in a ring R is a R-module, but a subring S of R, in
general, is not, because multiplication by an element of R might not
produce a result in the subring.  R, however, can always be viewed as
an S-module.

% XXX
% If R is a principal ideal ring, then every ideal is also a
% free R-module, admitting a basis consisting of a single
% element, a generator of the ideal.
%
% Is this true?  Any zero divisor counterexamples?

\endexample


Note that it is vitally important to specify the ring used for the
coefficients.  For example, consider the basis $\{1, y\}$.  Treating
this as a ${\cal C}(x)$-module, I can form ${y \over x} = {1 \over x}
y$, since ${1 \over x} \in {\cal C}(x)$.  However, $y \over x$ does {\it
not} belong to the ${\cal C}[x]$-module generated by $\{1, y\}$.  I
would need to use polynomial coefficients to form a ${\cal
C}[x]$-module, not the rational functions coefficients allowed in a
${\cal C}(x)$-module.  We'll be primarily interested in ${\cal C}[x]$-modules,
${\cal C}(x)$-modules, and ${\cal I}$-modules, where ${\cal I}$ is the ring
of globally integral elements in ${\cal C}(x,y)$.


\input{09b-RATIONAL-FUNCTION-BASIS.tex}

\vfil\eject

Since polynomials have no finite poles, they are integral elements,
and thus ${\rm K}[x] \subseteq {\cal I}$.  Thus, ${\cal I}$ (the ring of
integral elements) is trivially a ${\rm K}[x]$-module (see Example
\ref{sample modules}), but what is not nearly so obvious is that it is
also a free module, a fact which underlies a great deal of our theory.
I'll prove this first by showing that ${\cal I}$ is finitely generated
as a ${\rm K}[x]$-module, then showing the existance of a linearly
independent set of generators.

Let's start with a preliminary theorem.

\theorem
\label{construction of dual basis}

If $\{w_1,\ldots,w_n\}$ is a basis for a finite separable field
extension $E/K$, then a dual basis $\{u_1,\ldots,u_n\}$ can be
constructed such that ${\rm Tr}(w_i u_j) = \delta_{ij}$.
([Lang] Corollary VI.5.3)

\proof

Consider the following matrix:

$$M = \pmatrix{{\rm Tr}(w_1 w_1) & & \cr \vdots & \ddots & \cr {\rm Tr}(w_1 w_n) & \cdots & {\rm Tr}(w_n w_n)}$$

Now take an element $x \in E$, and represent it relative to the basis
$\{w_1,\ldots,w_n\}$ as a row vector $X = (x_i)$.  Multiplying $X M$
produces a row vector whose $j^{\rm th}$ element can be written:

$$\sum_i x_i {\rm Tr}(w_j w_i) = {\rm Tr}(w_j \sum_i x_i w_i) = {\rm Tr}(w_j x) = {\rm Tr}_x(w_j)$$

where I used first the $K$-linearity and additive distributive
properties of ${\rm Tr}$, then wrote ${\rm Tr}_x: f(a) = {\rm Tr}(ax)$
to emphasize that I'm regarding ${\rm Tr}_x$ as a linear form in ${\rm
Hom}_E(E,K)$.  So, if $M$ is singular, then there exists some non-zero
element $x$ such that ${\rm Tr}_x$ is zero for all of $w_i$, which
form a basis set, so ${\rm Tr}_x$ must therefore be the zero map.
This can only happen if ${\rm Tr}$ is identically zero, which would be
the case for an inseparable extension.  For the separable case,
therefore, $M$ must be invertible, and we can write:

$$M^{-1} M = \pmatrix{1 & & \cr & \ddots & \cr & & 1}$$

A moment's thought now shows that the rows of $M^{-1}$ are the desired
dual basis elements, written with respect to $\{w_1,\ldots,w_n\}$.

\endtheorem

\vfil\eject

\theorem
\label{I is finitely generated}

${\cal I}$ is a finitely generated ${\rm K}$-module.
([A+MacD] Proposition 5.17; [Lang] Exercise VII.3)

\proof

Regarding ${\rm K}(x,y)$ as a vector space over ${\rm K}(x)$, we can
easily construct a basis of integral elements by starting with $\{1,
y, \ldots, y^{n-1}\}$ and multiplying each element (if needed) by a
polynomial in $x$ which cancels all of its poles:

$${\rm K}(x,y) = {\rm K}(x)\{w_1,\ldots,w_n\} \qquad \forall i(w_i \in {\cal I})$$

Using Theorem \ref{construction of dual basis}, construct a dual basis
$\{u_1,\ldots,u_n\}$ so that ${\rm Tr}(w_i u_j) = \delta_{ij}$.  Take
any $x \in {\cal I}$ and write it using the dual basis:

$$x = \sum_i a_i u_i \qquad a_i \in {\rm K}(x)$$

Now consider ${\rm Tr}(x w_j)$.  Now, $x$ and $w_j$ are both in ${\cal
I}$, so $x w_j$ is in ${\cal I}$, and therefore has a monic minimum
polynomial with coefficients in ${\rm K}[x]$.  Since ${\rm Tr}$ equals
some integer multiple of the negative of the second coefficient in a
monic minimum polynomial, ${\rm Tr}(x w_j) \in {\rm K}[x]$.  But also,

$${\rm Tr}(x w_j) = {\rm Tr}(\sum_i a_i u_i w_j)
 = \sum_i {\rm Tr}(a_i u_i w_j) %\hfil (Tr an additive homomorphism)
 = \sum_i a_i {\rm Tr}(u_i w_j) %\hfil (linearity of Tr)
 = a_i$$

which establishes that $\forall i (a_i \in {\rm K}[x])$, so

$${\cal I} \subseteq {\rm K}[x]\{u_1,\ldots,u_n\} $$

${\rm K}[x]$ is a Noetherian ring (Theorem ??), so ${\rm
K}[x]\{u_1,\ldots,u_n\}$ is a Noetherian module (Theorem ??),
which means that ${\cal I}$, as a submodule, is Noetherian
and thus finitely generated (Theorem ??).

\endtheorem

\vfil\eject

\theorem
\label{submodules of free modules over PIRs are free}

Any submodule of a finite free module over a principal ideal ring is free.
([Lang] Theorem III.7.1)

\proof

Let $F = R\{w_1,\ldots,w_n\}$ be a free $R$-module ($R$ a principal
ideal ring) with a submodule $M$.  Consider $F_i =
R\{w_1,\ldots,w_i\}$, the free $R$-module generated by the first $i$
basis elements, and $M_i = F_i \cap M$.  We will show inductively that
all of the $M_i$ are free $R$-modules, and since $F_i = F$ and $M_i =
M$, this will prove the theorem.

First, consider $M_1 = R\{w_1\} \cap M$.  If $M_1$ is not empty (and
thus free), then any $m \in M_1$ can be written $r w_1$.  Since a
module forms an additive group, and we can operate on the module using
all the elements of $R$, it follows that all the $r$'s must form an
ideal, and since $R$ is principal, that ideal can be written with a
single generator, say $(r_1)$, and $M_1 = R\{r_1 w_1\}$ (or is empty).

Now, assume that $M_j$ is free for all $j<i$.  Consider all $x \in
M_i$, which can be written $r_1 w_1 + \cdots + r_i w_i$.  Either $M_i
= M_{i-1}$ (and is therefore free), or at least some of the $r_i$ are
non-zero.  By the same rationale as the last paragraph, these $r_i$
form an ideal, which can be written $(r_i)$.  Take any element $x \in
M_i$ with its $i^{\rm th}$ coefficient $r_i$ and add it to
$M_{i-1}$'s basis set to form a basis set for $M_i$, since some
multiple of this element can be used to cancel any $i^{\rm th}$
coefficient from an element in $M_i$ and leave an element in $M_{i-1}$
which can be formed using the remaining basis elements.

\endtheorem

A {\it torsion-free} module has no ``zero divisors'', in the sense
that no non-zero element of its associated ring can operate on a
non-zero element of the module and produce zero.  Since fields are
torsion-free, and all of our modules are subsets of the field
$K(x,y)$, they are all torsion-free.

\theorem
\label{finitely generated torsion-free modules over a PIR are free}

Any finitely generated, torsion-free module $M$ over a principal ideal
ring $R$ is free. ([Lang] Theorem III.7.3)

\proof

Take a maximal set of $M$'s linearly independent generators $\{w_1,
\ldots, w_n\}$ and any remaining generators $\{y_{n+1}, \ldots,
y_m\}$.  Every $y_i$ is therefore linearly dependant on $\{w_1,
\ldots, w_n\}$:

$$a_i y_i + r_1 w_1 + \cdots + r_n w_n = 0 \qquad a_i \ne 0$$

Take the product of all the $a_i$'s: $a = a_{n+1} a_{n+2} \cdots a_m$
and consider the mapping $x \mapsto ax$ which is injective, since $a
\ne 0$ and the module is torsion-free, so therefore maps $M$ to $aM$,
an isomorphic image which is a submodule of the free module
$R\{w_1,\ldots,w_n\}$.  By Theorem \ref{submodules of free modules
over PIRs are free}, $aM$ is therefore free, and since it is
isomorphic to $M$, we can take a basis of $aM$, divide all of its
basis elements by $a$ (they are all multiples of $a$), and obtain
a basis for $M$.

\endtheorem

Now, since $K[x]$ is a principal ideal ring (Theorem ??), Theorems
\ref{I is finitely generated} and \ref{finitely generated torsion-free
modules over a PIR are free} demonstrate that ${\cal I}$
is a free $K[x]$-module.

\definition

A basis for ${\cal I}$ will be called an {\bf integral basis}.

\enddefinition

\theorem

Any integral basis is also a basis for the $K(x,y)$ field as a
$K(x)$-module.

\endtheorem

While the preceding theorems offer an existance proof for an integral
basis, it is not immediately clear how to obtain one for any
particular field, and in fact the calculation of an integral basis
ultimately becomes one of the biggest computational barriers in this
theory.  Therefore, I will defer a more detailed discussion until a
later chapter, and instead present a simple construction for the
special case of a simple radical extension.

\definition

A basis is said to be a {\bf locally integral basis} at a place $p$ in
${\cal C}(x)$ if the basis elements are locally integral at all places
in ${\cal C}(x,y)$ over $p$ and the determinant of its conjugate
matrix assumes a finite, non-zero value at that place.

\enddefinition

\theorem

A function $f$ in ${\cal C}(x,y)$ has no poles over a place $p$ in
${\cal C}(x)$ iff $f$ can be formed from a basis locally integral at
$p$, using elements from ${\cal C}(x)$ locally integral at $p$.

\endtheorem


With a local integral basis (no pole at a place), there are 1-to-1
correspondances:

	use coeffs in K(x) w/ no pole at place <=> elem in K(x,y) w/ no pole

	use coeff w/ pole at place <=> elem in K(x,y) w/ pole at place
