
\mychapter{Algebraic Extensions}

{\bf THIS CHAPTER IS VERY VAGUE AND INCOMPLETE.}

We now turn to the algebraic extension in general.  Theorem ? allows
us to collapse any two adjacent algebraic extensions together, so we
need only consider an algebraic extension over a transcendental
extension.  The most basic case, the one that we've studied in the
last two chapters, is when the integrand involves only polynomials and
a single root, so we are integrating on an algebraic curve and our
algebraic extension occurs directly over the variable of integration:
${\bf C}(x,y)$.  However, many of these results are applicable in the
more general case where we have a series of field extensions that end
in a transcendental (exponential or logarithmic) extension followed by
an algebraic extension.  I'll use the notation ${\rm K}(\theta, y)$ to
emphasize when this is the case.

\mysection{Risch Equations over Simple Algebraic Extensions}

As a bit of a warm-up exercise, let's revisit a linguring topic from
Chapter 6: solving Risch equations.  We haven't yet developed the
tools to handle this problem in general algebraic extensions, but we
can use the tools of Chapter 7 to solve Risch equations in a simple
algebraic extension of $\CC(x)$.  This will allow us to solve integrals
involving exponentials of polynomial roots.  While such integrals
aren't terribly common, this exercise will reinforce the principles
of working over algebraic extensions.

Recall that a Risch equation has the form

\begin{equation}
r' + S r = T \qquad S,T,r \in K
\end{equation}

$K$ is a differential field, that in our case will be an algebraic
extension of $\CC(x)$:

\begin{equation}
\label{eq: simple algebraic Risch}
r' + S r = T \qquad S,T,r \in \CC(x) \mod p(x)
\end{equation}

Our approach in Chapter 6 was to identify the poles in $S$ and $T$
and then use this information to restrict $r$ enough to solve it.

This same approach works with algebraic extensions; we just need to
work with places and principal parts expansions instead of denominator
factors and partial fractions expansions.

We know that differentiating an algebraic function (w.r.t what?)
causes its finite poles to increase in order.  Therefore, we can't
have any finite poles at places where there aren't poles in at least
one of $S$ or $T$.  So, we find the poles of $S$ and $T$ and construct
principle parts expansions there.  If the order of $S$'s pole equals
the ramification index at the place, then the $r'$ and $S r$ terms
could cancel, so we look at $S$'s residue (is that the right term?)
to see if it's an integer.  If so, that gives us the strength
of the pole in $r$.

Otherwise, if the order of $S$'s pole is greater than the ramification
index, then $S r$ must cancel with $T$, and the difference between the
order of $T$'s pole and $S$'s pole gives us the order of $r$'s pole.
On the other hand, if the order of $S$'s pole is less than the
ramification index, then $r'$ must cancel with $T$, and the difference
between the order of $T$'s pole and the ramification index gives us
the order of $r$'s pole.

{\bf Summarize with a chart}

We also need to apply a similar analysis to the places at infinity,
the difference being that differentiating here can lower the order of
an $r'$ pole.

Once we've figured out the maximum orders of all of $r$'s poles, it's
then a straightforward matter to construct a suitable divisor and
compute a basis for its Riemann-Roch space.  Because of the $S r$
product, the resulting system of equations is not as simple as a
matrix equation, but can still be solved using fairly straightfoward
techniques of polynomial elimination.

\example
Integrate $\int \left(\frac{5x^4+2x-2}{x^2}\left(1+\frac{1}{\sqrt{x^3+1}}\right) + \frac{x}{\sqrt{x^3+1}}\right)e^{x\sqrt{x^3+1}} dx$

From \cite{bronstein algebraic curve}
\endexample

\cite{bronstein algebraic curve} gives a more sophisticated algorithm for attacking these
kinds of problems.


\mysection{Integral Elements}

\definition

An element $f \in {\rm K}(\theta,y)$ is {\bf integral} if it satisfies
a monic polynomial with coefficients in ${\rm K}[\theta]$.

\enddefinition

Intuitively, an integral element is one with no finite poles.
To see this, at least in the case where $K = {\bf C}$, define
$z = {1 \over f}$ and substitute this into $f$'s monic polynomial:

	$$f^n + a_{n-1} f^{n-1} + \cdots + a_1 f + a_0 = 0$$

	$$z^{-n} + a_{n-1} z^{-n+1} + \cdots + a_1 z^{-1} + a_0 = 0$$

	$$1 + a_{n-1} z + \cdots + a_1 z^{n-1} + a_0 z^n = 0$$

Now, if $z$ is zero at a place $p$ over $x=x_0$, at least one of this
polynomial's roots must be zero at $x=x_0$.  Since all of the $a_i$
are finite at $x=x_0$ (they are polynomials), multiplying any of them
by zero yields zero, so substituting in $z=0$ yields $1=0$. We
conclude that $z$ can not be zero, and thus $f$ can not have a pole
over $p$.

What's special about infinity?  Why not exclude some other place?
Well, nothing's all that special about infinity.  We've already seen
how a birational transformation can be used to swap infinity with any
finite point.  Demanding that a field element have no poles anywhere
is too restrictive, because Theorem ? tells us that such an element
must be constant.  So we want to relax this requirement slightly by
allowing poles over a single point.  We use infinity because it's
convenient.

It's not always obvious from inspection which functions are integral.
Something like $y \over x$, which appears to have a pole at $x=0$, is
actually integral if, for example, $y^2=x^3$.  Then we can consider
squaring $y \over x$ to obtain ${y^2 \over x^2} = {x^3 \over x^2} =
x$.  If the square is finite, then the original function had to be
finite (you can't square infinity and get a finite value), so we
conclude that $y \over x$ is, in fact, globally integral in ${\bf
C}(x,y); y^2=x^3$, as it satisfies the monic polynomial $f^2 - x = 0$.

Unfortunately, we have no straightforward means to construct such a
polynomial, or prove that one doesn't exist, for any particular
function $f$.  To test a function to determine if it is integral,
we'll need a more advanced approach.

%\definition
%
%A field's {\bf local ring} at a place $p$ is the set of
%all functions in the field with no pole at $p$; that
%is, $ \lim_{x\to p} f(x) \ne \infty $.
%
%A function is {\bf locally integral} at a place $p$
%if it belongs to its field's local ring at $p$.
%
%\enddefinition
%
%Remember the distinction I drew between a {\it pole} and a {\it
%removable singularity} is Chapter ?.  That's the reason for the limit
%in the definition; removable singularities are specifically included
%in a local ring.  Thus, $x\over\sqrt{x}$ is locally integral at $x=0$,
%despite the fact that it has a zero denominator, because
%$\lim_{x\to0}{x\over\sqrt{x}}=\sqrt{x}=0$, a finite value.
%
%It is often important, however, to specify which field we are
%refering to when we discuss a local ring.
%
%\example \quad
%
%In the field ${\cal C}(x)$, the function $x$ is locally integral at $x=0$.
%
%In the field $F = {{\cal C}(x,y); y^2=x+1}$, the function $x$ is locally integral
%at both $(x=0, y=1)$ and $(x=0, y=-1)$.  There is no place $x=0$ in
%this field; to speak of it so is ambiguous, as there are two places
%in $F$ where $x=0$.
%
%Likewise, $y$ is locally integral in $F$ at both $(x=0, y=1)$ and
%$(x=0, y=-1)$.  It is not locally integral in ${\cal C}(x)$ at $x=0$ simply
%because $y$ does not exist in the field ${\cal C}(x)$.
%
%\endexample
%
%Thus, a function like $x$ can be locally integral in both ${\cal C}(x)$ and
%${\cal C}(x,y)$; it is important to specify which we are talking about.
%
%Since the fields ${\cal C}(x)$ and ${\cal C}(x,y)$ are distinct, and have different
%places, they also have different local rings.  There is an important
%connection between them, however.
%
%\theorem
%\label{local integral polynomial}
%
%A function $f$ is locally integral at a place $p$ in ${\cal C}(x,y)$ if it
%satisfies a monic polynomial with coefficients in the local ring of
%${\cal C}(x)$ at the corresponding place in that field.
%
%\proof
%
%
%\endtheorem
%
%The converse is not true; just because a function does not satisfy
%such a monic polynomial does not mean that it is not locally integral.
%A slightly weaker converse does hold, however:
%
%\theorem
%
%A function $f$ in ${\cal C}(x,y)$ which is locally integral at all
%places in ${\cal C}(x,y)$ over a place $p$ in ${\cal C}(x)$ satisfies
%a monic polynomial with coefficients in the local ring of ${\cal C}(x)$
%under $p$.
%
%\proof
%
%Construct the polynomial
%
%$$(z - f(x,y))(z - f(x,y_2)) \cdots (z - f(x,y_n)) = 0$$
%
%$f$ clearly satisfies this polynomial at every place over $p$.
%Furthermore, multiplying out the terms yields:
%
%$$\matrix{z^n & - (f(x,y)+f(x,y_2)+\cdots+f(x,y_n)) z^{n-1} \hfill\cr
%   & + \quad (f(x,y)f(x,y_2)+f(x,y)f(x,y_3)+\cdots+f(x,y_{n-1})f(x,y_n)) z^{n-2} \hfill\cr
%   & + \cdots + f(x,y)f(x,y_2)\cdots f(x,y_n) = 0 \hfill\cr}$$
%
%All of these coefficients are symmetric functions in $y_i$, and thus
%exist in ${\cal C}(x)$.  Furthermore, since $f$ is finite (by
%assumption) at all conjugate values over $p$, each of these
%coefficients must also be finite at $p$ (since they are constructed by
%addition and multiplication from the conjugate values of $f$), and are
%thus in the local ring of ${\cal C}(x)$ at $p$.
%
%\endtheorem
%
%In short, a function in ${\cal C}(x,y)$ which satisfies a monic
%polynomial whose coefficients are in a local ring of ${\cal C}(x)$ at
%a point $p$ is locally integral in ${\cal C}(x,y)$ at all places over
%$p$.  A function which does not satisfy such a polynomial has
%a pole at at least one place over $p$.
%
%A more advanced, more purely algebraic approach to this subject would
%{\it define} local rings using Theorem \ref{local integral
%polynomial}, then use valuations and completions to {\it prove} the
%connection between local rings and poles (or the lack thereof).  See
%[van der Waerden], Chapter 18.  I will forego this in favor of this
%simpler, more analytic (note the use of the limit) approach.
%
%
%Having established the existence and basic properties of {\it locally}
%integral elements, we can now discuss {\it globally} integral
%elements.  Obviously, elements in a field will be locally integral at
%more than one place.  In fact, field elements will be locally integral
%at almost all places (because they only have a finite number of
%poles).  So, intuitively, a {\it globally} integral element would be
%one that is integral at every place in a field.  However, Theorem ?
%immediately tell us that the only such functions (those which have no
%poles anywhere) are the constants, so this turns out to be too
%restrictive to be useful.  Instead, we exclude infinity and define:
%
%\definition
%
%A function is {\bf globally integral} in a field if it is locally
%integral at all {\it finite} places in the field.
%
%\enddefinition
%
%
%We could use Theorem \ref{local integral polynomial} and the fact
%that, in ${\cal C}(x,y); y^2=x^3$,
%
%	$$ \left({y \over x}\right)^2 - x = 0 $$
%
%to conclude that, since this is a monic polynomial with coefficients
%globally integral in ${\cal C}(x)$, $y \over x$ must be globally
%integral in ${\cal C}(x,y); y^2=x^3$.  Theorem \ref{local integral
%polynomial} can be easily generalized:
%
%\theorem
%
%A function $f$ is globally integral in ${\cal C}(x,y)$ if it
%satisfies a monic polynomial with globally integral coefficients in
%${\cal C}(x)$.
%
%\proof
%
%\endtheorem
%
%Proof that globally integral elements form a finite extension:
%[Eichler], p. 54
%
%Proof that field elements are determined by their divisors, up to a
%constant multiple: [Eicher], p. 84
%
%[Eicher], p. 86: Prime divisors are isomorphic with special local rings.
%

\mysection{Modules}

We'll resort now to {\it modules}, a fairly important algebra concept
backed by a substantial body of theory, upon which I shall only draw
as needed.  General references include [Atiyah+McDonald] and [Lang].

\definition

An {\it R-module} over a ring R is an additive group M acted on by R
(i.e, there is a mapping $R \times M \to M$) in a distributive manner:

$$(r_1 + r_2)m = r_1 m + r_2 m \qquad r_1, r_2 \in {\rm R};\, m \in {\rm M}$$

where we have adopted the usual convention of writing R's action on M
as a multiplication.

\enddefinition

\definition

A {\it free R-module} is an R-module spanned by a linearly independent basis
$\{b_1, b_2, ... b_n\}$.  It consists of all elements formed as follows:
\footnote{I'll also note that a multiplication rule needs to be specified
between the basis elements and the elements of the ring, and an
addition rule between the elements of the module.  Also,
the expression has to be {\it unique} --- you can't be
able to write an element two different ways.  In our
case, these rules are obvious, but that's not always the case.}

	$$ a_1 b_1 + a_2 b_2 + ... + a_n b_n; \qquad a_i \in R $$


\enddefinition

Not all modules have a finite set of generators, and not all those
have a linearly independent set of generators.  Elements formed from a
basis can be added by using the module's distributive property to
factor out the coefficients from of each basis element and then
performing the addition in the ring R:

$$ (a_1 b_1 + a_2 b_2 + ... + a_n b_n) + (c_1 b_1 + c_2 b_2 + ... + c_n b_n) $$
$$  = (a_1 + c_1) b_1 + (a_2 + c_2) b_2 + ... + (a_n + c_n) b_n $$

So the elements generated from a basis clearly form a module.  R operates
on them by multiplication by every coefficient.

\example \quad
\label{sample modules}

An ideal I in a ring R is a R-module, but a subring S of R, in
general, is not, because multiplication by an element of R might not
produce a result in the subring.  R, however, can always be viewed as
an S-module.

% XXX
% If R is a principal ideal ring, then every ideal is also a
% free R-module, admitting a basis consisting of a single
% element, a generator of the ideal.
%
% Is this true?  Any zero divisor counterexamples?

\endexample


Note that it is vitally important to specify the ring used for the
coefficients.  For example, consider the basis $\{1, y\}$.  Treating
this as a ${\bf C}(x)$-module, I can form ${y \over x} = {1 \over x}
y$, since ${1 \over x} \in {\bf C}(x)$.  However, $y \over x$ does
{\it not} belong to the ${\bf C}[x]$-module generated by $\{1, y\}$.
I would need to use polynomial coefficients to form a ${\bf
C}[x]$-module, not the rational functions coefficients allowed in a
${\bf C}(x)$-module.  We'll be primarily interested in ${\rm
K}[\theta]$-modules, ${\rm K}(\theta)$-modules, and ${\cal
I}$-modules, where ${\cal I}$ is the ring of integral elements in
${\rm K}(\theta,y)$.


% \vfil\eject
% \mysection doesn't work because of the math in the section title
\section{The ${\rm K}[\theta]$-module ${\cal I}$}

Since polynomials have no finite poles, they are integral elements,
and thus ${\rm K}[\theta] \subseteq {\cal I}$.  Thus, ${\cal I}$ (the
ring of integral elements) is trivially a ${\rm K}[\theta]$-module
(see Example \ref{sample modules}), but what is not nearly so obvious
is that it is also a free module, a fact which underlies a great deal
of our theory.  I'll prove this first by showing that ${\cal I}$ is
finitely generated as a ${\rm K}[\theta]$-module, then showing the
existance of a linearly independent set of generators.

Let's start with a preliminary theorem.

\theorem
\label{construction of dual basis}

If $\{w_1,\ldots,w_n\}$ is a basis for a finite separable field
extension $E/K$, then a dual basis $\{u_1,\ldots,u_n\}$ can be
constructed such that ${\rm Tr}(w_i u_j) = \delta_{ij}$.
([Lang] Corollary VI.5.3)

\proof

Consider the following matrix:

$$M = \begin{pmatrix}{\rm Tr}(w_1 w_1) & & \cr \vdots & \ddots & \cr {\rm Tr}(w_1 w_n) & \cdots & {\rm Tr}(w_n w_n)\end{pmatrix}$$

Now take an element $x \in E$, and represent it relative to the basis
$\{w_1,\ldots,w_n\}$ as a row vector $X = (x_i)$.  Multiplying $X M$
produces a row vector whose $j^{\rm th}$ element can be written:

$$\sum_i x_i {\rm Tr}(w_j w_i) = {\rm Tr}(w_j \sum_i x_i w_i) = {\rm Tr}(w_j x) = {\rm Tr}_x(w_j)$$

where I used first the $K$-linearity and additive distributive
properties of ${\rm Tr}$, then wrote ${\rm Tr}_x: f(a) = {\rm Tr}(ax)$
to emphasize that I'm regarding ${\rm Tr}_x$ as a linear form in ${\rm
Hom}_E(E,K)$.  So, if $M$ is singular, then there exists some non-zero
element $x$ such that ${\rm Tr}_x$ is zero for all of $w_i$, which
form a basis set, so ${\rm Tr}_x$ must therefore be the zero map.
This can only happen if ${\rm Tr}$ is identically zero, which would be
the case for an inseparable extension.  For the separable case,
therefore, $M$ must be invertible, and we can write:

$$M^{-1} M = \begin{pmatrix}1 & & \cr & \ddots & \cr & & 1\end{pmatrix}$$

A moment's thought now shows that the rows of $M^{-1}$ are the desired
dual basis elements, written with respect to $\{w_1,\ldots,w_n\}$.

\endtheorem

% \vfil\eject

\theorem
\label{I is finitely generated}

${\cal I}$ is a finitely generated ${\rm K}[\theta]$-module.
([A+MacD] Proposition 5.17; [Lang] Exercise VII.3)

\proof

Regarding ${\rm K}(\theta,y)$ as a vector space over ${\rm
K}(\theta)$, we can easily construct a basis of integral elements by
starting with $\{1, y, \ldots, y^{n-1}\}$ and multiplying each element
(if needed) by a polynomial in $\theta$ which cancels all of its poles:

$${\rm K}(\theta,y) = {\rm K}(\theta)\{w_1,\ldots,w_n\} \qquad \forall i(w_i \in {\cal I})$$

Using Theorem \ref{construction of dual basis}, construct a dual basis
$\{u_1,\ldots,u_n\}$ so that ${\rm Tr}(w_i u_j) = \delta_{ij}$.  Take
any $x \in {\cal I}$ and write it using the dual basis:

$$x = \sum_i a_i u_i \qquad a_i \in {\rm K}(x)$$

Now consider ${\rm Tr}(x w_j)$.  Now, $x$ and $w_j$ are both in ${\cal
I}$, so $x w_j$ is in ${\cal I}$, and therefore has a monic minimum
polynomial with coefficients in ${\rm K}[\theta]$.  Since ${\rm Tr}$ equals
some integer multiple of the negative of the second coefficient in a
monic minimum polynomial, ${\rm Tr}(x w_j) \in {\rm K}[\theta]$.  But also,

$${\rm Tr}(x w_j) = {\rm Tr}(\sum_i a_i u_i w_j)
 = \sum_i {\rm Tr}(a_i u_i w_j) %\hfil (Tr an additive homomorphism)
 = \sum_i a_i {\rm Tr}(u_i w_j) %\hfil (linearity of Tr)
 = a_i$$

which establishes that $\forall i (a_i \in {\rm K}[\theta])$, so

$${\cal I} \subseteq {\rm K}[x]\{u_1,\ldots,u_n\} $$

${\rm K}[\theta]$ is a Noetherian ring (Theorem ??), so ${\rm
K}[\theta]\{u_1,\ldots,u_n\}$ is a Noetherian module (Theorem ??),
which means that ${\cal I}$, as a submodule, is Noetherian
and thus finitely generated (Theorem ??).

\endtheorem

% \vfil\eject

\theorem
\label{submodules of free modules over PIRs are free}

Any submodule of a finite free module over a principal ideal ring is free.
([Lang] Theorem III.7.1)

\proof

Let $F = R\{w_1,\ldots,w_n\}$ be a free $R$-module ($R$ a principal
ideal ring) with a submodule $M$.  Consider $F_i =
R\{w_1,\ldots,w_i\}$, the free $R$-module generated by the first $i$
basis elements, and $M_i = F_i \cap M$.  We will show inductively that
all of the $M_i$ are free $R$-modules, and since $F_i = F$ and $M_i =
M$, this will prove the theorem.

First, consider $M_1 = R\{w_1\} \cap M$.  If $M_1$ is not empty (and
thus free), then any $m \in M_1$ can be written $r w_1$.  Since a
module forms an additive group, and we can operate on the module using
all the elements of $R$, it follows that all the $r$'s must form an
ideal, and since $R$ is principal, that ideal can be written with a
single generator, say $(r_1)$, and $M_1 = R\{r_1 w_1\}$ (or is empty).

Now, assume that $M_j$ is free for all $j<i$.  Consider all $x \in
M_i$, which can be written $r_1 w_1 + \cdots + r_i w_i$.  Either $M_i
= M_{i-1}$ (and is therefore free), or at least some of the $r_i$ are
non-zero.  By the same rationale as the last paragraph, these $r_i$
form an ideal, which can be written $(r_i)$.  Take any element $x \in
M_i$ with its $i^{\rm th}$ coefficient $r_i$ and add it to
$M_{i-1}$'s basis set to form a basis set for $M_i$, since some
multiple of this element can be used to cancel any $i^{\rm th}$
coefficient from an element in $M_i$ and leave an element in $M_{i-1}$
which can be formed using the remaining basis elements.

\endtheorem

A {\it torsion-free} module has no ``zero divisors'', in the sense
that no non-zero element of its associated ring can operate on a
non-zero element of the module and produce zero.  Since fields are
torsion-free, and all of our modules are subsets of the field
${\rm K}(\theta,y)$, they are all torsion-free.

\theorem
\label{finitely generated torsion-free modules over a PIR are free}

Any finitely generated, torsion-free module $M$ over a principal ideal
ring $R$ is free. ([Lang] Theorem III.7.3)

\proof

Take a maximal set of $M$'s linearly independent generators $\{w_1,
\ldots, w_n\}$ and any remaining generators $\{y_{n+1}, \ldots,
y_m\}$.  Every $y_i$ is therefore linearly dependant on $\{w_1,
\ldots, w_n\}$:

$$a_i y_i + r_1 w_1 + \cdots + r_n w_n = 0 \qquad a_i \ne 0$$

Take the product of all the $a_i$'s: $a = a_{n+1} a_{n+2} \cdots a_m$
and consider the mapping $x \mapsto ax$ which is injective, since $a
\ne 0$ and the module is torsion-free, so therefore maps $M$ to $aM$,
an isomorphic image which is a submodule of the free module
$R\{w_1,\ldots,w_n\}$.  By Theorem \ref{submodules of free modules
over PIRs are free}, $aM$ is therefore free, and since it is
isomorphic to $M$, we can take a basis of $aM$, divide all of its
basis elements by $a$ (they are all multiples of $a$), and obtain
a basis for $M$.

\endtheorem

Now, since ${\rm K}[\theta]$ is a principal ideal ring (Theorem ??),
Theorems \ref{I is finitely generated} and \ref{finitely generated
torsion-free modules over a PIR are free} demonstrate that ${\cal I}$
is a free ${\rm K}[\theta]$-module.

\definition

A basis for ${\cal I}$ will be called an {\bf integral basis}.

\enddefinition

\theorem

Any integral basis is also a basis for the ${\rm K}(\theta,y)$ field as a
${\rm K}(\theta)$-module.

\endtheorem

While the preceding theorems offer an existance proof for an integral
basis, it is not immediately clear how to obtain one for any
particular field, and in fact the calculation of an integral basis
ultimately becomes one of the biggest computational barriers in this
theory.  Therefore, I will defer a more detailed discussion until a
later chapter, and instead present a simple construction for the
special case of a simple radical extension.

%\definition
%
%A basis is said to be a {\bf locally integral basis} at a place $p$ in
%${\cal C}(x)$ if the basis elements are locally integral at all places
%in ${\cal C}(x,y)$ over $p$ and the determinant of its conjugate
%matrix assumes a finite, non-zero value at that place.
%
%\enddefinition
%
%\theorem
%
%A function $f$ in ${\cal C}(x,y)$ has no poles over a place $p$ in
%${\cal C}(x)$ iff $f$ can be formed from a basis locally integral at
%$p$, using elements from ${\cal C}(x)$ locally integral at $p$.
%
%\endtheorem
%
%
%With a local integral basis (no pole at a place), there are 1-to-1
%correspondances:
%
%	use coeffs in K(x) w/ no pole at place <=> elem in K(x,y) w/ no pole
%
%	use coeff w/ pole at place <=> elem in K(x,y) w/ pole at place

\mysection{Basis for all Rational Functions}

The first kind of basis we're interested in, a {\it basis for all
rational functions}, is one than spans the entire ${\cal C}(x,y)$ field
as a ${\cal C}(x)$-module.
In other words, we're looking for a basis $\{b_1, b_2,
... b_n\}$ so that everything in ${\cal C}(x,y)$ can be expressed
in the form:

	$$ a_1 b_1 + a_2 b_2 + ... + a_n b_n; a_i \in {\cal C}(x) $$

Such a basis will always have $n$ elements, where $n$ is the degree of
the ${\cal C}(x,y)$ extension over ${\cal C}(x)$, and can be most
conveniently characterized using its {\it conjugate matrix}:

\definition

The {\bf conjugates} of a rational function $\eta(x,y)$ in ${\bf
C}(x,y)$ are the functions formed by replacing $y$ with its conjugate
values.

The {\bf trace} of a rational function $\eta(x,y)$ is the sum of
its conjugates:

$${\rm T}(\eta(x,y)) = \sum_i \eta(x,y_i)$$

The {\bf norm} of a rational function $\eta(x,y)$ is the product of
its conjugates:

$${\rm N}(\eta(x,y)) = \prod_i \eta(x,y_i)$$

Both the trace and norm, as symmetric functions in $y_1,...,y_n$, are
functions in ${\bf C}(x)$.

The {\bf conjugate matrix} ${\bf M}_\omega$ of $n$ elements $\omega_i$
in ${\cal C}(x,y)$, where $n$ is the degree of ${\cal C}(x,y)$ over
${\cal C}(x)$, is the matrix whose each row consists of the $n$
conjugate values of a single element, and whose $n$ rows are formed in
this way from the $n$ elements.

A set of $n$ elements $\omega_i \in {\bf C}(x,y)$ form a {\bf rational
function basis} for ${\bf C}(x,y)$ if the determinant of their
conjugate matrix is non-zero: $|{\bf M}_\omega| \ne 0$

\enddefinition

\definition

For any function $\eta \in {\bf C}(x,y)$ and any rational function
basis $\omega_i$, the {\bf trace vector}
${\bf T}_{\eta/\omega} = \Big({\rm T}(\eta \omega_i)\Big)$ 
of $\eta$ relative to $\omega$
is formed from the
traces of the $n$ products of $\eta$ with the $n$ functions
$\omega_i$.

The {\bf conjugate vector} ${\bf C}_\eta = (\eta(x,y_i))$ is formed from the
$n$ conjugates of $\eta$.

\enddefinition

\theorem
\label{function is zero if trace vector is zero}

For any function $\eta \in {\bf C}(x,y)$ and any rational function
basis $\omega_i$, if ${\bf T}_{\eta/\omega}$ is the zero vector,
then $\eta$ is zero.

\proof

${\bf T}_{\eta/\omega}$, ${\bf M}_\omega$ and ${\bf C}_\eta$
satisfy the matrix equation

$${\bf T}_{\eta/\omega} = {\bf M}_\omega {\bf C}_\eta$$

since each row of this matrix equation has the form

$$ {\rm T}(\eta \omega_i) = \sum_j \omega_i(x,y_j)\eta(x,y_j) $$

Since ${\bf M}_\omega$ is invertible (since its determinant is
non-zero), if ${\bf T}_{\eta/\omega}$ is identically zero, then so must be
${\bf C}_\eta$, and $\eta$ is the first element in ${\bf C}_\eta$.

\endtheorem

\theorem
\label{|M| != 0 implies C(x) basis}

A rational function basis $\omega_i$ spans ${\bf C}(x,y)$ as
a ${\bf C}(x)$-module. ([Bliss], Theorem 19.1)

\proof

Note that when we multiply ${\bf M}_\omega$ by its transpose ${\bf
M}_\omega^T$, the $ij^{\rm th}$ element of ${\bf M}_\omega{\bf
M}_\omega^T$ is:

$$ \sum_k \omega_i(x, y_k)\omega_j(x, y_k) = {\rm T}(\omega_i \omega_j)$$

Since $|{\bf M}_\omega|$ is non-zero, $|{\bf M}_\omega^T|$ is
non-zero, and $|{\bf M}_\omega{\bf M}_\omega^T|$ is non-zero, so given
any function $\eta \in {\bf C}(x,y)$, we can solve the following
equation for ${\bf R}$:

$${\bf T}_\eta = {\bf M}_\omega {\bf M}_\omega^T {\bf R}$$

each of row of which reads:

$$ {\rm T}(\eta \omega_i) = \sum_j {\rm T}(\omega_i \omega_j) r_j $$

Since both ${\bf T}_\eta$ and ${\bf M}_\omega{\bf M}_\omega^T$ are composed of
nothing but traces, they exist in ${\bf C}(x)$, so ${\bf R}$ must also
exist in ${\bf C}(x)$ and its elements therefore commute with the
trace:

$$ {\rm T}(\eta \omega_i) = \sum_j {\rm T}(r_j \omega_j \omega_i) $$

Since the trace of a sum is the sum of the traces:

$$ {\rm T}(\eta \omega_i) = {\rm T}(\sum_j r_j \omega_j \omega_i) $$
$$ {\rm T}((\eta - \sum_j r_j \omega_j) \omega_i) = 0 $$

which implies that $\eta = \sum_j r_j \omega_j$, by Theorem
\ref{function is zero if trace vector is zero}, and since we've
already shown that the $r_j$ are rational functions in ${\bf C}(x)$,
this proves the theorem.

\endtheorem

Let me illustrate with a simple example.

\example

Consider the basis $\{1, y\}$ over the field ${\cal C}(x,y); y^2=x$.
The conjugate value of $y$ is $-y$ (PROVE THIS), so the conjugate
matrix is:

$$C=\left(\begin{matrix}1&1\cr y&-y\cr\end{matrix}\right)$$

and its determinant:

$$\det C=\left|\begin{matrix}1&1\cr y&-y\cr\end{matrix}\right| = -2y$$

Since $-2y$ is not zero, we conclude that $\{1, y\}$ is a basis
for all rational functions over ${\cal C}(x,y); y^2=x$.

\endexample

Notice that I didn't ask whether $-2y$ was zero at some place in the
field.  The determinant of the conjugate matrix can be zero at certain
places; in fact, often is.  It just can't be {\it identically} zero;
i.e, it can't be zero {\it everywhere}.  If this isn't clear, reread
Theorems \ref{function is zero if trace vector is zero} and \ref{|M|
!= 0 implies C(x) basis}, noting that all the matrices are defined
over the {\it fields} ${\bf C}(x)$ and ${\bf C}(x,y)$, where the only
zero element is 0.


% \vfill\eject
\mysection{Divisors and Integral Modules}

In ${\bf C}(x)$, we were working with the quotient field of a
principal ideal ring, so we could always find a single function to
generate any finitely generated ${\bf C}[x]$-module, simply by putting
all the generators over a common denominator, then taking the
G.C.D. of the numerators.

In ${\rm K}(\theta,y)$, we are no longer working with a principal ideal
ring, so we can't guarantee that any particular ideal can be generated
by a single function, but it turns out that every ideal can be
generated by a {\it pair} of functions.  Our course of attack is first
to construct that pair of functions, then use them to determine if in
fact the ideal is principal.

\definition

An {\bf integral module} (or ${\cal I}$-module) is a module formed
over ${\cal I}$, the ring of integral elements in ${\rm K}(\theta,y)$.

\enddefinition

Since ${\cal I}$ itself can be expressed as a ${\rm K}[\theta]$-module
using an integral basis, any ${\cal I}$-module is also a ${\rm
K}[\theta]$-module.  Not all ${\rm K}[\theta]$-modules are ${\cal
I}$-modules, however, since ${\cal I}$ is typically larger than ${\rm
K}[x]$.

% Existance of an integral basis is demonstrated by Atiyah and MacDonald
% via Propositions 5.17, 6.5, and 6.2.  Van der Waerden proves this on
% pp. 174-175 of volume 2.  That ${\cal I}$ is a free module can be
% proven using Lang's theorem 7.3.

Some authors use the term {\it fractional ideal} to refer to an ${\cal
I}$-module.  I have avoided use of this term for two reasons.  First,
I wish to emphasize the concept of a module.  Second, ${\cal
I}$-modules are not ideals, either in the ring ${\cal I}$ (since they
may contain elements not in ${\cal I}$), nor in the field ${\rm
K}(\theta,y)$, since, as a field, ${\rm K}(\theta,y)$ has only the trivial
ideals.  The term {\it fractional ideal} is used because an ${\cal
I}$-module can be regarded as a fraction of ideals in ${\cal I}$.

\theorem ${\cal I}$ is a Noetherian ring.
\label{I is Noetherian}

\proof

Since ${\rm K}$ is a field, ${\rm K}[\theta]$ is a Noetherian ring by
the Hilbert basis theorem, ${\rm K}[\theta] \subseteq {\cal I}$, and
${\cal I}$ is finitely generated as a ${\rm K}[\theta]$-module, so
${\cal I}$ is a Noetherian ring by [Atiyah+McDonald] Proposition 7.2.

\endtheorem

\theorem
\label{order of norm}
The order of the norm of $f$ at a point $\theta_0$ is the sum of the orders
of $f$ at all places over $\theta_0$.

\endtheorem


\theorem
\label{simple zero construction}

A function can always be constructed with a simple zero at a specified
finite, ordinary place $(\alpha, \beta)$, zero order at an additional
finite set of finite, ordinary places $\Sigma$, and non-negative order
at all other finite places.

\proof

Begin with the function $(x-\alpha)$, which is a uniformizing variable
and thus has a simple zero at $(\alpha, \beta)$.  If none of the other
places in $\Sigma$ have x-value $\alpha$, then we are done, since
$(x-\alpha)$ has no finite poles.

Otherwise, compute $(x-\alpha)\over(y-\beta)$ at all places in
$\Sigma$ that do {\it not} have $y = \beta$.  Select a number
$\gamma$ different from all of these values.  The function $(x-\alpha)
- \gamma (y-\beta)$ has no finite poles and
is non-zero at all places in $\Sigma$, but it may now have a zero of
higher order at $(\alpha, \beta)$.  Consider a series expansion
of $y$ in terms of $(x-\alpha)$:

$$y = \beta + c_1 (x-\alpha) + c_2 (x-\alpha)^2 + \cdots$$

So long as $\gamma$ is also selected different from $c_1$, $(x-\alpha)
- \gamma (y-\beta)$ will have a first order zero at $(\alpha, \beta)$
and meet all requirements of the theorem.  The simplest way to do this
is to pick a value for $\gamma$, use Theorem \ref{order of norm} to
check if the function has a simple zero, and if not, choose a
different value for $\gamma$.

% SINGULARITIES?

\endtheorem

\theorem
\label{simple pole construction}

A function can always be constructed with a simple pole at a specified
finite, ordinary place $(\alpha, \beta)$, zero order at an additional
finite set of finite, ordinary places $\Sigma$, and non-negative order
at all other places.

\proof

Begin with the function:

$$f(\alpha,y)\over(x-\alpha)(y-\beta)$$

where $f(x,y)$ is the minimum polynomial of the algebraic extension.
Note that the division by $(y-\beta)$ will always be exact, since
$f(\alpha, \beta)=0$.  So we have a rational function
$P(y)\over(x-\alpha)$, where $P(y)$ is a polynomial in $y$.  It has a
simple pole at $(\alpha, \beta)$, as can be seen from a series
expansion in $x-\alpha$ (again, a uniformizing variable).  Since the
$y-\beta$ factor has been divided out of $f(\alpha,y)$, the numerator
is non-zero at $(\alpha, \beta)$, so the leading term in the series
expansion involves $(x-\alpha)^{-1}$, and the pole is thus simple.

This function is finite at all other places, which is obvious except
when $x=\alpha$ and $y\ne\beta$, where it takes the form $0\over0$,
so we can expand it using L'H\^opital's rule:

$$\lim_{(x,y)\to p} {{P(y)}\over(x-\alpha)}
  = \lim_{{(x,y)\to p}} {{{dP(y)}\over{dx}}\over{{d(x-\alpha)}\over{dx}}}
  = P'(y) \, {{dy}\over{dx}} $$

where $'$ denotes differentiation with respect to the polynomial's
variable.  $P'(y)$ is a polynomial, and is thus finite where $y$ is
finite, as is ${dy}\over{dx}$ (consider a series expansion of $y$ in
terms of $(x-\alpha)$, since all places in $\Sigma$ are finite and
non-singular).  It follows that the function is at least finite
everywhere except at $(\alpha, \beta)$.

% This function is finite at all other places, which is obvious except
% when $x=\alpha$ and $y\ne\beta$, where it takes the form $0\over0$.
A more algebraic way to prove this is to note that
$f(x,y)$ has a simple zero at every place over $x=\alpha$ (assuming
there are no multiple points over $x=\alpha$), so $P(y)$ will have a
simple zero at every place over $x=\alpha$ except $(\alpha, \beta)$,
which will exactly cancel the simple pole from $(x-\alpha)$.

Now, compute the value of the function at all other places in
$\Sigma$, using either L'H\^opital's rule or Puiseux expansion if some
of these are over $\alpha$.  If the value of the function is non-zero
at all of these places, then we are done.  Otherwise, select a number
$\gamma$ different from all of these values.  The function:

$${f(\alpha,y)\over(x-\alpha)(y-\beta)} - \gamma$$

has the desired properties, since it still has a simple pole at
$(\alpha,\beta)$, has no other poles, and is now non-zero at all
places in $\Sigma$.

We can avoid computing any expansions by picking random values of
$\gamma$, and using Theorem \ref{order of norm} to check for any extra
zeros.  Since only a finite number of $\gamma$ values produce extra
zeros, this process is guaranteed to terminate.

% SINGULARITIES?

\endtheorem

\theorem
\label{finite orders construction}

A function can always be constructed with specified integer orders at
a finite set of finite, non-singular places $\Sigma$ and non-negative
order at all other finite places.

\proof

For each pole or zero, use Theorems \ref{simple zero construction} or
\ref{simple pole construction} to construct a function with a simple
pole or a simple zero at that place, zero order at all other places in
$\Sigma$ and non-negative order at all other finite places.  Raise
each of these function to the integer power that is the order of the
corresponding pole or zero, then multiply them all together.

\endtheorem

\definition

A {\bf finite multiple} of a divisor is a function with order equal to
or greater than that required by the divisor at all {\it finite} places.

\enddefinition

For the remainder of this section, I'll assume that our divisors
involve only finite, ordinary places, which can always be guaranteed
in the case of the integration theory.

\theorem
\label{exact order existance}

For any divisior ${\cal D}$ and any finite, non-singular place
$\mathfrak{p}$, at least one finite multiple of ${\cal D}$ exists with
order at $\mathfrak{p}$ exactly that required by ${\cal D}$.

\proof

Use Theorem \ref{finite orders construction} with the zeros and poles
required by ${\cal D}$, adding $\mathfrak{p}$
to $\Sigma$ if necessary.

\endtheorem

\theorem
\label{divisor-module isomorphism}

There is a one-to-one relationship between finitely generated integral
modules and divisors.  Such a module consists of all finite multiples
of its associated divisor, and the order of a module's divisor at
every finite place is the minimum of the orders of the module's
generators at that place.

\proof

For a given divisor ${\cal D}$, consider the set ${\cal M}({\cal D})$
of all finite multiples of ${\cal D}$.
Now, adding two
elements can not reduce their order at any finite place, nor can
multiplying an element by an integral element $i \in {\cal I}$, so
${\cal M}({\cal D})$ is clearly an ${\cal I}$-module, but it
might not be finitely generated.

Since ${\cal D}$ has only a finite number of poles, we can always
construct a function with order equal or less than that of ${\cal D}$
at all finite places simply by taking the inverse of the polynomial
$r=(x-p_1)^{m_1} \cdots (x-p_n)^{m_n}$ where $p_i$ are the x-coordinates
of the poles in ${\cal D}$ and $m_i$ are their multiplicities.
For any $m \in {\cal M}({\cal D})$, $mr$ is integral, so ${\cal M}({\cal D})
\subseteq {\cal I}\{r^{-1}\}$, where ${\cal I}\{r^{-1}\}$ is the ${\cal I}$-module
generated by $r^{-1}$.  Now, since ${\cal I}\{r^{-1}\}$ is a finitely
generated module over a Noetherian ring (remember Theorem \ref{I is
Noetherian}), ${\cal I}\{r^{-1}\}$ is a Noetherian module by
[Atiyah+McDonald] Proposition 6.5, and ${\cal M}({\cal D})$ must also
be a finitely generated ${\cal I}$-module by [Atiyah+McDonald]
Proposition 6.2.

Let $(b_1,...,b_n)$ be an ${\cal I}$-module basis for ${\cal M}({\cal
D})$.  Since there is no way to lower the orders of an element using
${\cal I}$-module constructions, and by Theorem \ref{exact order
existance} for each place there is at least one function in ${\cal
M}({\cal D})$ with order exactly that required by ${\cal D}$, it
follows that for each place there must be at least one basis element
with exactly the order required by ${\cal D}$.  Futhermore, no basis
element can have an order less than required by ${\cal D}$ at any
finite place, since that element would not be a finite multiple of
${\cal D}$.  Therefore, at each place $\mathfrak{p}$, the minimum of
the orders of the basis elements must be exactly the order required by
${\cal D}$.

Conversely, given a finitely generated ${\cal I}$-module $M$,
construct its associated divisor ${\cal D}$ by taking at every place
the minimum of the orders of the module's generators at that place.
Clearly, $M \subseteq {\cal M}({\cal D})$, but some finite multiple of
${\cal D}$ might not be in $M$.

To eliminate this possibility, take the module's generators, say
$\{b_1, b_2, b_3\}$ and expand them into a set where each additional
generator beyond the first lowers the module's order by one at a
single place, say $\{b_1, b_2', b_2, b_3'', b_3', b_3\}$.  These
additional generators can be constructed by multiplying the original
generators by integral elements (constructed using Theorem \ref{finite
orders construction}) to remove any additional poles, so $b_2' = i_2'
b_2$, where $i_2' \in {\cal I}$.

This new module $M'$ clearly has the same associated divisor as $M$,
and I'll now show inductively that any $f \in {\cal M}({\cal D})$ can
be found in $M'$.  Let ${\cal D}_n$ be the divisor associated with the
first $n$ basis elements of $M'$.  Clearly, any finite multiple of
${\cal D}_1$ can be constructed as integral element times $b_1$, so
let's now assume that any finite multiple of ${\cal D}_{n-1}$ can be
constructed with the first $n-1$ generators, and consider the $n^{\rm
th}$ generator $g_n$.  It lowers the order by one at a single place,
so any $f \in {\cal M}({\cal D}_n) - {\cal M}({\cal D}_{n-1})$ must
have exactly the same order as $g_n$.  Multiplying $g_n$ by a suitable
constant (the ratio of coefficients in $f$ and $g_n$'s series
expansion at this place) will exactly cancel this pole, so $f - cg \in
{\cal M}({\cal D}_n)$.

So any $f \in {\cal M}({\cal D})$ can be constructed using the
integral module $M'$.  Writing this construction in matrix form
shows how $f$ can be constructed as an $M$-module element:

$$\begin{pmatrix}a_1 & \cdots & a_m\end{pmatrix} \begin{pmatrix}b_1 \cr b_2' \cr b_2 \cr b_3' \cr b_3\end{pmatrix}
= \begin{pmatrix}a_1 & \cdots & a_m\end{pmatrix}\begin{pmatrix}1 & 0 & 0 \cr 0 & i_2' & 0 \cr 0 & 1 & 0 \cr 0 & 0 & i_3' \cr 0 & 0 &1\end{pmatrix}\begin{pmatrix}b_1 \cr b_2 \cr b_3\end{pmatrix}
= \begin{pmatrix}a_1' & \cdots & a_3'\end{pmatrix}\begin{pmatrix}b_1 \cr b_2 \cr b_3\end{pmatrix}
$$


  Consider such a finite multiple $f$.  For every
$m \in {\cal M}$ (in particular, its basis elements), $f$ must have
lower order than $m$ at at least one finite place $\mathfrak{p}$,
since otherwise $i = fm^{-1}$ would be integral and $f$ would exist in
${\cal M}$ as $mi$.  Yet ${\cal D}$, by definition, is the minimum of
the orders of ${\cal M}$'s basis elements at every finite place.
Therefore $f$ can not have lower order than a basis element at any
finite place and thus can not exist.

\endtheorem

Theorem \ref{divisor-module isomorphism} shows that an
${\cal I}$-module is associated with every divisor, but
now we need a constructive procedure for forming an ${\cal I}$-module
basis for a given divisor.

\theorem
\label{divisor basis construction}

Given a divisor ${\cal D}$, a pair of functions can always be
constructed that generate the divisor's associated integral module.

\proof

Use Theorem \ref{finite orders construction} to construct a function
$f$ with the divisor's required poles and zeros, zero order at all
other places conjugate to those poles and zeros, and non-negative
order elsewhere.  Construct $g$, a polynomial in $x$ with n-th order
roots at all points under n-th order zeros.  $(f,g)$ is the required
basis.  The only finite poles are those of $f$ and $g$ has zero order
everywhere except at $f$'s zeros and their conjugates, so by Theorem
\ref{divisor-module isomorphism}, $(f,g)$ forms a basis for
${\cal D}$'s associated ${\cal I}$-module.

\endtheorem

% \vfill\eject

Of course, the whole point here is to actually find a function with a
specified set of zeros and poles, so once we have constructed a basis
for a divisor's associated integral module, we need to determine if
the module is principal.  Since the total order of a field element is
always zero, this only makes sense for divisors of zero order, since
divisors of non-zero order can never be principal.  Futhermore, since
an integral module corresponds to {\it finite} multiples of a divisor,
we can't use this technique to find functions with poles or zeros at
infinity, but that isn't a serious limitation since if we need such a
function, we can just transform into a field with a different point at
infinity.

Since a finite multiple of a divisor differs from an exact multiple in
that the finite multiple can have additional finite zeros, and thus
additional infinite poles (since they always balance), a zero order
${\cal I}$-module is principal iff it contains a function with no
poles at infinity.  We can determine this by expressing the ${\cal
I}$-module as a ${\rm K}[x]$-module, simply by multiplying the ${\cal
I}$-module basis through by an integral basis (remember that an
integral basis is simply a basis for ${\cal I}$ as a ${\rm
K}[x]$-module).

We now transform this ${\rm K}[x]$-module basis to make it {\it normal
at infinity}, i.e, to ensure that poles don't cancel between terms.
First, we use a series of row-equivalent transformations to reduce our
$2n$ basis elements to $n$ elements, then additional transformations
to make it normal.

We then check these basis elements to see if one of them has no poles
at infinity.  The most straightforward way to do this is to invert the
field using $z={1\over x}$, which swaps zero with infinity.  We can
then construct an integral basis for the inverse field, and express
each of the module's basis elements (after inverting them) using this
inverse basis.  If there are any poles at infinity in the original
field, they will appear as poles at zero in the inverse field, and can
easily be detected by checking if $z=0$ is a zero of the denominators.

Finally, let me note that since $g$ in Theorem \ref{divisor basis
construction} is a polynomial, it always has poles at infinity (unless
the divisor has no zeros, and is thus trivially constant), and can
thus be excluded from consideration.  We need only look at the
function $f$, and perhaps not even all of its integral multiples
(CHECK THIS).

\theorem
\label{Trager's residue theorem}

Let $f \,dx$ be a differential with order greater than or equal to -1
at some place $\mathfrak{p}$ with branching index $r$ centered at
$x_0$.  The residue of $f \,dx$ at $\mathfrak{p}$ is equal to the
value of the function $r(x-x_0)f$ at $\mathfrak{p}$. ([Trager], p. 56,
taken almost verbatim)

\proof

Let $t$ be a uniformizing parameter at $\mathfrak{p}$.  Since
$x-x_0$ has order $r$ at $\mathfrak{p}$, it can be written as

$$x-x_0 = t^r g$$

where $g$ has order zero at $\mathfrak{p}$

$$dx = (rt^{r-1}g + t^r{{dg}\over{dt}})dt$$

Since ${dg}\over{dt}$ has non-negative order at $\mathfrak{p}$,
$dx$ has order $r-1$ at $\mathfrak{p}$ and $f$ must have order
greater than or equal to $-r$ at $\mathfrak{p}$

$$f\,dx = rt^{r-1}f g\, dt + t^rf({{dg}\over{dt}})dt$$

the second term on the right side is holomorphic at $\mathfrak{p}$ so
the residue of $f\,dx$ at $\mathfrak{p}$ is the same as the residue of
the first term on the right side.  Since this term is expressed using
the differential of a uniformizing paramter, its residue is the
residue of $rt^{r-1}fg$, which is the value of $rt^rfg = r(x-x_0)f$.

\endtheorem

\mysection{Examples}

\example Compute $\int \sqrt{4-x^2} \,dx$

A solution method from first year calculus might be to note that
this integrand forms one leg of a right triangle:

\begin{center}
\setlength{\unitlength}{1cm}
\begin{picture}(6,5)
\put(5,1){\line(0,1){3}}
\put(5,1){\line(-1,0){4}}
\put(1,1){\line(4,3){4}}
\put(2.5,0.5){$\sqrt{4-x^2}$}
\put(3,2.8){2}
\put(5.2,2.5){$x$}
\put(1.7,1.15){$\theta$}
\end{picture}
\end{center}

$$x=2\sin\theta \qquad \sqrt{4-x^2}=2\cos\theta \qquad dx=2\cos\theta\,d\theta$$


\begin{eqnarray*}
\int \sqrt{4-x^2} \, dx & = & \int 4 \cos^2\theta \, d\theta \\
& = & \int \left( 2 + 2\cos 2\theta \right) \, d\theta \\
& = & 2\theta + \sin 2\theta \\
& = & 2\theta + 2\sin\theta\cos\theta \\
& = & 2\arcsin\frac{x}{2} + \frac{x \sqrt{4-x^2}}{2} \\
\end{eqnarray*}

Now let's attack this integral using the methods of this chapter.
First, transform the problem into an algebraic curve:

$$\int y\,dx \qquad y^2 = 4-x^2$$

Since $\lim_{x\to\infty} y = \infty$, the integrand has poles at
infinity.  We want infinity to be an ordinary point of the curve (no
ramification; no singularities) with no poles in the integrand.  The
simplest transformation is to exchange zero with infinity, and in this
case zero is an ordinary point with places $(0,2)$ and $(0,-2)$,
neither of which is a pole of the integrand.  So we'll invert
$x$ and $y$ into $u$ and $v$:

$$x=\frac{1}{u} \qquad y=\frac{1}{v}$$
$$\left(\frac{1}{v}\right)^2 = 4 - \left(\frac{1}{u}\right)^2 \Longrightarrow 4u^2v^2 - v^2 - u^2=0$$
$$\int\frac{1}{v} \, d\left(\frac{1}{u}\right) \Longrightarrow -\int\frac{1}{vu^2}\,du$$

The only poles in this integrand occur when either $u=0$ or $v=0$.
Substituting these values into $4u^2v^2 - v^2 -u^2=0$, we see that
these condiutions only occur at $(u,v)=(0,0)$, so let's analyze our
curve at that point, starting with the Newton polygon:

\begin{center}
$4 u^2 v^2 - v^2 - u^2 = 0$ \\
\setlength{\unitlength}{1cm}
\begin{picture}(3,3)
\put(0,0){\line(0,1){2.5}}
\put(0,0){\line(1,0){3}}
\put(1.9,-0.1){x}
\put(1.9,1.9){x}
\put(-0.1,1.9){x}
\thicklines
\put(0,2){\line(1,-1){2}}
\end{picture}
\end{center}

The Newton polygon has a single line segment of span 2 and slope -1, so
we have two cycles, each with ramification index one: a singularity.
Since there is no ramification, $u$ is a uniformizing parameter
and we expect to expand $v$ as follows:

$$v = c_1 u + c_2 u^2 + c_3 u^3 + \cdots$$
$$v^2 = c_1^2 u^2 + 2 c_1 c_2 u^3 + (2 c_1 c_3 + c_2^2) u^4 + \cdots$$

Substituting these expansions into $4u^2v^2 - v^2 - u^2 = 0$, we obtain:

$$ 4 c_1^2 u^4 + 8 c_1 c_2 u^5 + (8 c_1 c_3 + 4 c_2^2) u^6 + \cdots $$
$$ - c_1^2 u^2 - 2 c_1 c_2 u^3 - (2 c_1 c_3 + c_2^2) u^4 + \cdots - u^2 = 0$$

Equating terms in $u^2$, we see that $c_1 = \pm i$.  Each of these
two values corresponds to one branch of the singularity.  There
is only a single term in $u^3$, which forces $c_2$ to be zero,
and equating terms in $u^4$ produces $c_3 = 2 c_1$, so

$$v = \pm (iu + 2iu^3 + \cdots) \qquad @(0,0)$$

Inverting $v$ and substituting into our 1-form, we obtain

$$\frac{1}{v} = \pm (-i \frac{1}{u} + 2i u + \cdots) \qquad @(0,0)$$

$$\frac{1}{vu^2}\, du = \pm \left[ -i \frac{1}{u^3} + 2i \frac{1}{u} + \cdots \right] \, du \qquad @(0,0)$$

The $u^{-1}$ terms will integrate into logarithms, so let's ignore
them for the moment and concentrate on the $u^{-3}$ terms, which will
integrate into $u^{-2}$ terms, so we're looking for a function with
second order poles at both places at the $(0,0)$ singularity.

Starting with our standard basis for all rational functions,
$\{1,\,v\}$, we seek to modify it into a basis for
${\rm P}^2(0,0)_a{\rm P}^2(0,0)_b$.  Note first that $v$ has
poles at $u=\pm\frac{1}{2}$.  Using $y=1/u$, we analyze
at $(\pm\frac{1}{2}, \infty)$ as follows:

\begin{center}
$y^2\left[(u-\frac12)^2+(u-\frac12)+\frac14\right]-4(u-\frac12)^2-4(u-\frac12)$
\\
\setlength{\unitlength}{1cm}
\begin{picture}(3,3)
\put(0,0){\line(0,1){2.5}}
\put(0,0){\line(1,0){3}}
\put(0.9,-0.1){x}
\put(1.9,-0.1){x}
\put(-0.1,1.9){x}
\put(0.9,1.9){x}
\put(1.9,1.9){x}
\thicklines
\put(0,2){\line(1,-2){1}}
\end{picture}
\end{center}

Our line segment has span 1 and slope -2, indicating a single place
with ramification 2, and $y$ as a uniformizing parameter.  Setting

$$(u-\frac12) = c_1 y + c_2 y^2 + \cdots$$
$$(u-\frac12)^2 = c_1^2 y^2 + \cdots$$

Substituting, we find that $c_1 = 0$ and $c_2 = \frac{1}{16}$, so

$$(u-\frac12) = \frac{1}{16} y^2 + \cdots \qquad v=y^{-1} \qquad @(\frac12, \infty)$$

$$(u+\frac12) = \frac{1}{16} y^2 + \cdots \qquad v=y^{-1} \qquad @(-\frac12, \infty)$$

In short, $v$ has first order poles at $(\pm\frac12,\infty)$ and
$(u\pm\frac12)$ has second order zeros, so we can adjust our basis
accordingly and obtain $\{1,\,(4u^2-1)v\}$ for a basis with no finite
poles.  We can also use a theorem of Trager to shortcut this calculation.

Returning to our analysis at $(0,0)$, we see that 1 has zero order
(obviously) and $(4u^2-1)v$ has a first order zero at both sheets
there, since $4u^2-1=-1$ is finite and $v$ has first order zeros.
We also know that $u$ is a uniformizing parameter, so it's easy
to modify our basis and obtain

$$\left\{\frac{1}{u^2},\,\frac{4u^2-1}{u^3}v\right\} {\rm is\, a\,} {\bf C}[x]{\rm -basis\, for\, P^2(0,0)_aP^2(0,0)_b}$$

Is this basis normal at infinity?  Well, the representation order of
$\frac{1}{u^2}$ is 2 and its $u^-2$ coefficients at $(\infty, \pm
\frac12)$ are both 1, while the representation order of $\frac{4u^2-1}{u^3}v$
is 1, and its $u^-1$ coefficients are 2 and -2.  Since

$$\det C = \begin{array}{|cc|} 1 & 2 \\ 1 & -2 \end{array} = -4$$

is non-zero, the basis is normal at infinity.

The Riemann-Roch theorem says that the dimension of ${\mathfrak l}(D)$ is 5,
$\frac{1}{u^2}$ can be multiplied by any polynomial up to second
degree without introducing poles at infinity, and $\frac{4u^2-1}{u^3}v$
can be multiplied by any polynomial up to first degree, so

$$\left\{\frac{1}{u^2},\, \frac{1}{u},\, 1,\, \frac{4u^2-1}{u^3}v,\, \frac{4u^2-1}{u^2}v\right\}$$

is a ${\cal C}$-module basis for ${\mathfrak l}(D)$.

Any linear combination of these functions is a multiple of the
divisor, but not all of them produce the correct residues.  Looking at
the residues, we see that only $\frac{4u^2-1}{u^3}v = \frac{1}{uv}$
has residues of $\pm i$ on the two sheets at the $(0,0)$ singularity.
Dividing by 2 to correct for the 2 that will be introduced by the
integration, we conclude that $\frac{1}{2uv} = \frac{xy}{2} =
\frac{x\sqrt{4-x^2}}{2}$ is the desired function.

Next, we have to deal with the logarithms.  Going back to the
series expansions of our 1-form, we see that we have residues
of $\pm 2i$ on our two sheets at $(0,0)$.  The objective
now is a bit different; we want a function with exactly
the divisor $Z(0,0)_a P(0,0)_b$.  Starting with an integral basis:

$$\{1, (4u^2-1)v\}$$

we want to modify these functions to make them multiples
of $Z(0,0)_a P(0,0)_b$.  The pole isn't a problem for
an integral basis, and looking at the series expansion
for $v$ at $(0,0)$ we see that it (and therefore $(4u^2-1)v$)
has a simple zero there, but $1$ needs to be replaced with $u$:

$$\{u, (4u^2-1)v\}$$

Now we construct a matrix with the coefficients in the series expansions:

$$\left[ \begin{array}{cc} 1 & -i \\ 0 & 0 \end{array} \right] \begin{array}{ll} \leftarrow (0,0)_a \\ \leftarrow (0,0)_b \end{array} $$

$$\left[ \begin{array}{cc} 1 & -i \\ 0 & 0 \end{array} \right] \left[ \begin{array}{c} i \\ 1 \end{array} \right] = 0$$

The solution shows us how to modify the basis:

$$\{u, \frac{iu + (4u^2-1)v}{u}\} = \{u, i + \frac{(4u^2-1)v}{u}\}$$

$$\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right] \begin{array}{ll} \leftarrow (0,0)_a \\ \leftarrow (0,0)_b \end{array} $$

$$\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right] \left[ \begin{array}{c} 0 \\ 1 \end{array} \right] = 0$$

$$\{u, i\frac{1}{u} + \frac{(4u^2-1)v}{u^2}\}$$

$$\left| \begin{array}{cc} 1 & -2i \\ 0 & 2i \end{array} \right| = 2i$$

At the last step, the determinant is non-zero, which shows that we
now have a basis for multiples of the divisor except at infinity.
Is it normal at infinity?  $u$'s expansion at both places at infinity
is $\left(\frac{1}{u}\right)^{-1}$, so its representation order is -1,
and the second element's expansion at infinity starts $\pm 2 + \cdots$,
so its representation order is 0 and:

$$\det C = \begin{array}{|cc|} 1 & 2 \\ 1 & -2 \end{array} = -4$$

So the basis is normal at infinity.  If an exact multiple of
the divisor exists, it is one of the basis elements.  It's not $u$,
since $u$ has a pole at infinity, but the second element is exact:

$$i\frac{1}{u} + \frac{(4u^2-1)v}{u^2} = i\frac{1}{u} - \frac{1}{v} = ix-y$$

The desired residues are $\pm 2i$, so the function we want is

$$2i \ln(ix-y) = 2i \ln(\frac{y}{2}-i\frac{x}{2}) + 2i \ln(-2) $$
$$= 2i \ln\left(\sqrt{1-\left(\frac{x}{2}\right)^2} - i\frac{x}{2}\right) = 2i (-i \arcsin \frac{x}{2}) = 2 \arcsin \frac{x}{2}$$

(the constant disappears into the constant of integration) and the final answer is:

$$ \int \sqrt{4-x^2} \, dx  = 2\arcsin\frac{x}{2} + \frac{x \sqrt{4-x^2}}{2}$$

\endexample


\vfill\eject
\mysection{arcsin}

\example Compute $\int {1\over{\sqrt{1-x^2}}} \,dx$

The obvious attempt is to use the algebraic extension $y^2=1-x^2$ and
integrate ${1\over y}\,dx$.

But we first need to determine if this differential has any poles at
infinity, by inverting the field and looking for poles at zero.
Setting $u={1\over x}$, we convert our minimal polynomial into
$u^2y^2=u^2-1$ (after multiplying through by $u^2$), and using
$v=uy$ we obtain our inverse field ${\bf C}(u,v); v^2=u^2-1$.

Since $x={1\over u}$ and $y={v\over u}$, we convert our differential as follows:

 $${1\over y}\,dx ={u\over v} (-{1\over{u^2}} \, du) = -{1\over{uv}} \, du$$

Now, $\{1, v\}$ is an integral basis for the inverse field, so we
multiply through by $v\over v$ to obtain:

 $$= -{v\over{uv^2}} du = -{1\over{u(u^2-1)}}v \, du $$

which is now in normal form and clearly has a pole at $u=0$, or $x=\infty$.  Note that

 $${1\over y} = {u\over v} = {{uv}\over{v^2}}
 = {u\over{u^2-1}} v$$

has no pole at $u=0$, a clear example of a differential having a pole
at a place where its constituent function has none.

In any event, we clearly can not use the original field to conduct the
integration, since it would require constructing a function with a
pole at infinity, and our algorithm can't handle this.  So we need to
transform into a field where the differential has no pole at infinity.

Actually, we've already done this!  Note that the integrand had no pole
at zero in the original field:

 $${1\over y}\,dx = {y\over y^2}\,dx = {1\over{1-x^2}}y \,dx $$

Since the inverse field swapped zero with infinity, it follows that
there is no pole at infinity in the inverse field, so we can proceed
to integrate $-{1\over{u(u^2-1)}}v \,du$ in ${\bf C}(u,v)$;
$v^2=u^2-1$.

Simple inspection of the integrand (already in normal form) shows that
its poles are at $(0, i)$, $(0, -i)$, $(1, 0)$, and $(-1, 0)$.
Remember that we're now working on the Riemann surface of an algebraic
extension, so we need to specify $\it both$ $u$ and $v$ to
specify a place.

The next step is to compute the residues at each of these places,
using Theorem \ref{Trager's residue theorem}:

\begin{center}
\begin{supertabular}{l l l}
  $(0, i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, i)$     & = $i$    \cr
  $(0, -i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, -i)$   & = $-i$    \cr
  $(1, 0)$  &  $\displaystyle -2{1\over{u(u+1)}}v$ @ $(1, 0)$      & = $0$    \cr
  $(-1, 0)$  &  $\displaystyle -2{1\over{u(u-1)}}v$ @ $(-1, 0)$    & = $0$    \cr
\end{supertabular}
\end{center}

The poles with zero residues can be ignored.  We're interested in the
other two, which exist in ${\bf Q}[i]$, which can be regarded as a
vector field over ${\bf Q}$ with basis $\{1, i\}$, and we want to
construct a function whose poles and zeros match the $i$-component of
the residues (the 1-component is uniformly zero).

We start by constructing an ${\cal I}$-module generator set for the divisor
with a simple zero at $(0,i)$ and a simple pole at $(0,-i)$.  Theorem
\ref{simple pole construction} shows that:

$$f = {{v^2+1}\over{u(v+i)}} = {{v-i}\over{u}} $$

has a simple pole at $(0,-i)$.  At $(0,i)$, L'H\^opital's rule gives:

$$ \lim_{(u,v)\to (0,i)} {{v-i}\over{u}}
   = {{(v-i)'}\over{u'}} {{dv}\over{du}} = {{dv}\over{du}} = {u\over v} = 0 $$

where the last transformation was accomplished by differentiating the
mimimal polynomial.  So $f$ has a zero at $(0,i)$, and I'll note that
we've just stumbled into the solution.  Theorem \ref{simple pole
construction} already assures us that $f$ has only a single finite
simple pole, and we can see that its only zeros occur when
$v-i=0$, which, according to the minimum polynomial, can only
occur at $u=0$, thus $(0,i)$ is its only finite zero, and it is
simple, as we can verify by showing that the corresponding pole in its
inverse is simple:

$$ {1\over f} = {u\over{v-i}} = {{u(v+i)}\over{v^2+1}}
  = {{u(v+i)}\over{u^2}} = {1\over u}v + {i\over u} $$


So we've found the function we're looking for by accident.  Let's save the
general case for the next example, and convert back to
our original field:

$${{v-i}\over{u}} = x({y\over x}-i) = y - ix $$

Remembering that our residues came multiplied by a factor of $i$, we
conclude that our solution is $i\,\ln(y-ix)$, or:

\begin{eqnarray*}
\int {1\over{\sqrt{1-x^2}}} \,dx &=& i\,\ln(\sqrt{1-x^2}-ix) \\
                                 &=& -i\,\ln({1\over{\sqrt{1-x^2}-ix}}) \\
                                 &=& -i\,\ln({{\sqrt{1-x^2}+ix}\over{1-x^2+x^2}}) \\
                                 &=& -i\,\ln({\sqrt{1-x^2}+ix}) \\
                                 &=& \arcsin x \\
\end{eqnarray*}

where I used the negative of a logarithm being the logarithm of the
inverse, and the last transformation came from section
\ref{sec:Liouvillian Forms}.


\endexample

\vfill\eject
\mysection{Se\~nor Gonzalez, otra vez}

The Rothstein-Trager resultant allows us to compute all the residues
at once.  Trager, in his Ph.D. thesis, then showed how to construct a
function that is zero at all poles with a given residue, and non-zero
at all other poles, as well as at all places conjugate to a pole.
