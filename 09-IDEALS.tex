
\setcounter{chapter}{8}
\mychapter{Simple Algebraic Extensions}

\input{09a-ALGEBRAIC-THEORY.tex}

\vfill\eject
\mysection{Divisors and Integral Modules}

%\section{Integral Modules}

In ${\bf C}(x)$, we were working with the quotient field of a
principal ideal ring, so we could always find a single function to
generate any ideal / ${\bf C}[x]$-module.

In ${\bf C}(x,y)$, we are no longer working with a principal ideal
ring, so we can't guarantee that any particular ideal can be generated
by a single function, but it turns out that every ideal can be
generated by a {\it pair} of functions.  Our course of attack is first
to construct that pair of functions, then use them to determine if in
fact the ideal is principal.

\definition

An {\it integral module} (or ${\cal I}$-module) is a module formed
over ${\cal I}$, the ring of integral elements in ${\bf C}(x,y)$.

\enddefinition

Since ${\cal I}$ itself can be expressed as a ${\bf C}[x]$-module
using an integral basis, any ${\cal I}$-module is also a ${\bf
C}[x]$-module.  Not all ${\bf C}[x]$-modules are ${\cal I}$-modules,
however, since ${\cal I}$ is typically larger than ${\bf C}[x]$.

Some authors use the term {\it fractional ideal} to refer to an ${\cal
I}$-module.  I have avoided use of this term for two reasons.  First,
I wish to emphasize the concept of a module.  Second, ${\cal
I}$-modules are not ideals, either in the ring ${\cal I}$ (since they
may contain elements not in ${\cal I}$), nor in the field ${\bf
C}(x,y)$, since, as a field, ${\bf C}(x,y)$ has only the trivial
ideals.  The term {\it fractional ideal} is used because an ${\cal
I}$-module can be regarded as a fraction of ideals in ${\cal I}$.

\theorem ${\cal I}$ is a Noetherian ring.
\label{I is Noetherian}

\proof

Since ${\bf C}$ is a field, ${\bf C}[x]$ is a Noetherian ring by the
Hilbert basis theorem, ${\bf C}[x] \subseteq {\cal I}$, and ${\cal I}$ is
finitely generated as a ${\bf C}[x]$-module, so ${\cal I}$ is a
Noetherian ring by [Atiyah+McDonald] Proposition 7.2.

\endtheorem

\theorem
\label{simple zero construction}

A function can always be constructed with a simple zero at a specified
finite, non-singular place $(\alpha, \beta)$, zero order at an
additional finite set of finite, non-singular places $\Sigma$, and
non-negative order at all other finite places.

\proof

Begin with the function $(x-\alpha)$, which is a uniformizing variable
and thus has a simple zero at $(\alpha, \beta)$.  If none of the other
places in $\Sigma$ have x-value $\alpha$, then we are done, since
$(x-\alpha)$ has no finite poles.

Otherwise, compute $(x-\alpha)\over(y-\beta)$ at all places in
$\Sigma$ that do {\it not} have $y = \beta$.  Select a number
$\gamma$ different from all of these values.  The function $(x-\alpha)
- \gamma (y-\beta)$ has no finite poles and
is non-zero at all places in $\Sigma$, but it may now have a zero of
higher order at $(\alpha, \beta)$.  Consider a series expansion
of $y$ in terms of $(x-\alpha)$:

$$y = \beta + c_1 (x-\alpha) + c_2 (x-\alpha)^2 + \cdots$$

So long as $\gamma$ is also selected different from $c_1$, $(x-\alpha)
- \gamma (y-\beta)$ will have a first order zero at $(\alpha, \beta)$
and meet all requirements of the theorem.

% SINGULARITIES?

\endtheorem

\theorem
\label{simple pole construction}

A function can always be constructed with a simple pole at a specified
finite, non-singular place $(\alpha, \beta)$, zero order at an
additional finite set of finite, non-singular places $\Sigma$, and
non-negative order at all other places.

\proof

Begin with the function:

$$f(\alpha,y)\over(x-\alpha)(y-\beta)$$

where $f(x,y)$ is the minimum polynomial of the algebraic extension.
Note that the division by $(y-\beta)$ will always be exact, since
$f(\alpha, \beta)=0$.  So we have a rational function
$P(y)\over(x-\alpha)$, where $P(y)$ is a polynomial in $y$.  It has a
simple pole at $(\alpha, \beta)$, as can be seen from a series
expansion in $x-\alpha$ (again, a uniformizing variable).  Since the
$y-\beta$ factor has been divided out of $f(\alpha,y)$, the numerator
is non-zero at $(\alpha, \beta)$, so the leading term in the series
expansion involves $(x-\alpha)^{-1}$, and the pole is thus simple.

This function is finite at all other places, which is obvious except
when $x=\alpha$ and $y\ne\beta$, where it takes the form $0\over0$,
so we can expand it using L'H\^opital's rule:

$$\lim_{(x,y)\to p} {{P(y)}\over(x-\alpha)}
  = \lim_{{(x,y)\to p}} {{{dP(y)}\over{dx}}\over{{d(x-\alpha)}\over{dx}}}
  = P'(y) \, {{dy}\over{dx}} $$

where $'$ denotes differentiation with respect to the polynomial's
variable.  $P'(y)$ is a polynomial, and is thus finite where $y$ is
finite, as is ${dy}\over{dx}$ (consider a series expansion of $y$ in
terms of $(x-\alpha)$, since all places in $\Sigma$ are finite and
non-singular).  It follows that the function is at least finite
everywhere except at $(\alpha, \beta)$.

Now, compute the value of the function at all other places in
$\Sigma$, using either L'H\^opital's rule or Puiseux expansion if some
of these are over $\alpha$.  If the value of the function is non-zero
at all of these places, then we are done.  Otherwise, select a number
$\gamma$ different from all of these values.  The function:

$${f(\alpha,y)\over(x-\alpha)(y-\beta)} - \gamma$$

has the desired properties, since it still has a simple pole at
$(\alpha,\beta)$, has no other poles, and is now non-zero at all
places in $\Sigma$.

% SINGULARITIES?

\endtheorem

\theorem
\label{finite orders construction}

A function can always be constructed with specified integer orders at
a finite set of finite, non-singular places $\Sigma$ and non-negative
order at all other finite places.

\proof

For each pole or zero, use Theorems \ref{simple zero construction} or
\ref{simple pole construction} to construct a function with a simple
pole or a simple zero at that place, zero order at all other places in
$\Sigma$ and non-negative order at all other finite places.  Raise
each of these function to the integer power that is the order of the
corresponding pole or zero, then multiply them all together.

\endtheorem

\definition

A {\it finite multiple} of a divisor is a function with order equal to
or greater than that required by the divisor at all {\it finite} places.

\enddefinition

For the remainder of this section, I'll assume that our divisors
involve only finite, non-singular places, which can always be
guaranteed in the case of the integration theory.

\theorem
\label{exact order existance}

For any divisior ${\cal D}$ and any finite, non-singular place
$\mathfrak{p}$, at least one finite multiple of ${\cal D}$ exists with
order at $\mathfrak{p}$ exactly that required by ${\cal D}$.

\proof

Use Theorem \ref{finite orders construction} with the zeros and poles
required by ${\cal D}$, adding $\mathfrak{p}$
to $\Sigma$ if necessary.

\endtheorem

\theorem
\label{divisor-module isomorphism}

There is a one-to-one relationship between finitely generated integral
modules and divisors.  Such a module consists of all finite multiples
of its associated divisor, and the order of a module's divisor at
every finite place is the minimum of the orders of the module's
generators at that place.

\proof

For a given divisor ${\cal D}$, consider the set ${\cal M}({\cal D})$
of all finite multiples of ${\cal D}$.
Now, adding two
elements can not reduce their order at any finite place, nor can
multiplying an element by an integral element $i \in {\cal I}$, so
${\cal M}({\cal D})$ is clearly an ${\cal I}$-module, but it
might not be finitely generated.

Since ${\cal D}$ has only a finite number of poles, we can always
construct a function with order equal or less than that of ${\cal D}$
at all finite places simply by taking the inverse of the polynomial
$r=(x-p_1)^{m_1} \cdots (x-p_n)^{m_n}$ where $p_i$ are the x-coordinates
of the poles in ${\cal D}$ and $m_i$ are their multiplicities.
For any $m \in {\cal M}({\cal D})$, $mr$ is integral, so ${\cal M}({\cal D})
\subseteq {\cal I}\{r^{-1}\}$, where ${\cal I}\{r^{-1}\}$ is the ${\cal I}$-module
generated by $r^{-1}$.  Now, since ${\cal I}\{r^{-1}\}$ is a finitely
generated module over a Noetherian ring (remember Theorem \ref{I is
Noetherian}), ${\cal I}\{r^{-1}\}$ is a Noetherian module by
[Atiyah+McDonald] Proposition 6.5, and ${\cal M}({\cal D})$ must also
be a finitely generated ${\cal I}$-module by [Atiyah+McDonald]
Proposition 6.2.

Let $(b_1,...,b_n)$ be an ${\cal I}$-module basis for ${\cal M}({\cal
D})$.  Since there is no way to lower the orders of an element using
${\cal I}$-module constructions, and by Theorem \ref{exact order
existance} for each place there is at least one function in ${\cal
M}({\cal D})$ with order exactly that required by ${\cal D}$, it
follows that for each place there must be at least one basis element
with exactly the order required by ${\cal D}$.  Futhermore, no basis
element can have an order less than required by ${\cal D}$ at any
finite place, since that element would not be a finite multiple of
${\cal D}$.  Therefore, at each place $\mathfrak{p}$, the minimum of
the orders of the basis elements must be exactly the order required by
${\cal D}$.

Conversely, given a finitely generated ${\cal I}$-module ${\cal M}$,
construct its associated divisor ${\cal D}$ by taking at every place
the minimum of the orders of the module's generators at that place.
Since all elements of the module have the form:

	$$\sum a_i g_i, \qquad a_i \in {\cal I}, g_i \in {\bf C}(x,y)$$

everything in the module must be a finite multiple of ${\cal D}$.

It is possible, though, that some finite multiple of ${\cal D}$ might
not be in ${\cal M}$.  Consider such a finite multiple $f$.  For every
$m \in {\cal M}$ (in particular, its basis elements), $f$ must have
lower order than $m$ at at least one finite place $\mathfrak{p}$,
since otherwise $i = fm^{-1}$ would be integral and $f$ would exist in
${\cal M}$ as $mi$.  Yet ${\cal D}$, by definition, is the minimum of
the orders of ${\cal M}$'s basis elements at every finite place.
Therefore $f$ can not have lower order than a basis element at any
finite place and thus can not exist.

\endtheorem

Theorem \ref{divisor-module isomorphism} shows that an
${\cal I}$-module is associated with every divisor, but
now we need a constructive procedure for forming an ${\cal I}$-module
basis for a given divisor.

\theorem
\label{divisor basis construction}

Given a divisor ${\cal D}$, a pair of functions can always be
constructed that generate the divisor's associated integral module.

\proof

Use Theorem \ref{finite orders construction} to construct a function
$f$ with the divisor's required poles and zeros, zero order at all
other places conjugate to those poles and zeros, and non-negative
order elsewhere.  Construct $g$, a polynomial in $x$ with n-th order
roots at all points under n-th order zeros.  $(f,g)$ is the required
basis.  The only finite poles are those of $f$ and $g$ has zero order
everywhere except at $f$'s zeros and their conjugates, so by Theorem
\ref{divisor-module isomorphism}, $(f,g)$ forms a basis for
${\cal D}$'s associated ${\cal I}$-module.

\endtheorem

Of course, the whole point here is to actually find a function with a
specified set of zeros and poles, so once we have constructed a basis
for a divisor's associated integral module, we need to determine if
the module is principal.  Since the total order of a field element is
always zero, this only makes sense for divisors of zero order, since
divisors of non-zero order can never be principal.  Futhermore, since
an integral module corresponds to {\it finite} multiples of a divisor,
we can't use this technique to find functions with poles or zeros at
infinity, but that isn't a serious limitation since if we need such a
function, we can just transform into a field with a different point at
infinity.

Since a finite multiple of a divisor differs from an exact multiple in
that the finite multiple can have additional finite zeros, and thus
additional infinite poles (since they always balance), a zero order
${\cal I}$-module is principle iff it contains a function with no
poles at infinity.  We can determine this by expressing the ${\cal
I}$-module as a ${\bf C}[x]$-module, simply by multiplying the ${\cal
I}$-module basis through by an integral basis (remember that an
integral basis is simply a basis for ${\cal I}$ as a ${\bf
C}[x]$-module).

We then check these basis elements to see if one of them has no poles
at infinity.  The most straightforward way to do this is to invert the
field using $z={1\over x}$, which swaps zero with infinity.  We can
then construct an integral basis for the inverse field, and express
each of the module's basis elements (after inverting them) using this
inverse basis.  If there are any poles at infinity in the original
field, they will appear as poles at zero in the inverse field, and can
easily be detected by checking if $z=0$ is a zero of the denominators.

This requires the basis to be {\it normal at infinity}, to ensure that
the poles don't cancel, or equivalently the inverse basis has to be
normal at 0.  Integral bases are normal everywhere except at infinity.

Finally, let me note that since $g$ in Theorem \ref{divisor basis
construction} is a polynomial, it always has poles at infinity (unless
the divisor has no zeros, and is thus trivially constant), and can
thus be excluded from consideration.  We need only look at the
function $f$, and perhaps not even all of its integral multiples
(CHECK THIS).

\example Compute $\int {1\over{\sqrt{1-x^2}}} \,dx$

The obvious attempt is to use the algebraic extension $y^2=1-x^2$ and
integrate ${1\over y}\,dx$.

But we first need to determine if this differential has any poles at
infinity, by inverting the field and looking for poles at zero.
Setting $u={1\over x}$, we convert our minimal polynomial into
$u^2y^2=u^2-1$ (after multiplying through by $u^2$), and using
$v=uy$ we obtain our inverse field ${\bf C}(u,v); v^2=u^2-1$.

Since $x={1\over u}$ and $y={v\over u}$, we convert our differential as follows:

 $${1\over y}\,dx ={u\over v} (-{1\over{u^2}} \, du) = -{1\over{uv}} \, du$$

Now, $\{1, v\}$ is an integral basis for the inverse field, so we
multiply through by $v\over v$ to obtain:

 $$= -{v\over{uv^2}} du = -{1\over{u(u^2-1)}}v \, du $$

which is now in normal form and clearly has a pole at $u=0$, or $x=\infty$.  Note that

 $${1\over y} = {u\over v} = {{uv}\over{v^2}}
 = {u\over{u^2-1}} v$$

has no pole at $u=0$, a clear example of a differential having a pole
at a place where its constituent function has none.

In any event, we clearly can not use the original field to conduct the
integration, since it would require constructing a function with a
pole at infinity, and our algorithm can't handle this.  So we need to
transform into a field where the differential has no pole at infinity.

Actually, we've already done this!  Note that the integrand had no pole
at zero in the original field:

 $${1\over y}\,dx = {y\over y^2}\,dx = {1\over{1-x^2}}y \,dx $$

Since the inverse field swapped zero with infinity, it follows that
there is no pole at infinity in the inverse field, so we can proceed
to integrate $-{1\over{u(u^2-1)}}v \,du$ in ${\bf C}(u,v)$;
$v^2=u^2-1$.

Simple inspection of the integrand (already in normal form) shows that
its poles are at $(0, i)$, $(0, -i)$, $(1, 0)$, and $(-1, 0)$.
Remember that we're now working on the Riemann surface of an algebraic
extension, so we need to specify $\it both$ $u$ and $v$ to
specify a place.

The next step is to compute the residues at each of these places,
using the Theorem on p. 56 of Trager's thesis:

\begin{center}
\begin{supertabular}{l l l}
  $(0, i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, i)$     & = $i$    \cr
  $(0, -i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, -i)$   & = $-i$    \cr
  $(1, 0)$  &  $\displaystyle -2{1\over{u(u+1)}}v$ @ $(1, 0)$      & = $0$    \cr
  $(-1, 0)$  &  $\displaystyle -2{1\over{u(u-1)}}v$ @ $(-1, 0)$    & = $0$    \cr
\end{supertabular}
\end{center}

The poles with zero residues can be ignored.  We're interested in the
other two, which exist in ${\bf Q}[i]$, which can be regarded as a
vector field over ${\bf Q}$ with basis $\{1, i\}$, and we want to
construct a function whose poles and zeros match the $i$-component of
the residues (the 1-component is uniformly zero).

We start by constructing an ${\cal I}$-module basis for the divisor
with a simple zero at $(0,i)$ and a simple pole at $(0,-i)$.  Theorem
\ref{simple pole construction} shows that:

$$f = {{v^2+1}\over{u(v+i)}} = {{v-i}\over{u}} $$

has a simple pole at $(0,-i)$.  At $(0,i)$, L'H\^opital's rule gives:

$$ \lim_{(u,v)\to (0,i)} {{v-i}\over{u}}
   = {{(v-i)'}\over{u'}} {{dv}\over{du}} = {{dv}\over{du}} = {u\over v} = 0 $$

where the last transformation was accomplished by differentiating the
mimimal polynomial.  So $f$ has a zero at $(0,i)$, and I'll note that
we've just stumbled into the solution.  Theorem \ref{simple pole
construction} already assures us that $f$ has only a single finite
simple pole, and we can see that its only zeros occur when
$v-i=0$, which, according to the minimum polynomial, can only
occur at $u=0$, thus $(0,i)$ is its only finite zero, and it is
simple, as we can verify by showing that the corresponding pole in its
inverse is simple:

$$ {1\over f} = {u\over{v-i}} = {{u(v+i)}\over{v^2+1}}
  = {{u(v+i)}\over{u^2}} = {1\over u}v + {i\over u} $$


So we've found the function we're looking for by accident.  Let's save the
general case for the next example, and convert back to
our original field:

$${{v-i}\over{u}} = x({y\over x}-i) = y - ix $$

Remembering that our residues came multiplied by a factor of $i$, we
conclude that our solution is $i\,\ln(y-ix)$, or:

\begin{eqnarray*}
\int {1\over{\sqrt{1-x^2}}} \,dx &=& i\,\ln(\sqrt{1-x^2}-ix) \\
                                 &=& -i\,\ln({1\over{\sqrt{1-x^2}-ix}}) \\
                                 &=& -i\,\ln({{\sqrt{1-x^2}+ix}\over{1-x^2+x^2}}) \\
                                 &=& -i\,\ln({\sqrt{1-x^2}+ix}) \\
                                 &=& \arcsin x \\
\end{eqnarray*}

where I used the negative of a logarithm being the logarithm of
the inverse, and the last transformation came from section ?.


\endexample


\section{Se\~nor Gonzalez, otra vez}

The Rothstein-Trager resultant allows us to compute all the residues
at once.  Trager, in his Ph.D. thesis, then showed how to construct a
function that is zero at all poles with a given residue, and non-zero
at all other poles, as well as at all places conjugate to a pole.
