
\setcounter{chapter}{8}
\mychapter{Simple Algebraic Extensions}

We now turn to the algebraic extension.  Theorem ? allows us to
collapse any two adjacent algebraic extensions together, so we need
only consider an algebraic extension over a transcendental extension.
The most basic case, the one that we will study in this chapter, is
when the integrand involves only polynomials and a single root, so we
are integrating on an algebraic curve and our algebraic extension
occurs directly over the variable of integration: ${\bf C}(x,y)$.
However, many of this chapter's basic results are applicable in the
more general case where we have a series of field extensions that end
in a transcendental (exponential or logarithmic) extension followed by
an algebraic extension.  I'll use the notation ${\rm K}(\theta, y)$ to
emphasize when this is the case.

\input{09a-ALGEBRAIC-THEORY.tex}
\input{09b-RATIONAL-FUNCTION-BASIS.tex}

% \vfill\eject
\mysection{Divisors and Integral Modules}

In ${\bf C}(x)$, we were working with the quotient field of a
principal ideal ring, so we could always find a single function to
generate any finitely generated ${\bf C}[x]$-module, simply by putting
all the generators over a common denominator, then taking the
G.C.D. of the numerators.

In ${\rm K}(\theta,y)$, we are no longer working with a principal ideal
ring, so we can't guarantee that any particular ideal can be generated
by a single function, but it turns out that every ideal can be
generated by a {\it pair} of functions.  Our course of attack is first
to construct that pair of functions, then use them to determine if in
fact the ideal is principal.

\definition

An {\bf integral module} (or ${\cal I}$-module) is a module formed
over ${\cal I}$, the ring of integral elements in ${\rm K}(\theta,y)$.

\enddefinition

Since ${\cal I}$ itself can be expressed as a ${\rm K}[\theta]$-module
using an integral basis, any ${\cal I}$-module is also a ${\rm
K}[\theta]$-module.  Not all ${\rm K}[\theta]$-modules are ${\cal
I}$-modules, however, since ${\cal I}$ is typically larger than ${\rm
K}[x]$.

% Existance of an integral basis is demonstrated by Atiyah and MacDonald
% via Propositions 5.17, 6.5, and 6.2.  Van der Waerden proves this on
% pp. 174-175 of volume 2.  That ${\cal I}$ is a free module can be
% proven using Lang's theorem 7.3.

Some authors use the term {\it fractional ideal} to refer to an ${\cal
I}$-module.  I have avoided use of this term for two reasons.  First,
I wish to emphasize the concept of a module.  Second, ${\cal
I}$-modules are not ideals, either in the ring ${\cal I}$ (since they
may contain elements not in ${\cal I}$), nor in the field ${\rm
K}(\theta,y)$, since, as a field, ${\rm K}(\theta,y)$ has only the trivial
ideals.  The term {\it fractional ideal} is used because an ${\cal
I}$-module can be regarded as a fraction of ideals in ${\cal I}$.

\theorem ${\cal I}$ is a Noetherian ring.
\label{I is Noetherian}

\proof

Since ${\rm K}$ is a field, ${\rm K}[\theta]$ is a Noetherian ring by
the Hilbert basis theorem, ${\rm K}[\theta] \subseteq {\cal I}$, and
${\cal I}$ is finitely generated as a ${\rm K}[\theta]$-module, so
${\cal I}$ is a Noetherian ring by [Atiyah+McDonald] Proposition 7.2.

\endtheorem

\theorem
\label{order of norm}
The order of the norm of $f$ at a point $\theta_0$ is the sum of the orders
of $f$ at all places over $\theta_0$.

\endtheorem


\theorem
\label{simple zero construction}

A function can always be constructed with a simple zero at a specified
finite, ordinary place $(\alpha, \beta)$, zero order at an additional
finite set of finite, ordinary places $\Sigma$, and non-negative order
at all other finite places.

\proof

Begin with the function $(x-\alpha)$, which is a uniformizing variable
and thus has a simple zero at $(\alpha, \beta)$.  If none of the other
places in $\Sigma$ have x-value $\alpha$, then we are done, since
$(x-\alpha)$ has no finite poles.

Otherwise, compute $(x-\alpha)\over(y-\beta)$ at all places in
$\Sigma$ that do {\it not} have $y = \beta$.  Select a number
$\gamma$ different from all of these values.  The function $(x-\alpha)
- \gamma (y-\beta)$ has no finite poles and
is non-zero at all places in $\Sigma$, but it may now have a zero of
higher order at $(\alpha, \beta)$.  Consider a series expansion
of $y$ in terms of $(x-\alpha)$:

$$y = \beta + c_1 (x-\alpha) + c_2 (x-\alpha)^2 + \cdots$$

So long as $\gamma$ is also selected different from $c_1$, $(x-\alpha)
- \gamma (y-\beta)$ will have a first order zero at $(\alpha, \beta)$
and meet all requirements of the theorem.  The simplest way to do this
is to pick a value for $\gamma$, use Theorem \ref{order of norm} to
check if the function has a simple zero, and if not, choose a
different value for $\gamma$.

% SINGULARITIES?

\endtheorem

\theorem
\label{simple pole construction}

A function can always be constructed with a simple pole at a specified
finite, ordinary place $(\alpha, \beta)$, zero order at an additional
finite set of finite, ordinary places $\Sigma$, and non-negative order
at all other places.

\proof

Begin with the function:

$$f(\alpha,y)\over(x-\alpha)(y-\beta)$$

where $f(x,y)$ is the minimum polynomial of the algebraic extension.
Note that the division by $(y-\beta)$ will always be exact, since
$f(\alpha, \beta)=0$.  So we have a rational function
$P(y)\over(x-\alpha)$, where $P(y)$ is a polynomial in $y$.  It has a
simple pole at $(\alpha, \beta)$, as can be seen from a series
expansion in $x-\alpha$ (again, a uniformizing variable).  Since the
$y-\beta$ factor has been divided out of $f(\alpha,y)$, the numerator
is non-zero at $(\alpha, \beta)$, so the leading term in the series
expansion involves $(x-\alpha)^{-1}$, and the pole is thus simple.

This function is finite at all other places, which is obvious except
when $x=\alpha$ and $y\ne\beta$, where it takes the form $0\over0$,
so we can expand it using L'H\^opital's rule:

$$\lim_{(x,y)\to p} {{P(y)}\over(x-\alpha)}
  = \lim_{{(x,y)\to p}} {{{dP(y)}\over{dx}}\over{{d(x-\alpha)}\over{dx}}}
  = P'(y) \, {{dy}\over{dx}} $$

where $'$ denotes differentiation with respect to the polynomial's
variable.  $P'(y)$ is a polynomial, and is thus finite where $y$ is
finite, as is ${dy}\over{dx}$ (consider a series expansion of $y$ in
terms of $(x-\alpha)$, since all places in $\Sigma$ are finite and
non-singular).  It follows that the function is at least finite
everywhere except at $(\alpha, \beta)$.

% This function is finite at all other places, which is obvious except
% when $x=\alpha$ and $y\ne\beta$, where it takes the form $0\over0$.
A more algebraic way to prove this is to note that
$f(x,y)$ has a simple zero at every place over $x=\alpha$ (assuming
there are no multiple points over $x=\alpha$), so $P(y)$ will have a
simple zero at every place over $x=\alpha$ except $(\alpha, \beta)$,
which will exactly cancel the simple pole from $(x-\alpha)$.

Now, compute the value of the function at all other places in
$\Sigma$, using either L'H\^opital's rule or Puiseux expansion if some
of these are over $\alpha$.  If the value of the function is non-zero
at all of these places, then we are done.  Otherwise, select a number
$\gamma$ different from all of these values.  The function:

$${f(\alpha,y)\over(x-\alpha)(y-\beta)} - \gamma$$

has the desired properties, since it still has a simple pole at
$(\alpha,\beta)$, has no other poles, and is now non-zero at all
places in $\Sigma$.

We can avoid computing any expansions by picking random values of
$\gamma$, and using Theorem \ref{order of norm} to check for any extra
zeros.  Since only a finite number of $\gamma$ values produce extra
zeros, this process is guaranteed to terminate.

% SINGULARITIES?

\endtheorem

\theorem
\label{finite orders construction}

A function can always be constructed with specified integer orders at
a finite set of finite, non-singular places $\Sigma$ and non-negative
order at all other finite places.

\proof

For each pole or zero, use Theorems \ref{simple zero construction} or
\ref{simple pole construction} to construct a function with a simple
pole or a simple zero at that place, zero order at all other places in
$\Sigma$ and non-negative order at all other finite places.  Raise
each of these function to the integer power that is the order of the
corresponding pole or zero, then multiply them all together.

\endtheorem

\definition

A {\bf finite multiple} of a divisor is a function with order equal to
or greater than that required by the divisor at all {\it finite} places.

\enddefinition

For the remainder of this section, I'll assume that our divisors
involve only finite, ordinary places, which can always be guaranteed
in the case of the integration theory.

\theorem
\label{exact order existance}

For any divisior ${\cal D}$ and any finite, non-singular place
$\mathfrak{p}$, at least one finite multiple of ${\cal D}$ exists with
order at $\mathfrak{p}$ exactly that required by ${\cal D}$.

\proof

Use Theorem \ref{finite orders construction} with the zeros and poles
required by ${\cal D}$, adding $\mathfrak{p}$
to $\Sigma$ if necessary.

\endtheorem

\theorem
\label{divisor-module isomorphism}

There is a one-to-one relationship between finitely generated integral
modules and divisors.  Such a module consists of all finite multiples
of its associated divisor, and the order of a module's divisor at
every finite place is the minimum of the orders of the module's
generators at that place.

\proof

For a given divisor ${\cal D}$, consider the set ${\cal M}({\cal D})$
of all finite multiples of ${\cal D}$.
Now, adding two
elements can not reduce their order at any finite place, nor can
multiplying an element by an integral element $i \in {\cal I}$, so
${\cal M}({\cal D})$ is clearly an ${\cal I}$-module, but it
might not be finitely generated.

Since ${\cal D}$ has only a finite number of poles, we can always
construct a function with order equal or less than that of ${\cal D}$
at all finite places simply by taking the inverse of the polynomial
$r=(x-p_1)^{m_1} \cdots (x-p_n)^{m_n}$ where $p_i$ are the x-coordinates
of the poles in ${\cal D}$ and $m_i$ are their multiplicities.
For any $m \in {\cal M}({\cal D})$, $mr$ is integral, so ${\cal M}({\cal D})
\subseteq {\cal I}\{r^{-1}\}$, where ${\cal I}\{r^{-1}\}$ is the ${\cal I}$-module
generated by $r^{-1}$.  Now, since ${\cal I}\{r^{-1}\}$ is a finitely
generated module over a Noetherian ring (remember Theorem \ref{I is
Noetherian}), ${\cal I}\{r^{-1}\}$ is a Noetherian module by
[Atiyah+McDonald] Proposition 6.5, and ${\cal M}({\cal D})$ must also
be a finitely generated ${\cal I}$-module by [Atiyah+McDonald]
Proposition 6.2.

Let $(b_1,...,b_n)$ be an ${\cal I}$-module basis for ${\cal M}({\cal
D})$.  Since there is no way to lower the orders of an element using
${\cal I}$-module constructions, and by Theorem \ref{exact order
existance} for each place there is at least one function in ${\cal
M}({\cal D})$ with order exactly that required by ${\cal D}$, it
follows that for each place there must be at least one basis element
with exactly the order required by ${\cal D}$.  Futhermore, no basis
element can have an order less than required by ${\cal D}$ at any
finite place, since that element would not be a finite multiple of
${\cal D}$.  Therefore, at each place $\mathfrak{p}$, the minimum of
the orders of the basis elements must be exactly the order required by
${\cal D}$.

Conversely, given a finitely generated ${\cal I}$-module $M$,
construct its associated divisor ${\cal D}$ by taking at every place
the minimum of the orders of the module's generators at that place.
Clearly, $M \subseteq {\cal M}({\cal D})$, but some finite multiple of
${\cal D}$ might not be in $M$.

To eliminate this possibility, take the module's generators, say
$\{b_1, b_2, b_3\}$ and expand them into a set where each additional
generator beyond the first lowers the module's order by one at a
single place, say $\{b_1, b_2', b_2, b_3'', b_3', b_3\}$.  These
additional generators can be constructed by multiplying the original
generators by integral elements (constructed using Theorem \ref{finite
orders construction}) to remove any additional poles, so $b_2' = i_2'
b_2$, where $i_2' \in {\cal I}$.

This new module $M'$ clearly has the same associated divisor as $M$,
and I'll now show inductively that any $f \in {\cal M}({\cal D})$ can
be found in $M'$.  Let ${\cal D}_n$ be the divisor associated with the
first $n$ basis elements of $M'$.  Clearly, any finite multiple of
${\cal D}_1$ can be constructed as integral element times $b_1$, so
let's now assume that any finite multiple of ${\cal D}_{n-1}$ can be
constructed with the first $n-1$ generators, and consider the $n^{\rm
th}$ generator $g_n$.  It lowers the order by one at a single place,
so any $f \in {\cal M}({\cal D}_n) - {\cal M}({\cal D}_{n-1})$ must
have exactly the same order as $g_n$.  Multiplying $g_n$ by a suitable
constant (the ratio of coefficients in $f$ and $g_n$'s series
expansion at this place) will exactly cancel this pole, so $f - cg \in
{\cal M}({\cal D}_n)$.

So any $f \in {\cal M}({\cal D})$ can be constructed using the
integral module $M'$.  Writing this construction in matrix form
shows how $f$ can be constructed as an $M$-module element:

$$\begin{pmatrix}a_1 & \cdots & a_m\end{pmatrix} \begin{pmatrix}b_1 \cr b_2' \cr b_2 \cr b_3' \cr b_3\end{pmatrix}
= \begin{pmatrix}a_1 & \cdots & a_m\end{pmatrix}\begin{pmatrix}1 & 0 & 0 \cr 0 & i_2' & 0 \cr 0 & 1 & 0 \cr 0 & 0 & i_3' \cr 0 & 0 &1\end{pmatrix}\begin{pmatrix}b_1 \cr b_2 \cr b_3\end{pmatrix}
= \begin{pmatrix}a_1' & \cdots & a_3'\end{pmatrix}\begin{pmatrix}b_1 \cr b_2 \cr b_3\end{pmatrix}
$$


  Consider such a finite multiple $f$.  For every
$m \in {\cal M}$ (in particular, its basis elements), $f$ must have
lower order than $m$ at at least one finite place $\mathfrak{p}$,
since otherwise $i = fm^{-1}$ would be integral and $f$ would exist in
${\cal M}$ as $mi$.  Yet ${\cal D}$, by definition, is the minimum of
the orders of ${\cal M}$'s basis elements at every finite place.
Therefore $f$ can not have lower order than a basis element at any
finite place and thus can not exist.

\endtheorem

Theorem \ref{divisor-module isomorphism} shows that an
${\cal I}$-module is associated with every divisor, but
now we need a constructive procedure for forming an ${\cal I}$-module
basis for a given divisor.

\theorem
\label{divisor basis construction}

Given a divisor ${\cal D}$, a pair of functions can always be
constructed that generate the divisor's associated integral module.

\proof

Use Theorem \ref{finite orders construction} to construct a function
$f$ with the divisor's required poles and zeros, zero order at all
other places conjugate to those poles and zeros, and non-negative
order elsewhere.  Construct $g$, a polynomial in $x$ with n-th order
roots at all points under n-th order zeros.  $(f,g)$ is the required
basis.  The only finite poles are those of $f$ and $g$ has zero order
everywhere except at $f$'s zeros and their conjugates, so by Theorem
\ref{divisor-module isomorphism}, $(f,g)$ forms a basis for
${\cal D}$'s associated ${\cal I}$-module.

\endtheorem

% \vfill\eject

Of course, the whole point here is to actually find a function with a
specified set of zeros and poles, so once we have constructed a basis
for a divisor's associated integral module, we need to determine if
the module is principal.  Since the total order of a field element is
always zero, this only makes sense for divisors of zero order, since
divisors of non-zero order can never be principal.  Futhermore, since
an integral module corresponds to {\it finite} multiples of a divisor,
we can't use this technique to find functions with poles or zeros at
infinity, but that isn't a serious limitation since if we need such a
function, we can just transform into a field with a different point at
infinity.

Since a finite multiple of a divisor differs from an exact multiple in
that the finite multiple can have additional finite zeros, and thus
additional infinite poles (since they always balance), a zero order
${\cal I}$-module is principle iff it contains a function with no
poles at infinity.  We can determine this by expressing the ${\cal
I}$-module as a ${\rm K}[x]$-module, simply by multiplying the ${\cal
I}$-module basis through by an integral basis (remember that an
integral basis is simply a basis for ${\cal I}$ as a ${\rm
K}[x]$-module).

We now transform this ${\rm K}[x]$-module basis to make it {\it normal
at infinity}, i.e, to ensure that poles don't cancel between terms.
First, we use a series of row-equivalent transformations to reduce our
$2n$ basis elements to $n$ elements, then additional transformations
to make it normal.

We then check these basis elements to see if one of them has no poles
at infinity.  The most straightforward way to do this is to invert the
field using $z={1\over x}$, which swaps zero with infinity.  We can
then construct an integral basis for the inverse field, and express
each of the module's basis elements (after inverting them) using this
inverse basis.  If there are any poles at infinity in the original
field, they will appear as poles at zero in the inverse field, and can
easily be detected by checking if $z=0$ is a zero of the denominators.

Finally, let me note that since $g$ in Theorem \ref{divisor basis
construction} is a polynomial, it always has poles at infinity (unless
the divisor has no zeros, and is thus trivially constant), and can
thus be excluded from consideration.  We need only look at the
function $f$, and perhaps not even all of its integral multiples
(CHECK THIS).

\theorem
\label{Trager's residue theorem}

Let $f \,dx$ be a differential with order greater than or equal to -1
at some place $\mathfrak{p}$ with branching index $r$ centered at
$x_0$.  The residue of $f \,dx$ at $\mathfrak{p}$ is equal to the
value of the function $r(x-x_0)f$ at $\mathfrak{p}$. ([Trager], p. 56,
taken almost verbatim)

\proof

Let $t$ be a uniformizing parameter at $\mathfrak{p}$.  Since
$x-x_0$ has order $r$ at $\mathfrak{p}$, it can be written as

$$x-x_0 = t^r g$$

where $g$ has order zero at $\mathfrak{p}$

$$dx = (rt^{r-1}g + t^r{{dg}\over{dt}})dt$$

Since ${dg}\over{dt}$ has non-negative order at $\mathfrak{p}$,
$dx$ has order $r-1$ at $\mathfrak{p}$ and $f$ must have order
greater than or equal to $-r$ at $\mathfrak{p}$

$$f\,dx = rt^{r-1}f g\, dt + t^rf({{dg}\over{dt}})dt$$

the second term on the right side is holomorphic at $\mathfrak{p}$ so
the residue of $f\,dx$ at $\mathfrak{p}$ is the same as the residue of
the first term on the right side.  Since this term is expressed using
the differential of a uniformizing paramter, its residue is the
residue of $rt^{r-1}fg$, which is the value of $rt^rfg = r(x-x_0)f$.

\endtheorem

\vfill\eject
\section{Se\~nor Gonzalez, otra vez}

The Rothstein-Trager resultant allows us to compute all the residues
at once.  Trager, in his Ph.D. thesis, then showed how to construct a
function that is zero at all poles with a given residue, and non-zero
at all other poles, as well as at all places conjugate to a pole.
