
\setcounter{chapter}{8}
\mychapter{Simple Algebraic Extensions}

We now turn to the algebraic extension.  Theorem ? allows us to
collapse any two adjacent algebraic extensions together, so we need
only consider an algebraic extension over a transcendental extension.
The most basic case, the one that we will study in this chapter, is
when the integrand involves only polynomials and a single root, so we
are integrating on an algebraic curve and our algebraic extension
occurs directly over the variable of integration: ${\bf C}(x,y)$.
However, many of this chapter's basic results are applicable in the
more general case where we have a series of field extensions that end
in a transcendental (exponential or logarithmic) extension followed by
an algebraic extension.  I'll use the notation ${\rm K}(\theta, y)$ to
emphasize when this is the case.

\input{09a-ALGEBRAIC-THEORY.tex}

% \vfill\eject
\mysection{Divisors and Integral Modules}

In ${\bf C}(x)$, we were working with the quotient field of a
principal ideal ring, so we could always find a single function to
generate any finitely generated ${\bf C}[x]$-module, simply by putting
all the generators over a common denominator, then taking the
G.C.D. of the numerators.

In ${\rm K}(\theta,y)$, we are no longer working with a principal ideal
ring, so we can't guarantee that any particular ideal can be generated
by a single function, but it turns out that every ideal can be
generated by a {\it pair} of functions.  Our course of attack is first
to construct that pair of functions, then use them to determine if in
fact the ideal is principal.

\definition

An {\bf integral module} (or ${\cal I}$-module) is a module formed
over ${\cal I}$, the ring of integral elements in ${\rm K}(\theta,y)$.

\enddefinition

Since ${\cal I}$ itself can be expressed as a ${\rm K}[\theta]$-module
using an integral basis, any ${\cal I}$-module is also a ${\rm
K}[\theta]$-module.  Not all ${\rm K}[\theta]$-modules are ${\cal
I}$-modules, however, since ${\cal I}$ is typically larger than ${\rm
K}[x]$.

% Existance of an integral basis is demonstrated by Atiyah and MacDonald
% via Propositions 5.17, 6.5, and 6.2.  Van der Waerden proves this on
% pp. 174-175 of volume 2.  That ${\cal I}$ is a free module can be
% proven using Lang's theorem 7.3.

Some authors use the term {\it fractional ideal} to refer to an ${\cal
I}$-module.  I have avoided use of this term for two reasons.  First,
I wish to emphasize the concept of a module.  Second, ${\cal
I}$-modules are not ideals, either in the ring ${\cal I}$ (since they
may contain elements not in ${\cal I}$), nor in the field ${\rm
K}(\theta,y)$, since, as a field, ${\rm K}(\theta,y)$ has only the trivial
ideals.  The term {\it fractional ideal} is used because an ${\cal
I}$-module can be regarded as a fraction of ideals in ${\cal I}$.

\theorem ${\cal I}$ is a Noetherian ring.
\label{I is Noetherian}

\proof

Since ${\rm K}$ is a field, ${\rm K}[\theta]$ is a Noetherian ring by
the Hilbert basis theorem, ${\rm K}[\theta] \subseteq {\cal I}$, and
${\cal I}$ is finitely generated as a ${\rm K}[\theta]$-module, so
${\cal I}$ is a Noetherian ring by [Atiyah+McDonald] Proposition 7.2.

\endtheorem

\theorem
\label{order of norm}
The order of the norm of $f$ at a point $\theta_0$ is the sum of the orders
of $f$ at all places over $\theta_0$.

\endtheorem


\theorem
\label{simple zero construction}

A function can always be constructed with a simple zero at a specified
finite, ordinary place $(\alpha, \beta)$, zero order at an additional
finite set of finite, ordinary places $\Sigma$, and non-negative order
at all other finite places.

\proof

Begin with the function $(x-\alpha)$, which is a uniformizing variable
and thus has a simple zero at $(\alpha, \beta)$.  If none of the other
places in $\Sigma$ have x-value $\alpha$, then we are done, since
$(x-\alpha)$ has no finite poles.

Otherwise, compute $(x-\alpha)\over(y-\beta)$ at all places in
$\Sigma$ that do {\it not} have $y = \beta$.  Select a number
$\gamma$ different from all of these values.  The function $(x-\alpha)
- \gamma (y-\beta)$ has no finite poles and
is non-zero at all places in $\Sigma$, but it may now have a zero of
higher order at $(\alpha, \beta)$.  Consider a series expansion
of $y$ in terms of $(x-\alpha)$:

$$y = \beta + c_1 (x-\alpha) + c_2 (x-\alpha)^2 + \cdots$$

So long as $\gamma$ is also selected different from $c_1$, $(x-\alpha)
- \gamma (y-\beta)$ will have a first order zero at $(\alpha, \beta)$
and meet all requirements of the theorem.  The simplest way to do this
is to pick a value for $\gamma$, use Theorem \ref{order of norm} to
check if the function has a simple zero, and if not, choose a
different value for $\gamma$.

% SINGULARITIES?

\endtheorem

\theorem
\label{simple pole construction}

A function can always be constructed with a simple pole at a specified
finite, ordinary place $(\alpha, \beta)$, zero order at an additional
finite set of finite, ordinary places $\Sigma$, and non-negative order
at all other places.

\proof

Begin with the function:

$$f(\alpha,y)\over(x-\alpha)(y-\beta)$$

where $f(x,y)$ is the minimum polynomial of the algebraic extension.
Note that the division by $(y-\beta)$ will always be exact, since
$f(\alpha, \beta)=0$.  So we have a rational function
$P(y)\over(x-\alpha)$, where $P(y)$ is a polynomial in $y$.  It has a
simple pole at $(\alpha, \beta)$, as can be seen from a series
expansion in $x-\alpha$ (again, a uniformizing variable).  Since the
$y-\beta$ factor has been divided out of $f(\alpha,y)$, the numerator
is non-zero at $(\alpha, \beta)$, so the leading term in the series
expansion involves $(x-\alpha)^{-1}$, and the pole is thus simple.

This function is finite at all other places, which is obvious except
when $x=\alpha$ and $y\ne\beta$, where it takes the form $0\over0$,
so we can expand it using L'H\^opital's rule:

$$\lim_{(x,y)\to p} {{P(y)}\over(x-\alpha)}
  = \lim_{{(x,y)\to p}} {{{dP(y)}\over{dx}}\over{{d(x-\alpha)}\over{dx}}}
  = P'(y) \, {{dy}\over{dx}} $$

where $'$ denotes differentiation with respect to the polynomial's
variable.  $P'(y)$ is a polynomial, and is thus finite where $y$ is
finite, as is ${dy}\over{dx}$ (consider a series expansion of $y$ in
terms of $(x-\alpha)$, since all places in $\Sigma$ are finite and
non-singular).  It follows that the function is at least finite
everywhere except at $(\alpha, \beta)$.

% This function is finite at all other places, which is obvious except
% when $x=\alpha$ and $y\ne\beta$, where it takes the form $0\over0$.
A more algebraic way to prove this is to note that
$f(x,y)$ has a simple zero at every place over $x=\alpha$ (assuming
there are no multiple points over $x=\alpha$), so $P(y)$ will have a
simple zero at every place over $x=\alpha$ except $(\alpha, \beta)$,
which will exactly cancel the simple pole from $(x-\alpha)$.

Now, compute the value of the function at all other places in
$\Sigma$, using either L'H\^opital's rule or Puiseux expansion if some
of these are over $\alpha$.  If the value of the function is non-zero
at all of these places, then we are done.  Otherwise, select a number
$\gamma$ different from all of these values.  The function:

$${f(\alpha,y)\over(x-\alpha)(y-\beta)} - \gamma$$

has the desired properties, since it still has a simple pole at
$(\alpha,\beta)$, has no other poles, and is now non-zero at all
places in $\Sigma$.

We can avoid computing any expansions by picking random values of
$\gamma$, and using Theorem \ref{order of norm} to check for any extra
zeros.  Since only a finite number of $\gamma$ values produce extra
zeros, this process is guaranteed to terminate.

% SINGULARITIES?

\endtheorem

\theorem
\label{finite orders construction}

A function can always be constructed with specified integer orders at
a finite set of finite, non-singular places $\Sigma$ and non-negative
order at all other finite places.

\proof

For each pole or zero, use Theorems \ref{simple zero construction} or
\ref{simple pole construction} to construct a function with a simple
pole or a simple zero at that place, zero order at all other places in
$\Sigma$ and non-negative order at all other finite places.  Raise
each of these function to the integer power that is the order of the
corresponding pole or zero, then multiply them all together.

\endtheorem

\definition

A {\bf finite multiple} of a divisor is a function with order equal to
or greater than that required by the divisor at all {\it finite} places.

\enddefinition

For the remainder of this section, I'll assume that our divisors
involve only finite, ordinary places, which can always be guaranteed
in the case of the integration theory.

\theorem
\label{exact order existance}

For any divisior ${\cal D}$ and any finite, non-singular place
$\mathfrak{p}$, at least one finite multiple of ${\cal D}$ exists with
order at $\mathfrak{p}$ exactly that required by ${\cal D}$.

\proof

Use Theorem \ref{finite orders construction} with the zeros and poles
required by ${\cal D}$, adding $\mathfrak{p}$
to $\Sigma$ if necessary.

\endtheorem

\theorem
\label{divisor-module isomorphism}

There is a one-to-one relationship between finitely generated integral
modules and divisors.  Such a module consists of all finite multiples
of its associated divisor, and the order of a module's divisor at
every finite place is the minimum of the orders of the module's
generators at that place.

\proof

For a given divisor ${\cal D}$, consider the set ${\cal M}({\cal D})$
of all finite multiples of ${\cal D}$.
Now, adding two
elements can not reduce their order at any finite place, nor can
multiplying an element by an integral element $i \in {\cal I}$, so
${\cal M}({\cal D})$ is clearly an ${\cal I}$-module, but it
might not be finitely generated.

Since ${\cal D}$ has only a finite number of poles, we can always
construct a function with order equal or less than that of ${\cal D}$
at all finite places simply by taking the inverse of the polynomial
$r=(x-p_1)^{m_1} \cdots (x-p_n)^{m_n}$ where $p_i$ are the x-coordinates
of the poles in ${\cal D}$ and $m_i$ are their multiplicities.
For any $m \in {\cal M}({\cal D})$, $mr$ is integral, so ${\cal M}({\cal D})
\subseteq {\cal I}\{r^{-1}\}$, where ${\cal I}\{r^{-1}\}$ is the ${\cal I}$-module
generated by $r^{-1}$.  Now, since ${\cal I}\{r^{-1}\}$ is a finitely
generated module over a Noetherian ring (remember Theorem \ref{I is
Noetherian}), ${\cal I}\{r^{-1}\}$ is a Noetherian module by
[Atiyah+McDonald] Proposition 6.5, and ${\cal M}({\cal D})$ must also
be a finitely generated ${\cal I}$-module by [Atiyah+McDonald]
Proposition 6.2.

Let $(b_1,...,b_n)$ be an ${\cal I}$-module basis for ${\cal M}({\cal
D})$.  Since there is no way to lower the orders of an element using
${\cal I}$-module constructions, and by Theorem \ref{exact order
existance} for each place there is at least one function in ${\cal
M}({\cal D})$ with order exactly that required by ${\cal D}$, it
follows that for each place there must be at least one basis element
with exactly the order required by ${\cal D}$.  Futhermore, no basis
element can have an order less than required by ${\cal D}$ at any
finite place, since that element would not be a finite multiple of
${\cal D}$.  Therefore, at each place $\mathfrak{p}$, the minimum of
the orders of the basis elements must be exactly the order required by
${\cal D}$.

Conversely, given a finitely generated ${\cal I}$-module $M$,
construct its associated divisor ${\cal D}$ by taking at every place
the minimum of the orders of the module's generators at that place.
Clearly, $M \subseteq {\cal M}({\cal D})$, but some finite multiple of
${\cal D}$ might not be in $M$.

To eliminate this possibility, take the module's generators, say
$\{b_1, b_2, b_3\}$ and expand them into a set where each additional
generator beyond the first lowers the module's order by one at a
single place, say $\{b_1, b_2', b_2, b_3'', b_3', b_3\}$.  These
additional generators can be constructed by multiplying the original
generators by integral elements (constructed using Theorem \ref{finite
orders construction}) to remove any additional poles, so $b_2' = i_2'
b_2$, where $i_2' \in {\cal I}$.

This new module $M'$ clearly has the same associated divisor as $M$,
and I'll now show inductively that any $f \in {\cal M}({\cal D})$ can
be found in $M'$.  Let ${\cal D}_n$ be the divisor associated with the
first $n$ basis elements of $M'$.  Clearly, any finite multiple of
${\cal D}_1$ can be constructed as integral element times $b_1$, so
let's now assume that any finite multiple of ${\cal D}_{n-1}$ can be
constructed with the first $n-1$ generators, and consider the $n^{\rm
th}$ generator $g_n$.  It lowers the order by one at a single place,
so any $f \in {\cal M}({\cal D}_n) - {\cal M}({\cal D}_{n-1})$ must
have exactly the same order as $g_n$.  Multiplying $g_n$ by a suitable
constant (the ratio of coefficients in $f$ and $g_n$'s series
expansion at this place) will exactly cancel this pole, so $f - cg \in
{\cal M}({\cal D}_n)$.

So any $f \in {\cal M}({\cal D})$ can be constructed using the
integral module $M'$.  Writing this construction in matrix form
shows how $f$ can be constructed as an $M$-module element:

$$\begin{pmatrix}a_1 & \cdots & a_m\end{pmatrix} \begin{pmatrix}b_1 \cr b_2' \cr b_2 \cr b_3' \cr b_3\end{pmatrix}
= \begin{pmatrix}a_1 & \cdots & a_m\end{pmatrix}\begin{pmatrix}1 & 0 & 0 \cr 0 & i_2' & 0 \cr 0 & 1 & 0 \cr 0 & 0 & i_3' \cr 0 & 0 &1\end{pmatrix}\begin{pmatrix}b_1 \cr b_2 \cr b_3\end{pmatrix}
= \begin{pmatrix}a_1' & \cdots & a_3'\end{pmatrix}\begin{pmatrix}b_1 \cr b_2 \cr b_3\end{pmatrix}
$$


  Consider such a finite multiple $f$.  For every
$m \in {\cal M}$ (in particular, its basis elements), $f$ must have
lower order than $m$ at at least one finite place $\mathfrak{p}$,
since otherwise $i = fm^{-1}$ would be integral and $f$ would exist in
${\cal M}$ as $mi$.  Yet ${\cal D}$, by definition, is the minimum of
the orders of ${\cal M}$'s basis elements at every finite place.
Therefore $f$ can not have lower order than a basis element at any
finite place and thus can not exist.

\endtheorem

Theorem \ref{divisor-module isomorphism} shows that an
${\cal I}$-module is associated with every divisor, but
now we need a constructive procedure for forming an ${\cal I}$-module
basis for a given divisor.

\theorem
\label{divisor basis construction}

Given a divisor ${\cal D}$, a pair of functions can always be
constructed that generate the divisor's associated integral module.

\proof

Use Theorem \ref{finite orders construction} to construct a function
$f$ with the divisor's required poles and zeros, zero order at all
other places conjugate to those poles and zeros, and non-negative
order elsewhere.  Construct $g$, a polynomial in $x$ with n-th order
roots at all points under n-th order zeros.  $(f,g)$ is the required
basis.  The only finite poles are those of $f$ and $g$ has zero order
everywhere except at $f$'s zeros and their conjugates, so by Theorem
\ref{divisor-module isomorphism}, $(f,g)$ forms a basis for
${\cal D}$'s associated ${\cal I}$-module.

\endtheorem

% \vfill\eject

Of course, the whole point here is to actually find a function with a
specified set of zeros and poles, so once we have constructed a basis
for a divisor's associated integral module, we need to determine if
the module is principal.  Since the total order of a field element is
always zero, this only makes sense for divisors of zero order, since
divisors of non-zero order can never be principal.  Futhermore, since
an integral module corresponds to {\it finite} multiples of a divisor,
we can't use this technique to find functions with poles or zeros at
infinity, but that isn't a serious limitation since if we need such a
function, we can just transform into a field with a different point at
infinity.

Since a finite multiple of a divisor differs from an exact multiple in
that the finite multiple can have additional finite zeros, and thus
additional infinite poles (since they always balance), a zero order
${\cal I}$-module is principle iff it contains a function with no
poles at infinity.  We can determine this by expressing the ${\cal
I}$-module as a ${\rm K}[x]$-module, simply by multiplying the ${\cal
I}$-module basis through by an integral basis (remember that an
integral basis is simply a basis for ${\cal I}$ as a ${\rm
K}[x]$-module).

We now transform this ${\rm K}[x]$-module basis to make it {\it normal
at infinity}, i.e, to ensure that poles don't cancel between terms.
First, we use a series of row-equivalent transformations to reduce our
$2n$ basis elements to $n$ elements, then additional transformations
to make it normal.

We then check these basis elements to see if one of them has no poles
at infinity.  The most straightforward way to do this is to invert the
field using $z={1\over x}$, which swaps zero with infinity.  We can
then construct an integral basis for the inverse field, and express
each of the module's basis elements (after inverting them) using this
inverse basis.  If there are any poles at infinity in the original
field, they will appear as poles at zero in the inverse field, and can
easily be detected by checking if $z=0$ is a zero of the denominators.

Finally, let me note that since $g$ in Theorem \ref{divisor basis
construction} is a polynomial, it always has poles at infinity (unless
the divisor has no zeros, and is thus trivially constant), and can
thus be excluded from consideration.  We need only look at the
function $f$, and perhaps not even all of its integral multiples
(CHECK THIS).

\theorem
\label{Trager's residue theorem}

Let $f \,dx$ be a differential with order greater than or equal to -1
at some place $\mathfrak{p}$ with branching index $r$ centered at
$x_0$.  The residue of $f \,dx$ at $\mathfrak{p}$ is equal to the
value of the function $r(x-x_0)f$ at $\mathfrak{p}$. ([Trager], p. 56,
taken almost verbatim)

\proof

Let $t$ be a uniformizing parameter at $\mathfrak{p}$.  Since
$x-x_0$ has order $r$ at $\mathfrak{p}$, it can be written as

$$x-x_0 = t^r g$$

where $g$ has order zero at $\mathfrak{p}$

$$dx = (rt^{r-1}g + t^r{{dg}\over{dt}})dt$$

Since ${dg}\over{dt}$ has non-negative order at $\mathfrak{p}$,
$dx$ has order $r-1$ at $\mathfrak{p}$ and $f$ must have order
greater than or equal to $-r$ at $\mathfrak{p}$

$$f\,dx = rt^{r-1}f g\, dt + t^rf({{dg}\over{dt}})dt$$

the second term on the right side is holomorphic at $\mathfrak{p}$ so
the residue of $f\,dx$ at $\mathfrak{p}$ is the same as the residue of
the first term on the right side.  Since this term is expressed using
the differential of a uniformizing paramter, its residue is the
residue of $rt^{r-1}fg$, which is the value of $rt^rfg = r(x-x_0)f$.

\endtheorem

\vfill\eject

\example Compute $\int {1\over{\sqrt{1-x^2}}} \,dx$

The obvious attempt is to use the algebraic extension $y^2=1-x^2$ and
integrate ${1\over y}\,dx$.

But we first need to determine if this differential has any poles at
infinity, by inverting the field and looking for poles at zero.
Setting $u={1\over x}$, we convert our minimal polynomial into
$u^2y^2=u^2-1$ (after multiplying through by $u^2$), and using
$v=uy$ we obtain our inverse field ${\bf C}(u,v); v^2=u^2-1$.

Since $x={1\over u}$ and $y={v\over u}$, we convert our differential as follows:

 $${1\over y}\,dx ={u\over v} (-{1\over{u^2}} \, du) = -{1\over{uv}} \, du$$

Now, $\{1, v\}$ is an integral basis for the inverse field, so we
multiply through by $v\over v$ to obtain:

 $$= -{v\over{uv^2}} du = -{1\over{u(u^2-1)}}v \, du $$

which is now in normal form and clearly has a pole at $u=0$, or $x=\infty$.  Note that

 $${1\over y} = {u\over v} = {{uv}\over{v^2}}
 = {u\over{u^2-1}} v$$

has no pole at $u=0$, a clear example of a differential having a pole
at a place where its constituent function has none.

In any event, we clearly can not use the original field to conduct the
integration, since it would require constructing a function with a
pole at infinity, and our algorithm can't handle this.  So we need to
transform into a field where the differential has no pole at infinity.

Actually, we've already done this!  Note that the integrand had no pole
at zero in the original field:

 $${1\over y}\,dx = {y\over y^2}\,dx = {1\over{1-x^2}}y \,dx $$

Since the inverse field swapped zero with infinity, it follows that
there is no pole at infinity in the inverse field, so we can proceed
to integrate $-{1\over{u(u^2-1)}}v \,du$ in ${\bf C}(u,v)$;
$v^2=u^2-1$.

Simple inspection of the integrand (already in normal form) shows that
its poles are at $(0, i)$, $(0, -i)$, $(1, 0)$, and $(-1, 0)$.
Remember that we're now working on the Riemann surface of an algebraic
extension, so we need to specify $\it both$ $u$ and $v$ to
specify a place.

The next step is to compute the residues at each of these places,
using Theorem \ref{Trager's residue theorem}:

\begin{center}
\begin{supertabular}{l l l}
  $(0, i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, i)$     & = $i$    \cr
  $(0, -i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, -i)$   & = $-i$    \cr
  $(1, 0)$  &  $\displaystyle -2{1\over{u(u+1)}}v$ @ $(1, 0)$      & = $0$    \cr
  $(-1, 0)$  &  $\displaystyle -2{1\over{u(u-1)}}v$ @ $(-1, 0)$    & = $0$    \cr
\end{supertabular}
\end{center}

The poles with zero residues can be ignored.  We're interested in the
other two, which exist in ${\bf Q}[i]$, which can be regarded as a
vector field over ${\bf Q}$ with basis $\{1, i\}$, and we want to
construct a function whose poles and zeros match the $i$-component of
the residues (the 1-component is uniformly zero).

We start by constructing an ${\cal I}$-module generator set for the divisor
with a simple zero at $(0,i)$ and a simple pole at $(0,-i)$.  Theorem
\ref{simple pole construction} shows that:

$$f = {{v^2+1}\over{u(v+i)}} = {{v-i}\over{u}} $$

has a simple pole at $(0,-i)$.  At $(0,i)$, L'H\^opital's rule gives:

$$ \lim_{(u,v)\to (0,i)} {{v-i}\over{u}}
   = {{(v-i)'}\over{u'}} {{dv}\over{du}} = {{dv}\over{du}} = {u\over v} = 0 $$

where the last transformation was accomplished by differentiating the
mimimal polynomial.  So $f$ has a zero at $(0,i)$, and I'll note that
we've just stumbled into the solution.  Theorem \ref{simple pole
construction} already assures us that $f$ has only a single finite
simple pole, and we can see that its only zeros occur when
$v-i=0$, which, according to the minimum polynomial, can only
occur at $u=0$, thus $(0,i)$ is its only finite zero, and it is
simple, as we can verify by showing that the corresponding pole in its
inverse is simple:

$$ {1\over f} = {u\over{v-i}} = {{u(v+i)}\over{v^2+1}}
  = {{u(v+i)}\over{u^2}} = {1\over u}v + {i\over u} $$


So we've found the function we're looking for by accident.  Let's save the
general case for the next example, and convert back to
our original field:

$${{v-i}\over{u}} = x({y\over x}-i) = y - ix $$

Remembering that our residues came multiplied by a factor of $i$, we
conclude that our solution is $i\,\ln(y-ix)$, or:

\begin{eqnarray*}
\int {1\over{\sqrt{1-x^2}}} \,dx &=& i\,\ln(\sqrt{1-x^2}-ix) \\
                                 &=& -i\,\ln({1\over{\sqrt{1-x^2}-ix}}) \\
                                 &=& -i\,\ln({{\sqrt{1-x^2}+ix}\over{1-x^2+x^2}}) \\
                                 &=& -i\,\ln({\sqrt{1-x^2}+ix}) \\
                                 &=& \arcsin x \\
\end{eqnarray*}

where I used the negative of a logarithm being the logarithm of
the inverse, and the last transformation came from section ?.


\endexample

\vfill\eject
\mysection{Geddes's example}

\example Compute $\int {1\over{x\sqrt{x^4+1}}} \, dx$

We'll use ${\bf C}(x,y); y^2=x^4+1$ and integrate ${1\over{xy}} =
{y\over{x^5+x}}$.  Inverting this field ($z={1\over x}$) shows that
this integrand has no poles at infinity, so we can proceed directly:

$$ {y\over{x^5+x}} = {y\over{x(x+\omega)(x-\omega)(x+i\omega)(x-i\omega)}} \qquad \omega = \sqrt{i} = {\sqrt{2}\over2} + {\sqrt{2}\over2} i $$

\bigskip
\begin{center}
\begin{supertabular}{l l l}
  $(0, 1)$  &  $\displaystyle {y\over{x^4+1}}$ @ $(0, 1)$     & = $1$    \cr
  $(0, -1)$  &  $\displaystyle {y\over{x^4+1}}$ @ $(0, -1)$     & = $-1$    \cr
  $(\omega, 0)$  &  $\displaystyle 2{y\over{x(x^2+i)(x+\omega)}}$ @ $(\omega, 1)$     & = $0$    \cr
  \multicolumn{2}{l}{$(-\omega, 0), (i\omega, 0), (-i\omega, 0)\qquad\cdots$}    & = $0$    \cr
\end{supertabular}
\end{center}

We now use theorem \ref{simple pole construction} to construct a
function with a simple pole at $(0,-1)$:

$${{f(0,y)}\over{x(y+1)}} = {{y^2-1}\over{x(y+1)}} = {{y-1}\over{x}} $$

This function has a zero at $(0,1)$, but, unfortunately, it is third order,
as can be seen from either L'H\^opital's rule:

$$y^2=x^4+1$$
$$2y\,dy=4x^3\,dx$$
$${{dy}\over{dx}} = 2{x^3\over y}$$

$${{y-1}\over{x}} @ (0,1) = \lim {{dy}\over{dx}} = 2 {{x^3}\over y} = 0$$
$${{y-1}\over{x^2}} @ (0,1) = \lim {1\over{2x}}{{dy}\over{dx}} = {{x^2}\over y} = 0$$
$${{y-1}\over{x^3}} @ (0,1) = \lim {1\over{3x^2}}{{dy}\over{dx}} = {2\over3}{{x}\over y} = 0$$
$${{y-1}\over{x^4}} @ (0,1) = \lim {1\over{4x^3}}{{dy}\over{dx}} = {1\over2}{1\over y} = {1\over2}$$

\vfil\eject

\ldots or from a series expansion of $y$ at (0,1):

$$y^2 = x^4 + 1 $$
$$(y-1)^2 = x^4 - 2(y-1)$$
$$(y-1) = {1\over2}x^4 - {1\over2}(y-1)^2$$
$$(y-1) = c_0 + c_1 x + c_2 x^2 + c_3 x^3 + \cdots$$
$$(y-1)^2 = c_0^2 + (2 c_0 c_1) x + (2 c_0 c_2 + c_1^2) x^2 + (2 c_0 c_3 + 2 c_1 c_2) x^3 + \cdots$$

$$ c_0, c_1, c_2, c_3 = 0 $$
$$ c_4 = {1\over2}$$
$$ c_5, c_6, c_7 = 0 $$
$$ c_8 = -{1\over8} $$

$$ (y-1) = {1\over2} x^4 - {1\over8} x^8 + \cdots$$

$$ {(y-1)\over x} = {1\over2} x^3 - {1\over8} x^7 + \cdots$$

\ldots or from the norm:

$$N({{y-1}\over{x}}) = {{y-1}\over{x}} \cdot {{-y-1}\over{x}} = - {{y^2-1}\over{x^2}} = - {{x^4}\over{x^2}} = - x^2$$

Since we know that the function has a simple pole at $(0,-1)$, so it
must have a third order zero at $(0,1)$ to form a norm with a second
order zero.

We can eliminate the inconvenient zero by adding a constant to the
function, say 1: ${{x+y-1}\over x}$.  We can now use theorem
\ref{simple zero construction} to create a simple zero at
(0,1) by multiplying by $x+y-1$:

$${{x+y-1}\over x} (x+y-1) = {{2x-2}\over x} y + {{x^4+x^2-2x+2}\over x} $$

This function has a simple pole at (0,-1) and a simple zero at (0,1),
but does it have other poles and zeros?  If so, can it be modified to
eliminate them?  To find out, we form the generators of an ${\cal I}$-module:

$$\{ {{2x-2}\over x} y + {{x^4+x^2-2x+2}\over x}, x \}$$

Noting that ${{x^4+x^2}\over{x}} = x(x^2+1)$ and $x^2+1 \in {\cal I}$,
we can simplify this:

$$\{ {{2x-2}\over x} y - {{2x-2}\over x}, x \}$$

\vfill\eject

Using the integral basis $\{1, y \}$, we convert this to a
${\bf C}[x]$-module:

$$\{ {{2x-2}\over x} y - {{2x-2}\over x}, {{2x-2}\over x} (x^4+1) - {{2x-2}\over x} y, x, xy \}$$

and since $(2x-2){{x^4}\over{x}} = x(2x^3-2x^2)$ and
$2x^3-2x^2 \in {\bf C}[x]$, we simplify:

$$\{ {{2x-2}\over x} y - {{2x-2}\over x}, {{2x-2}\over x} - {{2x-2}\over x} y, x, xy \}$$

and write it in matrix form:

% $$\pmatrix{-{{2x-2}\over{x}} & {{2x-2}\over{x}} \cr {{2x-2}\over{x}} & -{{2x-2}\over{x}} \cr x & 0 \cr 0 & x} \pmatrix{1 \cr y}$$

$${1\over x}\begin{pmatrix}-(2x-2) & 2x-2 \cr 2x-2 & -(2x-2) \cr x^2 & 0 \cr 0 & x^2\end{pmatrix} \begin{pmatrix}1 \cr y\end{pmatrix}$$

Elementary row operations\footnote{Read right to left; $R_{i,j,\lambda}$ adds $\lambda$ times row $j$ to row $i$; $R_{i,\alpha}$ multiplies row $i$ by $\alpha$ (a unit)} $R_{4,3,x^2} R_{1,3,(2x-2)} R_{3,4,-1} R_{3,1,{1\over2}(x+1)} R_{2,1,1} $ yield:

$${1\over x}\begin{pmatrix}1 & -1 \cr x^2 & 0\end{pmatrix} \begin{pmatrix}1 \cr y\end{pmatrix} = \begin{pmatrix}{{1-y}\over{x}} \cr x\end{pmatrix} $$

so $\{ {{1-y}\over{x}}, x \} $ forms a generator set for
the ${\bf C}[x]$-module of the finite multiples of $Z(0,1)P(0,-1)$.
We convert to a basis normal at infinity: $\{1, v\} = \{1, {y\over x^2}\}$:

$$\begin{pmatrix}{{1-y}\over{x}} \cr x\end{pmatrix} = \begin{pmatrix}{1\over x} & -x \cr x & 0\end{pmatrix} \begin{pmatrix}1 \cr {y\over x^2}\end{pmatrix}
= \begin{pmatrix}x & \cr & x\end{pmatrix} \begin{pmatrix}{1\over x^2} & -1 \cr 1 & 0\end{pmatrix} \begin{pmatrix}1 \cr {y\over x^2}\end{pmatrix}$$

$$\det_{@ \infty} \begin{pmatrix}{1\over x^2} & -1 \cr 1 & 0\end{pmatrix} = 1$$

so $\{ {{1-y}\over{x}}, x \} $ is normal at infinity.  $x$ clearly has
a pole at infinity, so it can't be the function we're looking for, but
what about ${{1-y}\over{x}}$?  Switching back to $\{u,v\}$
coordinates, we obtain ${{1-y}\over{x}} = {{u^2-v}\over u}$, which has
$\{u,v\}$ poles at both $\{0,1\}$ and $\{0,-1\}$, which translate into
poles at $x=\infty$.  Therefore, no rational function on this
algebraic curve has a simple pole at (0, -1), a simple zero at (0,1),
and no other poles or zeros.

\vfill\eject

So, let's try a double pole at (0,-1) and a double zero at (0,1).  We
can just square our previous generators:
$\{ {{1-y}\over{x}}, x \} $
to obtain: 
$\{ {(1-y)^2\over{x^2}}, x^2 \} = \{ {{1-2y+x^4+1}\over{x^2}}, x^2 \}$
which simplifies to
$\{ {{1-y}\over{x^2}}, x^2 \}$.  We again check for normalcy
at infinity:

$$\begin{pmatrix}
{1\over{x^2}} & -1 \cr
x^2 & 0\end{pmatrix} \begin{pmatrix}1 \cr {y\over{x^2}}\end{pmatrix} = \begin{pmatrix}1 & \cr & x^2\end{pmatrix}\begin{pmatrix}
{1\over{x^2}} & -1 \cr
1 & 0\end{pmatrix}
\begin{pmatrix}1 \cr {y\over{x^2}}\end{pmatrix} $$

$$\det_{@\infty}\begin{pmatrix}
{1\over{x^2}} & -1 \cr
1 & 0\end{pmatrix} = 1$$

So, $\{{{1-y}\over{x^2}}, x^2\}$ is a ${\bf C}[x]$-module, normal at
infinity, containing the finite multiples of $Z^2(0,1)P^2(0,-1)$.
$x^2$ has a pole at infinity, but does ${1-y}\over{x^2}$?
Switching to $x={1\over z}; y={u\over{z^2}}; u^2 = z^4 + 1$ and
Writing it as $z^2(1-{u\over{z^2}}) = z^2 - u$ shows that it has no
zero at $(z,u) = (0, \pm 1)$, and thus no pole at $x = \infty$.  It
is, therefore, the function we are looking for:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx
   = {1\over2} \ln{{1-\sqrt{x^4+1}}\over{x^2}}$$

I'll now point out to you what's been pointed out to me, and that is a
traditional solution technique for this integral:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx$$
$$x^2 = u \qquad 2 x\, dx = du$$
$$\int {1\over{2u\sqrt{u^2+1}}} \, du$$
$$u = \tan z \qquad du = \sec^2 z dz$$
$${1 \over 2} \int \csc z \, dz = {1\over 2} \ln \tan {z\over2}$$
$$={1\over2} \ln {{\sec z - 1}\over{\tan z}}$$
$$={1\over2} \ln {{\sqrt{u^2+1}-1}\over{u}}$$
$$={1\over2} \ln {{\sqrt{x^4+1}-1}\over{x^2}}$$

\vskip 0.5in

$${{1 - \cos z}\over{\sin z}} = {{1 - \cos^2 {z\over2} + \sin^2 {z\over2}}\over{2\sin{z\over2}\cos{z\over2}}} = {{2\sin^2 {z\over2}}\over{2\sin{z\over2}\cos{z\over2}}} = {{\sin {z\over2}}\over{\cos{z\over2}}} $$

\vfill\eject

We can also proceed like this:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx$$
$$x^4 = u \qquad 4 x^3\, dx = du$$
$$\int {1\over{4u\sqrt{u+1}}} \, du$$
$$v = u + 1 \qquad dv = du$$
$$\int {1\over{4(v-1)\sqrt{v}}} \, dv$$
$$z^2 = v \qquad 2 z dz = dv$$
$$\int {1\over{4(z^2-1)z}} \, 2z\, dz$$
$$\int {1\over{2(z^2-1)}} \, dz$$
$${1\over{z^2-1}} = {1\over2}{1\over{z-1}} - {1\over2}{1\over{z+1}}$$
$${1\over4} \int {1\over{z-1}} - {1\over{z+1}} \, dz$$
$${1\over4} \ln (z-1) - \ln (z+1)$$
$${1\over4} \ln {{z-1}\over{z+1}}$$
$${1\over4} \ln {{\sqrt{v}-1}\over{\sqrt{v}+1}}$$
$${1\over4} \ln {{\sqrt{u+1}-1}\over{\sqrt{u+1}+1}}$$
$${1\over4} \ln {{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}}$$

so from the previous page and this one, we conclude

$$\int {1\over{x\sqrt{x^4+1}}} \, dx
={1\over2} \ln {{\sqrt{x^4+1}-1}\over{x^2}}
={1\over4} \ln {{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}}$$

Is this last equality true?  Well, $\ln f^2 = 2\ln f$, so
${1\over4}\ln f^2 = {1\over2}\ln f$, and\ldots

$$\Big({{\sqrt{x^4+1}-1}\over{x^2}}\Big)^2
= {{(\sqrt{x^4+1}-1)^2}\over{x^4}}$$

$${{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}} \cdot
{{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}-1}}
= {{(\sqrt{x^4+1}-1)^2}\over{x^4+1-1}}$$




\endexample

\vfill\eject
\mysection{Chebyshev's Integral}

\example Compute:
$$\int {{2x^6+4x^5+7x^4-3x^3-x^2-8x-8}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \,{\rm d}x$$

The polynomial under the square root is square-free:

{\small\begin{verbatim}
i8 : root = poly "x4+4x3+2x2+1"

      4     3     2
o8 = x  + 4x  + 2x  + 1

o8 : Rx

i9 : diff(x,root)

       3      2
o9 = 4x  + 12x  + 4x

o9 : Rx

i10 : gcd(o9, root)

o10 = 1

o10 : Rx
\end{verbatim}}

\ldots so $y^2 = x^4+4 x^3+2 x^2+1$; $\{1, y\}$ is an integral basis;
and our normal form for this integral is:

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)y}\over{(2x^2-1)^2(x^4+4 x^3+2 x^2+1)}} \,{\rm d}x$$

Applying now Bronstein's Hermite reduction:

{\small\begin{verbatim}
i28 : ytic=diff(x,root)//2/root*y

          3     2
        2x  + 6x  + 2x
o28 = ------------------*y
       4     3     2
      x  + 4x  + 2x  + 1

o28 : Fxy

i29 : U = root

       4     3     2
o29 = x  + 4x  + 2x  + 1

o29 : Rx

i30 : V=poly "2x2-1"

        2
o30 = 2x  - 1

o30 : Rx

i32 : S2=U*(V*ytic - diff(x,V)*y)

           4     3     2
o32 = (- 4x  - 6x  - 6x  - 6x)y

o32 : Fxy

i2 : num=poly "2x6+4x5+7x4-3x3-x2-8x-8"

       6     5     4     3    2
o2 = 2x  + 4x  + 7x  - 3x  - x  - 8x - 8

o2 : Rx

i6 : T2=num

       6     5     4     3    2
o6 = 2x  + 4x  + 7x  - 3x  - x  - 8x - 8

o6 : Rx

i79 : factor(U)

               3     2
o79 = (x + 1)(x  + 3x  - x + 1)

o79 : Expression of class Product

i92 : Q=S2

          4     3     2
o92 = - 4x  - 6x  - 6x  - 6x

o92 : Rx

i93 : A=(gcdCoefficients (V,Q))#1

        36  3   38  2   48
o93 = - --*x  - --*x  - --*x - 1
        49      49      49

o93 : Rx

i94 : R=(gcdCoefficients (V,Q))#2

        18      8
o94 = - --*x + --
        49     49

o94 : Rx

i96 : Q2=(T2*R)//V

        18  5   4  4   8  3   41  2   31     177
o96 = - --*x  - -*x  - -*x  + --*x  - --*x + ---
        49      7      7      49      49      98

o96 : Rx

i97 : B2=(T2*R)%V

          1
o97 = x + -
          2

o97 : Rx

i98 : h = A*num/(V*U) - (diff(x,V)*Q2+diff(x,B2))/V + Q2*ytic

               2
             6x  + 5x + 7
o98 = -------------------------
        6     5     4     3
      2x  + 8x  + 3x  - 4x  - 1

o98 : frac(Rx)

i100 : factor(denominator h)

                 2       3     2
o100 = (x + 1)(2x  - 1)(x  + 3x  - x + 1)

o100 : Expression of class Product


\end{verbatim}}

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)y}\over{(2x^2-1)^2(x^4+4 x^3+2 x^2+1)}} \,{\rm d}x
= {{(x+{1\over2})y}\over{2x^2-1}} + \int {{(6x^2+5x+7)y}\over{(2x^2-1)(x^4+4 x^3+2 x^2+1)}} \,{\rm d}x$$

\bigskip
\begin{center}
Non-zero residues

\begin{supertabular}{r @{} l | r @{} l | r @{} l}
\multicolumn{2}{c|}{x} & \multicolumn{2}{c|}{y} & \multicolumn{2}{c}{residue} \cr
\hline
&$\sqrt{2}\over 2$ & &${1\over 2} + \sqrt{2}$ & &${5\over2}$ \cr
&$\sqrt{2}\over 2$ & $-$&${1\over 2} - \sqrt{2}$ & $-$&${5\over2}$ \cr
$-$&${\sqrt{2}\over 2}$ & &${1\over 2} - \sqrt{2}$ & &${5\over2}$ \cr
$-$&${\sqrt{2}\over 2}$ & $-$&${1\over 2} + \sqrt{2}$ & $-$&${5\over2}$ \cr
\end{supertabular}
\end{center}


$$A(x) = 1023x^8+4104x^7+5048x^6+2182x^5+805x^4+624x^3+10x^2+28x$$
$$B(x) = 1025x^{10} + 6138x^9 + 12307x^8 + 10188x^7 + 4503x^6 + 3134x^5 + 1598x^4 + 140x^3 + 176x^2 +2$$
$$C(x) = 32x^{10}-80x^8+80x^6-40x^4+10x^2-1$$

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)y}\over{(2x^2-1)^2(x^4+4 x^3+2 x^2+1)}} \,{\rm d}x
= {{(x+{1\over2})y}\over{2x^2-1}} + {1\over2}\ln{{A(x)y - B(x)}\over{C(x)}}
$$


\endexample

\vfill\eject
\section{Se\~nor Gonzalez, otra vez}

The Rothstein-Trager resultant allows us to compute all the residues
at once.  Trager, in his Ph.D. thesis, then showed how to construct a
function that is zero at all poles with a given residue, and non-zero
at all other poles, as well as at all places conjugate to a pole.
