
\setcounter{chapter}{8}
\chapter{Simple Algebraic Extensions}

\input{09a-ALGEBRAIC-THEORY.tex}

\section{Integral Modules}

In ${\bf C}(x)$, we were working with the quotient field of a
principal ideal ring, so we could always find a single function to
generate any ideal / ${\bf C}[x]$-module.

In ${\bf C}(x,y)$, we are no longer working with a principal ideal
ring, so we can't guarantee that any particular ideal can be generated
by a single function, but it turns out that every ideal can be
generated by a {\it pair} of functions.  Our course of attack is first
to construct that pair of functions, then use them to determine if in
fact the ideal is principal.

DEFINITION

An {\it integral module} (or ${\cal I}$-module) is a module formed
over ${\cal I}$, the ring of integral elements in ${\bf C}(x,y)$.

Since ${\cal I}$ itself can be expressed as a ${\bf C}[x]$-module
using an integral basis, any ${\cal I}$-module is also a ${\bf
C}[x]$-module.  Not all ${\bf C}[x]$-modules are ${\cal I}$-modules,
however, since ${\cal I}$ is typically larger than ${\bf C}[x]$.

Some authors use the term {\it fractional ideal} to refer to an ${\cal
I}$-module.  I have avoided use of this term for two reasons.  First,
I wish to emphasize the concept of a module.  Second, ${\cal
I}$-modules are not ideals, either in the ring ${\cal I}$ (since they
may contain elements not in ${\cal I}$), nor in the field ${\bf
C}(x,y)$, since, as a field, ${\bf C}(x,y)$ has only the trivial
ideals.  The term {\it fractional ideal} is used because an ${\cal
I}$-module can be regarded as a fraction of ideals in ${\cal I}$.

THEOREM C

A function can always be constructed with a simple zero at a specified
place $(\alpha, \beta)$, $\alpha$ and $\beta$ both finite, zero order
at an additional finite set of places $\Sigma$, and non-negative order
at all other finite places.

PROOF

Begin with the function $(y-\beta)$, which has a simple zero at
$(\alpha, \beta)$ and non-negative order at all finite places.  If
none of the other places in $\Sigma$ have y-value $\beta$, then we are
done, since this function's zeros lie at exactly those places where $y
= \beta$.

Otherwise, compute $(y-\beta)\over(x-\alpha)$ at all places in $\Sigma$
that do {\it not} lie over $x = \alpha$.  Select a number
$\gamma$ different from all of these values.  The function
$(y-\beta) - \gamma (x-\alpha)$ has the desired properties, since
it has non-negative order at all finite places and zero order at
all places in $\Sigma$.

END THEOREM

THEOREM D

A function can always be constructed with a simple pole at a specified
place $(\alpha, \beta)$, $\alpha$ and $\beta$ both finite, zero order
at an additional finite set of places $\Sigma$, and non-negative order
at all other places.

PROOF

If $(\alpha, \beta)$ is non-singular, begin with the function:

$$f(\alpha,y)\over(x-\alpha)(y-\beta)$$

where $f(x,y)=0$ is the defining polynomial of the algebraic extension.
Note that the division by $(y-\beta)$ will always be exact, since
$f(\alpha, \beta)=0$.  We now have a rational function
$P(y)\over Q(x)$, where $P(y)$ is a polynomial in $y$ and $Q(x)$ is a
polynomial in $x$ ($x-\alpha$, to be precise).  It has a simple pole at
$(\alpha, \beta)$ and non-negative order at all other places, which
is obvious except for places over $x=\alpha$.

If $x=\alpha$ and $y\ne\beta$, $f$ takes the form $0\over0$, so we can
expand it using L'H\^opital's rule:

$$\lim_{(x,y)\to p} {{P(y)}\over{Q(x)}}
  = \lim_{{(x,y)\to p}} {{{dP(y)}\over{dx}}\over{{dQ(x)}\over{dx}}}
  = {{P'(y)}\over{Q'(x)}} \, {{dy}\over{dx}} $$

where $'$ denotes differentiation with respect to the polynomial's
variable.  Now, $Q(x)=(x-\alpha)$, so $Q'(x)=1$, and the denominator
is thus finite.  $P'(y)$ is a polynomial, and is thus also finite.
$(\alpha, \beta)$ is a finite place, so ${dy}\over{dx}$ is also
finite.  Since all the numerator terms are finite and the denominator
is non-zero, it follows that $f$ is at least finite, and thus has
non-negative order, at places over $x=\alpha$ other than $(\alpha,
\beta)$.

If $x=\alpha$ and $y=\beta$, then $f$'s numerator is non-zero and its
denominator is zero, so this is clearly a pole, but of what order?

Now, compute the value of the function at all other places in
$\Sigma$, using either L'H\^opital's rule or Puiseux expansion if some
of these are other places over $\alpha$.  If the value of the function
is non-zero at all of these places, then we are done.  Otherwise,
select a number $\gamma$ different from all of these values.  The
function:

$${f(\alpha,y)\over(x-\alpha)(y-\beta)} - \gamma$$

has the desired properties, since it still has a simple pole at
$(\alpha,\beta)$, and is now non-zero at all places in $\Sigma$.

SINGULARITIES?

END THEOREM

THEOREM E

A function can always be constructed with specified integer orders at
a finite set of places $\Sigma$ and non-negative order at all other finite
places.

PROOF

For each pole or zero, use Theorems C or D to construct a function
with a simple pole or a simple zero at that place, zero order at all
other places in $\Sigma$ and non-negative order at all other finite
places.  Raise each of these function to the integer power that is the
order of the corresponding pole or zero, then multiply them all
together.

END THEOREM

THEOREM A

There is a one-to-one relationship between integral modules and
divisors; divisors and integral modules are said to be {\it
associated}.  An integral module consists of all multiples except at
infinity of its associated divisor.

PROOF

Given an integral module, construct its associated divisor by taking
at every place the minimum of the orders of the module's generators at
that place.  This is a Notherian ring, so there is always a finite set
of generators, and each function has only a finite number of zeros and
poles.  Since integral elements have non-negative order at all places
other than infinity, multiplying a generator by an integral element
can only raise its orders at finite places.  Likewise, adding two
elements produces orders at each place greater than or equal to the
minimum of the orders of the summands.  Since all elements of an
integral module have the form:

	$$\sum a_i g_i, \qquad a_i \in {\cal I}, g_i \in {\bf C}(x,y)$$

everything constructed from the module's generators must be a
multiple of the associated divisor except at infinity.

Conversely, I will show that the set of all multiples of a given
divisor $\cal D$ form an integral module.  Use the preceding theorems
to construct a function $f$ with poles and zeros exactly as required
by $\cal D$.  Since ${\bf C}(x,y)$ is a field, the inverse of $f$ exists,
and any multiple $m$ of $\cal D$, when multiplied by $f^{-1}$, will be
integral, and can thus be expanded using an integral basis:

	$$mf^{-1} = \sum a_i b_i, \qquad a_i \in {\bf C}[x], b_i \in {\cal I}$$

Multiplying the integral basis $b_i$ though by $f$ produces a ${\bf
C}[x]$-module, which clearly contains all multiples $m$ of $\cal D$:

	$$m = \sum a_i (b_i f), \qquad a_i \in {\bf C}[x], b_i \in {\bf C}(x,y)$$

This ${\bf C}[x]$-module basis can now be used as the basis for an
${\cal I}$-module, which clearly contains the original ${\bf
C}[x]$-module, and thus all multiples of ${\cal D}$.  Now, since the
integral basis $b_i$ has non-negative order at all finite places, $b_i
f$ can have order no lower than that specified by ${\cal D}$, thus by
the first part of this theorem contains nothing but multiples of
${\cal D}$.  It is thus the integral module desired.

END THEOREM

Theorem A gives a constructive proof for forming an integral module
basis for a given divisor, but we can tighten the result and show that
only two generators are required, no matter how many generators form
the integral basis.

THEOREM B

For any integral module, the order of its associated divisor at every
place is the minimum of the orders of the module's generators at that
place.

END THEOREM

THEOREM F

Given a divisor ${\cal D}$, a pair of functions can always be
constructed that generate the divisor's associated integral module.

PROOF

Use Theorem E to construct a function $f$ with the divisor's required
poles and zeros, zero order at all other places conjugate to those
poles and zeros, and non-negative order elsewhere.  Construct $g$, a
polynomial in $x$ with n-th order roots at all points under n-th order
zeros.  $(f,g)$ is the required basis.  The only finite poles are
those of $f$ and $g$ has zero order everywhere except at $f$'s zeros
and their conjugates, so by Theorem B, $(f,g)$ forms a basis for
${\cal D}$'s associated ${\cal I}$-module.

END THEOREM

Once we have constructed a basis for the integral module, we need to
determine if the module is principal.  Since the total order of a
field element is always zero, this only makes sense for divisors of
zero order.  Futhermore, since an integral module corresponds to
multiples of a divisor {\it except at infinity}, we can't use this
technique to find functions with poles or zeros at infinity, but that
isn't a serious limitation since we can just transform into a field
with an ordinary point at infinity.

A zero order ${\cal I}$-module is principle iff it contains a function
with no poles at infinity.  We can determine this by expressing the
${\cal I}$-module as a ${\bf C}[x]$-module, simply by multiplying the
${\cal I}$-module basis through by an integral basis (remember that an
integral basis is simply a basis for ${\cal I}$ as a ${\bf
C}[x]$-module).

We then check these basis elements to see if one of them has no poles
at infinity.  The most straightforward way to do this is to invert the
field using $z={1\over x}$, which swaps zero with infinity.  We can
then construct an integral basis for the inverse field, and express
each of the module's basis elements (after inverting them) using this
inverse basis.  If there are any poles at infinity in the original
field, they will appear as poles at zero in the inverse field, and can
easily be detected by checking if $z=0$ is a zero of the denominators.

\example Compute $\int {1\over{\sqrt{1-x^2}}} \,dx$

The obvious attempt is to use the algebraic extension $y^2=1-x^2$ and
integrate ${1\over y}\,dx$.

But we first need to determine if this differential has any poles at
infinity, by inverting the field and looking for poles at zero.
Setting $z={1\over x}$, we convert our minimal polynomial into
$z^2y^2=z^2-1$ (after multiplying through by $z^2$), and using
$\hat{y}=zy$ we obtain our inverse field ${\bf C}(z,\hat{y}); \hat{y}^2=z^2-1$.

Since $x={1\over z}$ and $y={\hat{y}\over z}$, we convert our differential as follows:

 $${1\over y}\,dx ={z\over\hat{y}} (-{1\over{z^2}} dz) = -{1\over{z\hat{y}}} dz$$

Now, $\{1, \hat{y}\}$ is an integral basis for the inverse field, so we
multiply through by $\hat{y}\over\hat{y}$ to obtain:

 $${1\over y}\,dx = -{\hat{y}\over{z\hat{y}^2}} dz = -{1\over{z(z^2-1)}}\hat{y} \, dz $$

which is now in normal form and clearly has a pole at $z=0$, or $x=\infty$.  Note that

 $${1\over y} = {z\over\hat{y}} = {{z\hat{y}}\over{\hat{y}^2}}
 = {z\over{z^2-1}} \hat{y}$$

has no pole at $z=0$, a clear example of a differential having a pole
at a place where its constituent function has none.

In any event, we clearly can not use the original field to conduct the
integration, since it would require constructing a function with a
pole at infinity, and our algorithm can't handle this.  So we need to
transform into a field where the differential has no pole at infinity.

We've actually already done this!  Note that the integrand had no pole
at zero in the original field:

 $${1\over y}\,dx = {y\over y^2}\,dx = {1\over{1-x^2}}y \,dx $$

Since the inverse field swapped zero with infinity, it follows that
there is no pole at infinity in the inverse field, so we can proceed
to integrate $-{1\over{z(z^2-1)}}\hat{y} \,dz$ in ${\bf C}(z,\hat{y})$;
$\hat{y}^2=z^2-1$.

Simple inspection of the integrand (already in normal form) shows that
its poles are at $(0, i)$, $(0, -i)$, $(1, 0)$, and $(-1, 0)$.
Remember that we're now working on the Riemann surface of an algebraic
extension, so we need to specify $\it both$ $z$ and $\hat{y}$ to
specify a place.

The next step is to compute the residues at each of these places,
using the Theorem on p. 56 of Trager's thesis:

\begin{center}
\begin{supertabular}{l l l}
  $(0, i)$  &  $\displaystyle -{1\over{(z^2-1)}}\hat{y}$ @ $(0, i)$     & = $i$    \cr
  $(0, -i)$  &  $\displaystyle -{1\over{(z^2-1)}}\hat{y}$ @ $(0, -i)$   & = $-i$    \cr
  $(1, 0)$  &  $\displaystyle -2{1\over{z(z+1)}}\hat{y}$ @ $(1, 0)$      & = $0$    \cr
  $(-1, 0)$  &  $\displaystyle -2{1\over{z(z-1)}}\hat{y}$ @ $(-1, 0)$    & = $0$    \cr
\end{supertabular}
\end{center}

The poles with zero residues can be ignored.  We're interested in the
other two, which exist in ${\bf Q}[i]$, which can be regarded as a
vector field over ${\bf Q}$ with basis $\{1, i\}$, and we want to
construct a function whose poles and zeros match the $i$-component of
the residues (the 1-component is uniformly zero).

We start by constructing an ${\cal I}$-module basis for the divisor
with a simple zero at $(0,i)$ and a simple pole at $(0,-i)$.  Theorem
? shows that:

$$f = {{\hat{y}^2+1}\over{z(\hat{y}+i)}} = {{\hat{y}-i}\over{z}} $$

has a simple pole at $(0,-i)$.  At $(0,i)$, L'H\^opital's rule gives:

$$ \lim_{(z,\hat{y})\to (0,i)} {{\hat{y}-i}\over{z}}
   = {{(\hat{y}-i)'}\over{z'}} {{d\hat{y}}\over{dz}} = {{d\hat{y}}\over{dz}} = {z\over\hat{y}} = 0 $$

where the last transformation was accomplished by differentiating the
mimimal polynomial.  So $f$ has a zero at $(0,i)$, and I'll
note that we've just stumbled into the solution.  Theorem ? already
assures us that $f$ has only a single finite simple pole,
and we can see that its only zeros occur when $\hat{y}-i=0$, which,
according to the minimum polynomial, can only occur at $z=0$, thus
$(0,i)$ is its only finite zero, and it is simple, as we can
verify by showing that the corresponding pole in its inverse is simple:

$$ {1\over f} = {z\over{\hat{y}-i}} = {{z(\hat{y}+i)}\over{\hat{y}^2+1}}
  = {{z(\hat{y}+i)}\over{z^2}} = {1\over z}\hat{y} + {i\over z} $$


So we've found the function we're looking for by accident.  Let's save the
general case for the next example, and convert back to
our original field:

$${{\hat{y}-i}\over{z}} = x({y\over x}-i) = y - ix $$

Remembering that our residues came multiplied by a factor of $i$, we
conclude that our solution is $i\,\ln(y-ix)$, or:

\begin{eqnarray*}
\int {1\over{\sqrt{1-x^2}}} \,dx &=& i\,\ln(\sqrt{1-x^2}-ix) \\
                                 &=& -i\,\ln({1\over{\sqrt{1-x^2}-ix}}) \\
                                 &=& -i\,\ln({{\sqrt{1-x^2}+ix}\over{1-x^2+x^2}}) \\
                                 &=& -i\,\ln({\sqrt{1-x^2}+ix}) \\
                                 &=& \arcsin x \\
\end{eqnarray*}

where I used the negative of a logarithm being the logarithm of
the inverse, and the last transformation came from section ?.


\endexample


\section{Se\~nor Gonzalez, otra vez}

The Rothstein-Trager resultant allows us to compute all the residues
at once.  Trager, in his Ph.D. thesis, then showed how to construct a
function that is zero at all poles with a given residue, and non-zero
at all other poles, as well as at all places conjugate to a pole.
