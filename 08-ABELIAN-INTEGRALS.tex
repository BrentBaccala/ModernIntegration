
\mychapter{Abelian Integrals}

{\bf THIS CHAPTER IS INCOMPLETE.}

We can now use the machinery developed in the previous chapter to
solve Abelian integrals in general.

%\theorem
%Differentials of rational functions on an algebraic curve have no
%simple (i.e, first order) poles.
%\endtheorem

%\theorem
%Logarithmic differentials on an algebraic curve have only simple
%poles.
%\endtheorem

These techniques, taken together with Liouville's Theorem, provide
our method of attack for integration of Abelian integrals.  We find
all of the poles of the differential, construct Puiseux expansions of
the differential there, and split the principal parts easily into two
sets.  The first order poles arise from logarithm components in the
solution; all the higher order poles must come from the rational
function.  To find the rational function, we integrate term-wise to
obtain its principal parts, and then solve a Mittag-Leffler problem to
see if such a function exists.  For the logarithmic components,
it's a little more complicated.

Once we've calculated the principal parts of an integrand, we can
integrate the resulting series to obtain the principal parts of the
integral.  This is possible due to the simple but crucial observation
that poles in the integral can only appear where there are poles in
the integrand.  Once we've determined the principal parts of the
integral, we need to solve a Mittag-Leffler problem to find an
algebraic function that matches the given principal parts.  This can
be done by finding a basis for a suitable Riemann-Roch space.

Thus, having identified the
locations and orders of the integral's principal parts, we can compute
a Riemann-Roch basis for all functions with suitable poles at those
locations.  Having done so, it then becomes a straightforward exercise
in linear algebra to find a combination of those basis functions
that match a specified set of principal parts.

%\theorem
%Finite poles are always located over a zero of the denominator.
%\endtheorem

\begin{comment}
The techniques of the previous chapter suffice to compute the {\it
algebraic} portion of an Abelian integral, which is to say, an
algebraic function.  Liouville's theorem, however, tells us that there
can also be logarithmic components in the integral.  The two can be
easily separated, since the rational portion of the integral
corresponds to poles of second order poles and higher in the integrand,
while the logarithmic portion corresponds to first order poles
in the integrand.
\end{comment}

\vfill\eject
\mysection{The Abelian Integration Theorem}

\theorem\label{abelian integration theorem}
Let $\CC$ be the complex field, let
$\CC(x)$ be the rational function field in $x$ over $\CC$,
let $\CC(x,y)$ be an algebraic extension of $\CC(x)$,
and let $f$ be an element of $\CC(x,y)$
with pole divisor $(f)_\infty$ and principle parts expansion:

$$\forall P \in (f)_\infty\qquad f = \sum_{\nu_P<i<0} a_{P,i}\ t^i + \cdots$$

If $f$ has
an elementary anti-derivative $F$, then $F \in \CC(x, y, \Psi)$,
where $\CC(x, y, \Psi)$ is a finite logarithm extension
of $\CC(x,y)$, $F$ can be written as the sum of an algebraic
function in $A \in \CC(x,y)$ and logarithms of algebraic functions $B_j \in \CC(x,y)$:

$$F = A + \sum_j c_j \ln B_j$$

$A$'s pole divisor is a subset of $f$'s pole divisor, and its principle parts expansion has the form:

$$\forall P \in (f)_\infty\qquad A = \sum_{(\nu_P+1)<i<0} A_{P,i}\ t^i + \cdots$$

$$A_{P,i} = \frac{1}{i} a_{P,i-1}$$

(what about ramification and infinity?)

The residues $a_{P,-1}$ can be written in terms of a $\ZZ$-module in $\CC$, with basis $\{c_j\}$:

$$a_{P,-1} = \sum_j b_{P,j} c_j \qquad b_{P,j} \in \ZZ$$

and the divisors of the $B_j$ can be written:

$$(B_j) = \sum_P b_{P,j} P$$

(and any such basis is as good as any other)

%\vfill\eject
%\proof

\endtheorem

\vfill\eject
\mysection{Integration Examples}

\example Evaluate $\int \frac{x}{\sqrt{x^2+1}}\,dx$

This is a simple integral that can be easily solved using first year
calculus techniques, but let's see how to attack it using the more
sophisticated techniques of this chapter.

First, we convert the the integrand into
a rational function on an algebraic curve:

\begin{sageblock}[riemannroch2]
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 - (x^2+1))
\end{sageblock}

Next, we identify the poles of the integrand.  The finite
poles can only be located where the denominator is zero,
which is where $x^2+1=0$, over the points $x=\pm i$.

Let's compute the principal parts of $\frac{x}{y}$
at $x=i$.  Calling {\tt puiseux} with {\tt deg=-1} computes just
the principal part of the differential:

\begin{sageblock}[riemannroch2]
D = (x/y).divisor()
p = D.support()[0]
m = F.completion(p, prec=0)
m(x/y)
\end{sageblock}

There appears to be a pole here, but appearances are deceptive.
We need to expand the {\it differential}, not the {\it integrand}:

\begin{sageblock}[riemannroch2]
m(x/y * x.differential())
\end{sageblock}

So, even though the {\it integrand} has a pole at $(i,0)$, the {\it
differential} does not... and the differential is what matters!

The only other place we might have a pole is infinity.

\begin{sageblock}[riemannroch2]
D2 = (x/y * x.differential()).divisor()
F.maximal_order_infinite().decomposition()
\end{sageblock}

Now it appears that we have two sheets with no poles, the expansions indicating simply
that $\lim_{x\to\infty}\frac{x}{\sqrt{x^2+1}} = \pm 1$, depending
on whether we use the positive or negative square root,
but again
we have to take the differential into account.  Since $x=\frac{1}{t}$,
$dx=-\frac{1}{t^2} dt$, and we actually have second order poles
at infinity.

\begin{sageblock}[riemannroch2]
m2 = F.completion(D2.support()[0], prec=0)
m3 = F.completion(D2.support()[1], prec=0)
m2(x/y * x.differential())
m3(x/y * x.differential())
\end{sageblock}

Integrating termwise, we see that since our differential has second
order poles at infinity, our integral must have first order poles
at infinity, and theorem ? states that this completely
characterizes the integral.

%We already know of a rational function with first order poles
%at infinity -- $y$ itself!

What functions have first order poles at infinity and nowhere else?

\begin{sageblock}[riemannroch2]
D3 = add([-(m+1)*p for p,m in D2.list() if m < 0])
D3.basis_function_space()
\end{sageblock}

Our solution, if it exists, is in the vector space spanned by these
three basis elements.  $1$ is in the list, and we expect it to be
there, because of the presence of the constant of integration, so we
can always add a multiple of $1$ to our solution and get another
solution.

What about $x$ and $y$?  We want to
combine them in such a way as to match the principal parts of
our differential.

Let's expand them:

\begin{sageblock}[riemannroch2]
[m2(x), m3(x)]
[m2(y), m3(y)]
\end{sageblock}

No linear algebra games are required to see that $y$ will match the
principal parts of the differential if it is itself differentiated.
Therefore, $y=\sqrt{x^2+1}$ is our solution.

\begin{comment}
$$\sage{integrate(SR(x)/sqrt(x^2+1), SR(x), hold=True)} = \sage{integrate(SR(x)/sqrt(x^2+1), SR(x))}$$
\end{comment}

\endexample

\vfill\eject
\mysection{An integral Sage can't solve}

\example
\label{an integral Sage can't solve}
Integrate $\int \frac{x^9+2x^7-x}{(x^4+2x^2+1)\sqrt{x^8+1}}\, dx$

When I say that Sage can't solve this integral, I mean that its
built-in integration routine can't solve the integral:

\begin{sageblock}[riemannroch3-2]
y = sqrt(x^8+1);
integrand = (x^9+2*x^7-x)/((x^4+2*x^2+1)*y)
integrate(integrand,x)
\end{sageblock}

Now let's attack the problem using the techniques of this book.

\begin{sageblock}[riemannroch3]
QQbar.options.display_format = 'radical'
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 - (x^8+1))

integrand = (x^9+2*x^7-x)/((x^4+2*x^2+1)*y)

\end{sageblock}

Where are our poles?

\begin{sageblock}[riemannroch3]
D = (integrand * x.differential()).divisor_of_poles()
\end{sageblock}

We've found four second order poles at the ordinary points $(\pm i, \pm\sqrt{2})$,
as well as two third order poles at a singular point with two sheets at infinity.

Our next goal is to construct a basis for a suitable Riemann-Roch space.
We invert the signs of the finite poles,
since the convention for Riemann Roch spaces is their functions must
have order greater than the {\it negative} of a divisor, remember
that poles decrease in order by $1$ when they are integrated,
and conclude that the Riemann-Roch space that we're interested in is:

$${\cal L}(Z(i, \sqrt{2}) Z(-i, \sqrt{2}), Z(i, -\sqrt{2}) Z(-i, -\sqrt{2}) Z^2(\infty,\infty))$$

i.e, all the functions on our algebraic curve with at most first order
poles at $(\pm i,\pm\sqrt{2})$, no other finite poles, and at most
second order poles at infinity.

\begin{sageblock}[riemannroch3]
from sage.rings.function_field.divisor import FunctionFieldDivisor
D = FunctionFieldDivisor(D.parent().function_field(), {p:m-1 for p,m in D.list()})
\end{sageblock}

We now wish to see if a linear combination of these basis functions will
match the poles in the differential.  This problem is a bit more complicated
than the last one, so let's use the tools we developed in the
last chapter for solving Mittag-Leffler problems:

\begin{sageblock}[riemannroch3]
basis = D.basis_function_space()
\end{sageblock}

\begin{sageblock}[riemannroch3]
def principal_parts_matrix(div, basis):
    F = div.parent().function_field()
    coeffs = [(F.completion(p, prec=0), i) for p,m in div.list() for i in range(-m,0)]
    return matrix([[c[0](b)[c[1]] for c in coeffs] for b in basis]).transpose()
\end{sageblock}

\begin{sageblock}[riemannroch3]
m = principal_parts_matrix(D, basis)
\end{sageblock}

Another Sage function will extract the principal parts of the
differential, and divide them by the powers needed for term-wise
integration:

\begin{sageblock}[riemannroch3]
def solution_vector(div, differential):
    F = differential.parent().function_field()
    coeffs = [(F.completion(p, prec=0), i) for p,m in div.list() for i in range(-m,0)]
    return matrix([[c[0](differential)[c[1]-1]/c[1] for c in coeffs]]).transpose()
\end{sageblock}

\begin{sageblock}[riemannroch3]
b = solution_vector(D, integrand * x.differential())
\end{sageblock}

Now we have a matrix equation that we want to solve:

$$\sage[riemannroch3]{m} \cdot v = \sage[riemannroch3]{b}$$

The equation has a non-trivial null space, which we expect because
of the presence of the constant of integration.

\begin{sageblock}[riemannroch3]
m.kernel()
\end{sageblock}

Now let's compute the pseudoinverse:

\begin{sageblock}[riemannroch3]
pi = m.pseudoinverse()
\end{sageblock}

To find out if there actually is a solution, we simply have to check:

\begin{sageblock}[riemannroch3]
v = pi * b
m * v == b
\end{sageblock}

So, yes, this system does have a solution.  Now let's multiply our
solution vector by the original basis:

\begin{sageblock}[riemannroch3]
solution = (matrix(basis) * v)[0,0]
\end{sageblock}

...and convert back to our original form:

\begin{sageblock}[riemannroch3]
var('x')
F = FractionField(PolynomialRing(QQbar, x))
solution = solution.element().change_ring(F).subs({y:sqrt(x^8+1)})
\end{sageblock}

Now we can verify the solution:

\begin{sageblock}[riemannroch3]
integrand = integrand.element().change_ring(F).subs({y:sqrt(x^8+1)})
bool(solution.diff(x) == integrand)
\end{sageblock}

% https://ocw.mit.edu/courses/mathematics/18-782-introduction-to-arithmetic-geometry-fall-2013/lecture-notes/MIT18_782F13_lec21.pdf
% Theorem 21.9. For any divisor D we have dim L(D) ≤ deg D0 + 1.

% or use Riemann-Roch theorem directly

\endexample


\vfill\eject
\mysection{Examples}

\example Compute $\int {1\over{\sqrt{1-x^2}}} \,dx$

This is a familiar example from first year calculus, but let's
approach it using the techniques of this book.  We'll
use the algebraic extension $y^2=1-x^2$ and integrate ${1\over
y}\,dx$.

\begin{sageblock}[arcsin]
QQbar.options.display_format = 'radical'
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 - (1-x^2))

integrand = 1/y * x.differential()

D = integrand.divisor_of_poles()

[[pl, F.completion(pl)(integrand)] for pl in D.support()]
\end{sageblock}

Our only poles are at infinity, and they're first order poles,
so this solution will be a logarithm.

The poles' residues (coefficients) are $-i$ and $i$.  These
exist in the field ${\bf Q}[i]$, which can be regarded as a vector
field over ${\bf Q}$ with basis $\{1, i\}$, and we want to construct a
function whose poles and zeros match the $i$-component of the residues
(the 1-component is uniformly zero), with the signs flipped due to
the effect of differentiation of a negative power.

So, we have a singular point at infinity, and we want a function with
a simple zero on one cycle and a simple pole on the other.
The basis will have either one element or no elements, depending
on whether an algebraic function exists with the desired properties.

\begin{sageblock}[arcsin]
def logarithmic_divisor(differential, component):
    return add([- ZZ(differential.residue(pl) / component) * pl.divisor() for pl in differential.divisor_of_poles().support()])
\end{sageblock}

\begin{sageblock}[arcsin]
D = logarithmic_divisor(integrand, QQbar(-1).sqrt())
D.basis_function_space()
\end{sageblock}

Yes, it does exist.
Remembering that our residues came multiplied by a factor of $i$, we
conclude that our solution is $i\,\ln(y-ix)$, or:

\begin{eqnarray*}
\int {1\over{\sqrt{1-x^2}}} \,dx &=& i\,\ln(\sqrt{1-x^2}-ix) \\
                                 &=& -i\,\ln({1\over{\sqrt{1-x^2}-ix}}) \\
                                 &=& -i\,\ln({{\sqrt{1-x^2}+ix}\over{1-x^2+x^2}}) \\
                                 &=& -i\,\ln({\sqrt{1-x^2}+ix}) \\
                                 &=& \arcsin x \\
\end{eqnarray*}

where I used the negative of a logarithm being the logarithm of the
inverse, and the last transformation came from section
\ref{sec:Liouvillian Forms}.


\endexample

\example Compute $\int \sqrt{4-x^2} \,dx$

A solution method from first year calculus might be to note that this
integrand forms one leg of a right triangle with its other sides $2$
and $x$, but we'll attack this integral using the methods of this
chapter.

First, transform the problem into an algebraic curve:

\begin{sageblock}[ex8.7]
QQbar.options.display_format = 'radical'
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 - (4-x^2))
\end{sageblock}

With no denominator, there can be no poles at finite points, so we
just need to check infinity:

\begin{sageblock}[ex8.7]
integrand = y * x.differential()

D = integrand.divisor_of_poles()

[[pl, F.completion(pl)(integrand)] for pl in D.support()]
\end{sageblock}

Since these polar expansions have components at both $t^{-3}$ and
$t^{-1}$, we'll get a result with both algebraic and logarithmic
components.

Let's start with the algebraic part.  We need a Riemann-Roch
space with at most second order poles at infinity:

\begin{sageblock}[ex8.7]
from sage.rings.function_field.divisor import FunctionFieldDivisor
def reduced_divisor(D):
    return FunctionFieldDivisor(D.parent().function_field(),
                                {p:m-1 for p,m in D.list()})
D2 = reduced_divisor(D)
basis = D2.basis_function_space()
\end{sageblock}

Now let's construct a matrix of principal parts
and its pseudoinverse:

\begin{sageblock}[ex8.7]
def principal_parts_matrix(div, basis):
    F = div.parent().function_field()
    coeffs = [(F.completion(p, prec=0), i) for p,m in div.list() for i in range(-m,0)]
    return matrix([[c[0](b)[c[1]] for c in coeffs] for b in basis]).transpose()
\end{sageblock}

\begin{sageblock}[ex8.7]
m = principal_parts_matrix(D2, basis)
m.kernel()
pi = m.pseudoinverse()
\end{sageblock}

\begin{sageblock}[ex8.7]
def solution_vector(div, differential):
    F = differential.parent().function_field()
    coeffs = [(F.completion(p, prec=0), i) for p,m in div.list() for i in range(-m,0)]
    return matrix([[c[0](differential)[c[1]-1]/c[1] for c in coeffs]]).transpose()
\end{sageblock}

\begin{sageblock}[ex8.7]
b = solution_vector(D2, integrand)
\end{sageblock}

\begin{sageblock}[ex8.7]
v = pi * b
m * v == b
algebraic_solution = (matrix(basis) * v)[0,0]
\end{sageblock}

This is the rational part of our answer.  To find the logarithmic
part, let's return to the series expansion of the differential,
note that the residues are $-2i$ and $2i$, which can be placed
in the $Z$-module $\{2i\}$, and search for:

\begin{sageblock}[ex8.7]
pls = D.support()
basis2 = (pls[0].divisor() - pls[1].divisor()).basis_function_space()
log_solution = basis2[0]
\end{sageblock}

The logarithmic component of our solution is thus:

\begin{sageblock}[ex8.7]
var('x')
F = FractionField(PolynomialRing(QQbar, x))
log_solution.element().change_ring(F).subs({y:sqrt(4-x^2)})
\end{sageblock}

Remembering from the previous example that $i \ln(ix-\sqrt{1-x^2}) = \arcsin x$,
and that logarithmic components are only specified up
to a multiplicative constant (it disappears into the constant
of integration), we can rewrite this:

$$2i \ln\left(\sqrt{4-x^2} - ix\right) = 2i \ln\left(\sqrt{4-4\left(\frac{x}{2}\right)^2} - 2i\left(\frac{x}{2}\right)\right)$$
$$= 2i \ln\left(i\frac{x}{2} - \sqrt{1-\left(\frac{x}{2}\right)^2}\right) = 2 \arcsin \frac{x}{2}$$

Adding in the rational component we computed earlier, the final answer is:

$$ \int \sqrt{4-x^2} \, dx  = 2\arcsin\frac{x}{2} + \frac{x \sqrt{4-x^2}}{2}$$

\endexample

\vfill\eject
\mysection{Geddes's example}

\example Compute $\int {1\over{x\sqrt{x^4+1}}} \, dx$

Two traditional solution techniques are either the substitution $x^2 = \tan u$,
followed by the half angle formula for $\tan$, or $x^4+1=u^2$, which
leads to a rational function and a partial fractions expansion.

We'll use ${\bf C}(x,y); y^2=x^4+1$ and integrate ${1\over{xy}}$.

\begin{sageblock}[geddes]
QQbar.options.display_format = 'radical'
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 - (x^4+1))
integrand = 1/(x*y) * x.differential()
D = integrand.divisor_of_poles()
[[pl, F.completion(pl)(integrand)] for pl in D.support()]
\end{sageblock}

Our only poles are a pair of first order poles at $x=0$.  This will give rise to
a logarithmic term; can we find a function that matches?

\begin{sageblock}[geddes]
pls = D.support()
D2 = (pls[0].divisor() - pls[1].divisor())
D2.basis_function_space()
\end{sageblock}

Since we can't find a function that matches that divisor; let's try
doubling the strength of the poles.

\begin{sageblock}[geddes]
log_solution = (2*D2).basis_function_space()[0]
\end{sageblock}

This is the function we are looking for inside our logarithm.

\begin{sageblock}[geddes]
var('x')
F = FractionField(PolynomialRing(QQbar, x))
1/2*log(log_solution.element().change_ring(F).subs({y:sqrt(x^4+1)}))
\end{sageblock}

\begin{comment}
$$\int {1\over{x\sqrt{x^4+1}}} \, dx
   = {1\over2} \ln{{\sqrt{x^4+1}-1}\over{x^2}}$$
\end{comment}

Sage's built-in integrator produces the result in a different form:

\begin{sageblock}[geddes]
integrate(1/(x*sqrt(x^4+1)),x)
\end{sageblock}

We can obtain this result from our algorithm by factoring $\frac{1}{4}$ out
of the residues and using fourth-order poles:

\begin{sageblock}[geddes]
(4*D2).basis_function_space()
\end{sageblock}

which is another way of writing $\frac{y-1}{y+1}$ (remember
that $y^2 = x^4+1$).

We conclude that:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx
={1\over2} \ln {{\sqrt{x^4+1}-1}\over{x^2}}
={1\over4} \ln {{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}}$$

\endexample

%%% THIS EXAMPLE IS TOO COMPLICATED FOR THE CURRENT SIMPLE ALGORITHM
\begin{comment}
\vfill\eject
\mysection{Chebyshev's Integral}

\example Compute:
\label{Chebyshev's Integral}
$$\int {{2x^6+4x^5+7x^4-3x^3-x^2-8x-8}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \,{\rm d}x$$

The polynomial under the square root is square-free:

\begin{maximablock}
num : 2*x^6 + 4*x^5 + 7*x^4-3*x^3-x^2-8*x-8$
den : (2*x^2-1)^2$
root : x^4+4*x^3+2*x^2+1$

f : y^2 - root$
integral : num/(den*y) * del(x)$
\end{maximablock}

\begin{maximablock}
solve(root);
solve(den);
\end{maximablock}

\begin{maximablock}
for r in append(solve(root), solve(den), [x=inf]) do
  disp(puiseux(f,x,y,rhs(r),false,-1,integral))$
\end{maximablock}

We have second order poles at $x=\pm\frac{1}{\sqrt{2}}$ and infinity.
There are also first order components, which indicates that we
will have both algebraic and logarithmic components.

Let's extract the disivors at which there are poles:

\begin{maximablock}
extract_divisor(x0) :=
  apply(append,
    map(lambda([exp], if is(exp[1] # 0) then [[[if rhs(exp[2]) # 1/x then x-rhs(exp[2]) else inf, rhs(exp[3])], -2]] else []),
        puiseux(f,x,y,rhs(x0),false,-1,integral)))
$
div : unique(apply(append, map(extract_divisor, append(solve(root), solve(den), [x=inf]))));
\end{maximablock}

Our basis, principal parts matrix and its pseudoinverse:

\begin{maximablock}
/* This runs forever */
basis : riemannroch(f,x,y,div);

m : principal_parts_matrix(f,x,y, basis, div);

pi : pseudoinverse(m);
\end{maximablock}

\endexample
\end{comment}

\vfill\eject
\mysection{Chebyshev's Integral}


\example
\label{Chebyshev's Integral I}

\begin{sagecode}[chebyshev1]
root = x^4+4*x^3+2*x^2+1;
num = 6*x^2 + 5*x +7;
den = 2*x^6 + 8*x^5 + 3*x^4 + - 4*x^3 - 1;
\end{sagecode}

Compute: $$ \sage[chebyshev1]{integrate(sqrt(root)*num/den, x, hold=True)}$$

\begin{sageblock}[chebyshev]
QQbar.options.display_format = 'radical'
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
root = x^4+4*x^3+2*x^2+1
F.<y> = R.extension(y^2 - root)
#num = 2*x^6 + 4*x^5 + 7*x^4-3*x^3-x^2-8*x-8
#den = (2*x^2-1)^2
#integrand = num/(den*y) * x.differential();
num = 6*x^2 + 5*x +7
den = 2*x^6 + 8*x^5 + 3*x^4 + - 4*x^3 - 1
integrand = y*num/den * x.differential();
D = integrand.divisor_of_poles()
[[pl, F.completion(pl, prec=0)(integrand)] for pl in D.support()]

\end{sageblock}

We have poles at $x=\pm \frac{1}{\sqrt{2}}$
and nowhere else.

Now we can see that all of our residues are $\pm\frac{5}{2}$, so we need only
a single logarithmic term.  It must have two poles and two zeros, at the
coordinates we just calculated, and the obvious choice is for each of
these poles and zero to have degree one.  Let's use our Riemann-Roch
basis space algorithm to see if such a function exists:

\begin{sageblock}[chebyshev]
D2 = add([QQ(integrand.residue(pl)).sign() * pl for pl in D.support()])
D2.basis_function_space()
\end{sageblock}

No such function exists.  However, a function may exist with
higher degree poles and zero.  Let's see... how about a
function with second degree poles and zeros?  Third degree?

\begin{sageblock}[chebyshev]
(2*D2).basis_function_space()
(3*D2).basis_function_space()
(4*D2).basis_function_space()
res = (5*D2).basis_function_space()
\end{sageblock}

Turns out that fifth degree poles and zeros give us our
solution.  Leaving aside for a moment the question of
finding this degree in some other way than multiplying
the degree of the divisor by every larger integers
(and how would we ever know when to stop?), let's
continue with the problem by coaxing our result
into a more reasonable form:

\begin{sageblock}[chebyshev]
R2.<x> = ZZ[]
F = Frac(R2)
R3.<y> = F[]
f = y^2 - (x^4 + 4*x^3 + 2*x^2 + 1)
Q = R3.quo(ideal(f))

num2 = R._to_bivariate_polynomial(res[0])[0]
den2 = R._to_bivariate_polynomial(res[0])[1]

gn = gcd([QQ(c).numerator() for c in num2.coefficients()]);
gd = gcd([QQ(c).denominator() for c in num2.coefficients()]);

ld = lcm([QQ(c).denominator() for c in den2.coefficients()]);

num3 = num2/gn*gd

ln = lcm([QQ(c).denominator() for c in num3.coefficients()]);

# why not Q(num3*ln) / Q(den2*ld)
Q(num3*ln) / R2(den2*ld)
\end{sageblock}

%% This attempt with Sage and Macaulay 2 came to naught because
%% Macaulay 2 can't do fraction fields of polynomial rings over
%% anything but Z, Q, or GF(p).  We need to extend Q into an
%% algebraic number field, then form our coefficient ring over it,
%% then form our function field.

\begin{comment}
\begin{sageblock}
# (x,y) = polygen(QQ,('x', 'y'))
y = var('y')
num = 7 + 5*x + 6*x^2
root = x^4 + 4*x^3 + 2*x^2 + 1
den = 2*x^6 + 8*x^5 + 3*x^4 - 4*x^3 - 1
f = y^2 - root
den.roots()

Const = macaulay2('Const = QQ[sqrt2, gamma] / (sqrt2^2 - 2, gamma^2 - 9 - sqrt2^5)')
R = macaulay2('R = Const[x,y,z]')
x = macaulay2('x')
y = macaulay2('y')
z = macaulay2('z')


# I = macaulay2.ideal(fp)

macaulay2('num = 7 + 5*x + 6*x^2')
macaulay2('root = x^4 + 4*x^3 + 2*x^2 + 1')
macaulay2('den = 2*x^6 + 8*x^5 + 3*x^4 - 4*x^3 - 1')
macaulay2('f = y^2 - root')
macaulay2('fp = y^2*z^2 - (x^4 + 4*x^3*z + 2*x^2*z^2 + z^4)')

macaulay2('Coord = R / ideal(fp)')

macaulay2('MI = ideal(x-z*1/sqrt2, y+z*gamma/2)')
\end{sageblock}
\end{comment}

%%\vfill\eject
%%\bigskip

\begin{center}
Non-zero residues

\begin{supertabular}{r @{} l | r @{} l | r @{} l}
\multicolumn{2}{c|}{x} & \multicolumn{2}{c|}{y} & \multicolumn{2}{c}{residue} \cr
\hline
&$\sqrt{2}\over 2$ & &${1\over 2} + \sqrt{2}$ & &${5\over2}$ \cr
&$\sqrt{2}\over 2$ & $-$&${1\over 2} - \sqrt{2}$ & $-$&${5\over2}$ \cr
$-$&${\sqrt{2}\over 2}$ & &${1\over 2} - \sqrt{2}$ & &${5\over2}$ \cr
$-$&${\sqrt{2}\over 2}$ & $-$&${1\over 2} + \sqrt{2}$ & $-$&${5\over2}$ \cr
\end{supertabular}
\end{center}


$$A(x) = 1023x^8+4104x^7+5048x^6+2182x^5+805x^4+624x^3+10x^2+28x$$
$$B(x) = 1025x^{10} + 6138x^9 + 12307x^8 + 10188x^7 + 4503x^6 + 3134x^5 + 1598x^4 + 140x^3 + 176x^2 +2$$
$$C(x) = 32x^{10}-80x^8+80x^6-40x^4+10x^2-1$$

$$y = \sqrt{x^4+4 x^3+2 x^2+1}$$

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \,{\rm d}x
= {{(x+{1\over2})y}\over{2x^2-1}} + {1\over2}\ln{{A(x)y - B(x)}\over{C(x)}}
$$

\endexample


\vfill\eject
\mysection{The Risch Theorem: A First Look}

At this point, there is only one major missing piece in our
integration theory for Abelian integrals --- how do we limit the
multiples of a divisor to a testable set?  We've seen how to
repeatedly raise a divisor to higher and higher powers, but how do we
know when to stop?  At what point can we declare that a divisor has no
multiple that is principal?

We'll attack this problem the way Robert Risch discovered in 1970, by
mapping into a finite field, solving the corresponding problem there,
then lifting the result back to the original field.  The finite field
will be the integers modulo a prime.  Reducing modulo some primes
changes the genus of the algebraic curve, while other primes leave the
genus unchanged.  Those primes at which the genus of the curve remains
unchanged yield {\it good reduction}

The key theorems as stated by [Tr73] on page 67:

\theorem
The homomorphism between divisor class groups under good reduction
is an isomorphism when restricted to divisors whose orders are
relatively prime to the characteristic of the reduced function field.
\endtheorem

Let $p$ be the characteristic of the reduced field,

\theorem
if the divisor $D$ has order $p^k n$ where $\gcd(n,p)=1$, then
the reduction of $D$ must have order $p^j n$ for some $j\le k$.

\proof
Let the order of the reduction be $p^j m$.  Since reduction
is a group homomorphism, we must have $m|n$ and $j \le k$.
Since $D^{p^k}$ has order exactly $n$, its reduction
must have order exactly $n$.  But the order of its
reduction is a divisor of $m$ and thus $n|m$ and so
finally we have $n=m$.

\endtheorem


Since good reduction preserves the part of the divisor's order
relatively prime to the characteristic, by picking two different
primes we can completely reconstruct the divisor's order in
characteristic zero.

A property of algebraic curves over finite fields is that {\it all}
divisors of total degree zero have some multiple that is principal
(proof in Chapter 9 when we construct Jacobian variety).  Thus, to
carry out this program, we need to compute bases for Riemann-Roch
spaces in prime characteristic.  Then, we can keep raising a divisor
to higher and higher powers until we find its order.  Do this for two
different primes, each exhibiting good reduction, and then we can find
the order of the original divisor in characteristic zero.

To proof the above claims, I want to construct the Jacobian variety in
arbitrary characteristic, then show how Jacobians map under good
reduction.

\begin{comment}
x,y,z = PolynomialRing(QQ,3,'xyz').gens()
f = y^2*z^2 - (x^4+z^4)
X = Curve(f)
X.genus()

x,y,z = PolynomialRing(GF(5),3,'xyz').gens()
f = y^2*z^2 - (x^4+z^4)
X = Curve(f)
X.genus()
\end{comment}

%\begin{mdframed}[backgroundcolor=yellow!20]
%\printpythontex
%\end{mdframed}

\section{Simple Algebraic Extensions over Finite Fields}

Let's start with a simple but crucial observation:

\theorem

In an algebraic extension over a finite field, the evaluation field is
also finite.

\proof

Consider a finite field of constants ${\cal F}$, over which we'll
extend first into a rational function field ${\cal F}(x)$ and then add
an algebraic extension ${\cal F}(x,y)$, where $y$ satisfies some
minimial polynomial $f(x,y)=0$.  Start with the constant field, which
gives us a finite number of values for $x$.  Plugging each of these
values into the minimal polynomial gives a finite set of polynomials
$f(y_i)=0$.  By Theorem ?, we can extend ${\cal F}$ into a finite
extension field ${\cal F}[\gamma]$ where all the roots of the
polynomial exist.  Since there a only a finite number of polynomials,
we need at worst a finite set of extensions ${\cal
F}[\gamma_1,...,\gamma_k]$ to construct a field in which all the roots
of all the polynomials exist.  Using the Theorem of the Primitive
Element, we can collapse all of these into a single finite extension
field ${\cal F}[\phi]$.  Since all values of $x$ exist in ${\cal F}$,
and all values of $y$ exist in ${\cal F}[\phi]$, an evaluation
homomorphism carries any rational function in $x$ and $y$ into
${\cal F}[\phi]$.

\endtheorem

This theorem leads directly to the single more important difference
(to us) between divisors in an infinite field versus those in a finite
field.  {\it In a finite field, some multiple of every divisor is
principal.}  The reason is that the multiplicative group of the
evaluation field has finite order.  The simplest way to demonstrate
this is to construct theorems analogous to Theorems ? and ?:

\theorem

In an algebraic extension of a finite field with characteristic
greater than 2, a function can always be constructed with an $m^{\rm
th}$-order zero at a specified place $(\alpha, \beta)$ and zero order
at all other finite places, where $m$ is the multiplicative order of
the evaluation field.

\proof

The desired function is

$$(x-\alpha)^m + (y-\beta)^m$$.

Clearly, this function is zero at $(\alpha, \beta)$ and of $m^{\rm
th}$ order there (PROOF THIS).  At all other places one of the two
terms will be non-zero, and both exist in the evaluation field.  By
Theorem ?, any non-zero number raised to the multiplicative order of
its field is one.  Thus the value of this function will be either
$1+0$, $0+1$, or $1+1=2$, all finite and non-zero, and thus of zero
order.

\endtheorem

\theorem

In an algebraic extension of a finite field with characteristic
greater than 2, a function can always be constructed with an $m^{\rm
th}$-order pole at a specified place $(\alpha, \beta)$ and zero order
at all other finite places, where $m$ is the multiplicative order of
the evaluation field.

\proof

The desired function is

$${f(\alpha,y)^m\over(x-\alpha)^m(y-\beta)^m} + 1$$

where the division by $(y-\beta)^m$ is exact.
Clearly, this function has a pole at $(\alpha, \beta)$ and of $m^{\rm
th}$ order there (PROOF THIS).  CONSIDER OTHER PLACES OVER $\alpha$.
At all other places the denominator
term will be non-zero, and thus one, and the numerator will be
either zero or one (by Theorem ?)
Thus the value of this function at these places will be either
$0+1$ or $1+1=2$, both finite and non-zero, and thus of zero
order.

\endtheorem


\example

Show that some multiple of ${\mathrm Z}(1,1)$ is principal in
${\bf Z}_5(x,y); y^2=x$.

Let's first construct a multiplication table for ${\bf Z}_5$:

\begin{center}
\begin{tabular}{c|c c c c c}
  & 0 & 1 & 2 & 3 & 4 \cr
\hline
0 & 0 & 0 & 0 & 0 & 0 \cr
1 & 0 & 1 & 2 & 3 & 4 \cr
2 & 0 & 2 & 4 & 1 & 3 \cr
3 & 0 & 3 & 1 & 4 & 2 \cr
4 & 0 & 4 & 3 & 2 & 1 \cr
\end{tabular}
\end{center}

Now, let's list out the places on the Riemann surface for
${\bf Z}_5(x,y); y^2=x$.

\begin{center}
\begin{tabular}{c l}
$x$ & $(x,y)$ \cr
\hline
0 & (0,0) \cr
1 & (1,1) \quad (1,4) \cr
2 & $(2,\gamma) \quad (2,-\gamma); \quad \gamma^2 - 2 =0$ \cr
3 & $(3,\theta) \quad (3,-\theta); \quad \theta^2 - 3 =0$ \cr
4 & (4,2) \quad (4,3) \cr
\end{tabular}
\end{center}

It looks like we need ${\bf Z}_5[\gamma,\theta]$ to express these places.
It's simplest to collapse $\gamma$ and $\theta$ into a single algebraic
extension.  We could use the Theorem of the Primitive Element to
do this, but in this case just looking at the multiplication table
and noting that $3 = 2^3 = \gamma^6$ shows that $\theta = \pm \gamma^3$.
So, in fact, we only need ${\bf Z}_5[\gamma]$:

\begin{center}
\begin{tabular}{c l}
$x$ & $(x,y)$ \cr
\hline
0 & (0,0) \cr
1 & (1,1) \quad (1,4) \cr
2 & $(2,\gamma) \quad (2,-\gamma); \quad \gamma^2 - 2 =0$ \cr
3 & $(3,\gamma^3) \quad (3,-\gamma^3)$ \cr
4 & (4,2) \quad (4,3) \cr
\end{tabular}
\end{center}

Since ${\bf Z}_5[\gamma]$ has $5^2=25$ elements, its multiplicative
group has order one less than this.  We conclude that 24 is our
``magic'' multiple, and that ${\mathrm Z}^{24}(1,1)$ must be
principal in this field.  Its generator should be simply
$(x-1)^{24} + (y-1)^{24}$.  Clearly this function is zero for
$(x,y)=(1,1)$.  Let's verify that it's non-zero for some other
places on the Riemann surface:

\begin{eqnarray*}
(0,0) &:& (-1)^{24} + (-1)^{24} = 4^{24} + 4^{24} = 1+1 = 2 \cr
(1,4) &:& 3^{24} + 0^{24} = 1 + 0 = 1 \cr
(2,\gamma) &:& (\gamma-1)^{24} + (2-1)^{24} = 1+1 = 2 {\rm ,\quad since:} \cr
&&\cr
&& (\gamma-1)^2 = (\gamma^2-2\gamma+1) = 3-2\gamma \cr
&& (\gamma-1)^4 = (3-2\gamma)^2 = (9-12\gamma+4\gamma^2) = 2-2\gamma \cr
&& (\gamma-1)^8 = (2-2\gamma)^2 = (4-8\gamma+4\gamma^2) = 2-3\gamma \cr
&& (\gamma-1)^{12} = (2-2\gamma)(2-3\gamma) = (4-10\gamma+6\gamma^2) = 1 \cr
\end{eqnarray*}

In the final series of calculations, I used $\gamma^2=2$ and reduced
mod 5 repeatedly.  I think the pattern should be clear, and leave
further verification as an exercise.

\endexample


\vfill\eject
\mysection{Hermite reduction}

The previous sections in this chapter have laid out the theoretical
framework for the solution of Abelian integrals and presented the
simplest computational algorithm that I could formulate to acheive
that goal.  It is by no means the most efficient algorithm, and should
not be used as a basis for a professional implementation.  The
remaining sections of this chapter showcase optimization techniques,
and can be skipped without loss of continuity in the text.  In
particular, Puiseux expansions can be completely avoided, which offers
significant savings in computational complexity.

\cite{trager} shows how to extend Hermite reduction into the algebraic case.
This offers a means of calculating the rational parts of the integrals
without going through the calculations described in this chapter.

\example Compute:
\label{Chebyshev's Integral}
$$\int {{2x^6+4x^5+7x^4-3x^3-x^2-8x-8}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \,{\rm d}x$$

The polynomial under the square root is square-free:

\setpythontexautoprint{false}
\begin{sagecode}[chebyshev2]
load("sagecommon.sage");
\end{sagecode}
\setpythontexautoprint{true}

\begin{sageblock}[chebyshev2]
R.<x,y> = QQ[]
F=Frac(R)
root = x^4+4*x^3+2*x^2+1
num = 2*x^6 + 4*x^5 + 7*x^4-3*x^3-x^2-8*x-8
den = 2*x^2-1
root.factor()
\end{sageblock}

\ldots so $y^2 = x^4+4 x^3+2 x^2+1$; $\{1, y\}$ is an integral basis;
and our normal form for this integral is:

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)y}\over{(2x^2-1)^2(x^4+4 x^3+2 x^2+1)}} \,{\rm d}x$$

Applying now Bronstein's Hermite reduction from
section 2.1 of his ``Symbolic Integration Tutorial'' with $v=2x^2-1$
to eliminate this square in the denominator:

\begin{sageblock}[chebyshev2]
D = Derivation(F, {x: 1, y: root.derivative(x)/2*y/root})
D(y)
U = root
V = den
S2 = U*V^2*D(y/V)
\end{sageblock}
Now we want to solve $f_2 S_2 = A_2 y$ where $A_2 y$ is our numerator.

\begin{sageblock}[chebyshev2]
def diophantine(a,b,c):
   (g,s,t) = xgcd(b,c)
   if not g.divides(a):
      raise ValueError("diophantine: a doesn't divide gcd(b,c)")
   s = s * (a//g)
   t = t * (a//g)
   # g = sb + ct
   # a = mg = (ms)b + (mc)t
   (q,r) = s.quo_rem(c)
   # s = r + qb, so r = s - qb
   return (r, t + q*b)
\end{sageblock}

\begin{sageblock}[chebyshev2]
f2 = num*y/S2
T2 = f2.numerator()
Q = f2.denominator()
R2 = QQ['x']
(A,R) = diophantine(R2(1),R2(V),R2(Q))
(Q2,B2) = (T2*R).quo_rem(V)
h = A*num*y/(V*U) - (D(V)*Q2+D(B2))*y/V + Q2*D(y)
F3 = Frac(ZZ['x']['y'])
\end{sageblock}

$$\int \sage[chebyshev2]{F3(num*y/den)}\,{\rm d}x$$
$$=\sage[chebyshev2]{F3(B2*y/V)} + \int \sage[chebyshev2]{F3(h)}\,{\rm d}x$$

We solved this new integral in example \ref{Chebyshev's Integral I}.
The final answer is:

$$A(x) = 1023x^8+4104x^7+5048x^6+2182x^5+805x^4+624x^3+10x^2+28x$$
$$B(x) = 1025x^{10} + 6138x^9 + 12307x^8 + 10188x^7 + 4503x^6 + 3134x^5 + 1598x^4 + 140x^3 + 176x^2 +2$$
$$C(x) = 32x^{10}-80x^8+80x^6-40x^4+10x^2-1$$

$$y = \sqrt{x^4+4 x^3+2 x^2+1}$$

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \,{\rm d}x
= {{(x+{1\over2})y}\over{2x^2-1}} + {1\over2}\ln{{A(x)y - B(x)}\over{C(x)}}
$$

\endexample


\vfill\eject
\mysection{Se\~nor Gonzalez, otra vez}

The Rothstein-Trager resultant allows us to compute all the residues
at once.  Trager, in his Ph.D. thesis, then showed how to construct a
function that is zero at all poles with a given residue, and non-zero
at all other poles, as well as at all places conjugate to a pole.
