
\mychapter{Good Reduction}

At this point, there is only one major missing piece in our
integration theory for simple radicals --- how do we limit the
multiples of a divisor to a testable set?  We've seen how to
repeatedly raise a divisor to higher and higher powers, but how do we
know when to stop?  At what point can we declare that a divisor has no
multiple that is principle?

We'll attack this problem the same way we attacked polynomial
factorization in Chapter ?, by mapping into a finite field, solving a
corresponding problem there, then somehow lifting the result back to
the original field.  The details differ, but the basic idea is the
same.  Of course, divisors, like polynomials, behave somewhat
different in finite fields, so our first task is to study
some of their unique properties in this domain.

\section{Simple Algebraic Extensions over Finite Fields}

Let's start with a simple but crucial observation:

\theorem

In an algebraic extension over a finite field, the evaluation field is
also finite.

\proof

Consider a finite field of constants ${\cal F}$, over which we'll
extend first into a rational function field ${\cal F}(x)$ and then add
an algebraic extension ${\cal F}(x,y)$, where $y$ satisfies some
minimial polynomial $f(x,y)=0$.  Start with the constant field, which
gives us a finite number of values for $x$.  Plugging each of these
values into the minimal polynomial gives a finite set of polynomials
$f(y_i)=0$.  By Theorem ?, we can extend ${\cal F}$ into a finite
extension field ${\cal F}[\gamma]$ where all the roots of the
polynomial exist.  Since there a only a finite number of polynomials,
we need at worst a finite set of extensions ${\cal
F}[\gamma_1,...,\gamma_k]$ to construct a field in which all the roots
of all the polynomials exist.  Using the Theorem of the Primitive
Element, we can collapse all of these into a single finite extension
field ${\cal F}[\phi]$.  Since all values of $x$ exist in ${\cal F}$,
and all values of $y$ exist in ${\cal F}[\phi]$, an evaluation
homomorphism carries any rational function in $x$ and $y$ into
${\cal F}[\phi]$.

\endtheorem

This theorem leads directly to the single more important difference
(to us) between divisors in an infinite field versus those in a finite
field.  {\it In a finite field, some multiple of every divisor is
principle.}  The reason is that the multiplicative group of the
evaluation field has finite order.  The simplest way to demonstrate
this is to construct theorems analogous to Theorems ? and ?:

\theorem

In an algebraic extension of a finite field with characteristic
greater than 2, a function can always be constructed with an $m^{\rm
th}$-order zero at a specified place $(\alpha, \beta)$ and zero order
at all other finite places, where $m$ is the multiplicative order of
the evaluation field.

\proof

The desired function is

$$(x-\alpha)^m + (y-\beta)^m$$.

Clearly, this function is zero at $(\alpha, \beta)$ and of $m^{\rm
th}$ order there (PROOF THIS).  At all other places one of the two
terms will be non-zero, and both exist in the evaluation field.  By
Theorem ?, any non-zero number raised to the multiplicative order of
its field is one.  Thus the value of this function will be either
$1+0$, $0+1$, or $1+1=2$, all finite and non-zero, and thus of zero
order.

\endtheorem

\theorem

In an algebraic extension of a finite field with characteristic
greater than 2, a function can always be constructed with an $m^{\rm
th}$-order pole at a specified place $(\alpha, \beta)$ and zero order
at all other finite places, where $m$ is the multiplicative order of
the evaluation field.

\proof

The desired function is

$${f(\alpha,y)^m\over(x-\alpha)^m(y-\beta)^m} + 1$$

where the division by $(y-\beta)^m$ is exact.
Clearly, this function has a pole at $(\alpha, \beta)$ and of $m^{\rm
th}$ order there (PROOF THIS).  CONSIDER OTHER PLACES OVER $\alpha$.
At all other places the denominator
term will be non-zero, and thus one, and the numerator will be
either zero or one (by Theorem ?)
Thus the value of this function at these places will be either
$0+1$ or $1+1=2$, both finite and non-zero, and thus of zero
order.

\endtheorem


\example

Show that some multiple of ${\mathrm Z}(1,1)$ is principle in
${\bf Z}_5(x,y); y^2=x$.

Let's first construct a multiplication table for ${\bf Z}_5$:

\begin{center}
\begin{tabular}{c|c c c c c}
  & 0 & 1 & 2 & 3 & 4 \cr
\hline
0 & 0 & 0 & 0 & 0 & 0 \cr
1 & 0 & 1 & 2 & 3 & 4 \cr
2 & 0 & 2 & 4 & 1 & 3 \cr
3 & 0 & 3 & 1 & 4 & 2 \cr
4 & 0 & 4 & 3 & 2 & 1 \cr
\end{tabular}
\end{center}

Now, let's list out the places on the Riemann surface for
${\bf Z}_5(x,y); y^2=x$.

\begin{center}
\begin{tabular}{c l}
$x$ & $(x,y)$ \cr
\hline
0 & (0,0) \cr
1 & (1,1) \quad (1,4) \cr
2 & $(2,\gamma) \quad (2,-\gamma); \quad \gamma^2 - 2 =0$ \cr
3 & $(3,\theta) \quad (3,-\theta); \quad \theta^2 - 3 =0$ \cr
4 & (4,2) \quad (4,3) \cr
\end{tabular}
\end{center}

It looks like we need ${\bf Z}_5[\gamma,\theta]$ to express these places.
It's simplest to collapse $\gamma$ and $\theta$ into a single algebraic
extension.  We could use the Theorem of the Primitive Element to
do this, but in this case just looking at the multiplication table
and noting that $3 = 2^3 = \gamma^6$ shows that $\theta = \pm \gamma^3$.
So, in fact, we only need ${\bf Z}_5[\gamma]$:

\begin{center}
\begin{tabular}{c l}
$x$ & $(x,y)$ \cr
\hline
0 & (0,0) \cr
1 & (1,1) \quad (1,4) \cr
2 & $(2,\gamma) \quad (2,-\gamma); \quad \gamma^2 - 2 =0$ \cr
3 & $(3,\gamma^3) \quad (3,-\gamma^3)$ \cr
4 & (4,2) \quad (4,3) \cr
\end{tabular}
\end{center}

Since ${\bf Z}_5[\gamma]$ has $5^2=25$ elements, its multiplicative
group has order one less than this.  We conclude that 24 is our
``magic'' multiple, and that ${\mathrm Z}^{24}(1,1)$ must be
principle in this field.  Its generator should be simply
$(x-1)^{24} + (y-1)^{24}$.  Clearly this function is zero for
$(x,y)=(1,1)$.  Let's verify that it's non-zero for some other
places on the Riemann surface:

\begin{eqnarray*}
(0,0) &:& (-1)^{24} + (-1)^{24} = 4^{24} + 4^{24} = 1+1 = 2 \cr
(1,4) &:& 3^{24} + 0^{24} = 1 + 0 = 1 \cr
(2,\gamma) &:& (\gamma-1)^{24} + (2-1)^{24} = 1+1 = 2 {\rm ,\quad since:} \cr
&&\cr
&& (\gamma-1)^2 = (\gamma^2-2\gamma+1) = 3-2\gamma \cr
&& (\gamma-1)^4 = (3-2\gamma)^2 = (9-12\gamma+4\gamma^2) = 2-2\gamma \cr
&& (\gamma-1)^8 = (2-2\gamma)^2 = (4-8\gamma+4\gamma^2) = 2-3\gamma \cr
&& (\gamma-1)^{12} = (2-2\gamma)(2-3\gamma) = (4-10\gamma+6\gamma^2) = 1 \cr
\end{eqnarray*}

In the final series of calculations, I used $\gamma^2=2$ and reduced
mod 5 repeatedly.  I think the pattern should be clear, and leave
further verification as an exercise.

\endexample

\section{Jacobian Varieties}

An algebraic extension is a simple example of what algebraic geometers
term a {\it variety}, which is the zero locus of a set of polynomials
defined over some field.  Thus, for example, the unit circle is a
variety (defined over the real numbers), because it is the zero locus
of $x^2+y^2=1$.  But the points $(1,0)$ and $(-1,0)$ are also a
variety, because they are the zero locus of the {\it set} of
polynomials $\{x^2=1; y=0\}$.

An {\it abelian variety} is a variety accompanied by a commutative
group structure on its elements, which typically includes picking an
arbitrary zero point as the identity element.  The circle is an
abelian variety, if we identify its points with their angles from the
x-axis and make $(1,0)$ our identity element.  Now any two points can
be ``added'' or ``subtracted'' (by adding or subtracting their
respective angles) to obtain a third point, and each point has an
inverse associated with it (its mirror image across the x-axis).  It
should be obvious that the choice of a zero point was totally
arbitrary.  Likewise, the points ${(1,0), (-1,0)}$ also form an
abelian variety; their group structure is isomorphic to ${\bf Z}_2$ and
the choice of one of them as the identity is, again, arbitrary.

Is every variety abelian?  No, but any complete, non-singular variety
can be homomorphicly mapped into an associated abelian variety
(typically of higher dimension), called its {\it Jacobian variety}.
This fact, combined with the extensive body of literature on abelian
varieties ([Mumford], [Birkenhake and Lange], [Lang], to mention a
few), makes the Jacobian variety an important object of study (though
David Mumford, in the preface to [Mumford], described it as a
``crutch'').

We will be needing only a tiny bit of this theory here, so my goal in
this section is only to demonstrate how the Riemann-Roch Theorem
allows us to set up an abelian group structures on an algebraic
extension.

\section{The Riemann-Roch Theorem}

The Riemann-Roch Theorem is one of the most celebrated theorems in
mathematics.  Not only does it provide a crucial tool in understanding
the structure of algebraic extensions, but it does so by tying
together algebra, analysis, and geometry in one equation.

First, let's review that equation:

\theorem {\rm (Riemann-Roch)}

% For any divisor $\mathfrak{b}$,
% 
% $$l(\mathfrak{b}) = \deg \mathfrak{b} + 1 - g + l(\mathfrak{c}-\mathfrak{b}) $$
% 
% where $l(\mathfrak{b})$ is the dimension of the vector space
% $L(\mathfrak{b})$ of multiples of $-\mathfrak{b}$, $g$ is the genus of
% the extension, and $\mathfrak{c}$ is any divisor of the canonical class of
% differentials.

For any divisor $\mathfrak{b}$,

% $$l(\mathfrak{b}) = \deg \mathfrak{b} + 1 - g + l(\mathfrak{c}-\mathfrak{b}) $$
% $$l(-\mathfrak{b}) = \deg \mathfrak{b} + 1 - g + l(\mathfrak{b}-\mathfrak{c}) $$
% $$l(\mathfrak{b}) = \deg -\mathfrak{b} + 1 - g + l(-\mathfrak{b}-\mathfrak{c}) $$
% $$l(\mathfrak{b}) = - \deg \mathfrak{b} + 1 - g + l(-\mathfrak{b}-\mathfrak{c}) $$

$$l(\mathfrak{b}) = - \deg \mathfrak{b} + 1 - g + l(-\mathfrak{b}-\mathfrak{c}) $$

where $l(\mathfrak{b})$ is the dimension of the vector space
$L(\mathfrak{b})$ of multiples of $\mathfrak{b}$, $g$ is the genus of
the extension, and $\mathfrak{c}$ is any divisor of the canonical class of
differentials.

\endtheorem

Interrelated by this theorem is the purely algebraic concept of the
dimension of the vector space of multiples of a divisor, the geometric
concept of the genus, and the analytic concept of a differential.

However, this sophistication comes with a price.  Specifically, we
need a topology to define the genus, and we need a limit to define the
differential.

Andr\'e Weil showed how the Riemann-Roch theorem can be stripped of
the analysis and the geometry, and proved as purely a result in
algebra.  The genus, instead of a topological invariant, now appears
as merely a least upper bound on a divisor's degree of specialization,
and a differential becomes an object in a dual space that maps a
function into the field of constants.  The advantage of this
formulation is that does not require any topological structure, and is
therefore well suited to use with finite fields.  It is this
formulation I will now adopt.

First, at any place in the function field, there is a local valuation
ring with a maximal prime ideal.  We can normalize the valuation (it
is discrete) and pick a element of unit valuation to use as a
uniformizing variable.  By multiplying as necessary by some power of
this element, we can adjust any field element to be a unit of the
valuation ring and thus associate an order ${\rm ord}_{\mathfrak{p}}$
with that element.  The valuation ring's units are a finite extension
of the constant subfield; they are the constant subfield if it is
algebraically closed.  By subtracting out the remainder mod
$\mathfrak{p}$, we get an element of higher order, which we can again
subtract out, and so on, building a power series in the uniformizing
variable.  Each element of the function field thus has a power series
associated with it at each place $\mathfrak{p}$.

A collection of such power series, one at each place, with arbitrary
coefficients except that there are only a finite number of
coefficients with negative powers, is called a {\it vector}.  Each
individual power series is called a {\it component} of the vector.
Clearly, every function has a vector associated with it; but the
converse is not necessarily true.  The mapping from functions to
vectors is injective, though.  Any two different functions will have a
non-zero difference that must therefore have a finite value, of finite
order, at some place $\mathfrak{p}$, and their vectors will differ at
that point.

We also have a dual space of {\it covectors}.  The coefficients of a
covector component at a place are dual to the constant field at that
place; if the constant field is algebraically closed, then the
covector coefficients are in the constant field.  Like vectors,
covectors can only have a finite number of negative power
coefficients.

We define a dot product between a vector $v$ and a covector $\lambda$:

$$ v \cdot \lambda = \sum_{\mathfrak{p}} \sum_{i+j=-1} v_{\mathfrak{p},i}
  \lambda_{\mathfrak{p},j} $$

where $v_{\mathfrak{p},i}$ is the coefficient of the $i^{\rm th}$
power in $v$'s component at $\mathfrak{p}$, and likewise for
$\lambda_{\mathfrak{p},j}$.  Notice that the second summation requires
at least one of $i$ or $j$ to be negative, so there will only be a
finite number of places for the first sum at which the second sum
contributes anything at all.

Weil also requires the {\it Theorem of Independence}, which states
that, although an arbitrary (full) vector may not have a function
associated with it, a function can always be found which matches a set
of finite prefixes at a finite number of places.  This can be
demonstrated using Theorem \ref{finite orders construction},
repeatedly applied a finite number of times.  We also need to know
that a function without a pole is constant.

With this setup, we can now prove a series of theorems that lead up
the the Riemann-Roch Theorem.

\theorem

$$l(\mathfrak{p}) \leq \deg \mathfrak{p} +1$$

i.e, $l(\mathfrak{p})$, the dimension (over the constants) of
$L(\mathfrak{p})$, the multiples of $-\mathfrak{p}$, is no more than
the degree of the divisor $\mathfrak{p}$, plus one.

\proof

Since $\deg -\mathfrak{p} = - \deg \mathfrak{p}$, there are at least
$\deg \mathfrak{p}$ poles (counting multiplicities) in
$-\mathfrak{p}$, and at least $\deg \mathfrak{p}$
coefficients with negative powers in the vectors corresponding
to the elements in $L(\mathfrak{p})$

.  We can impose

\endtheorem

\theorem

If $\mathfrak{A}$ is divisible by $\mathfrak{B}$, i.e, if
$\mathfrak{A}\mathfrak{B}^{-1} \subseteq {\cal I}$, then

$$n(\mathfrak{A}) - l(\mathfrak{A}) \leq n(\mathfrak{B}) - l(\mathfrak{B}) $$

$$n(\mathfrak{p}) \equiv {\rm deg} \mathfrak{p}$$

\proof

Consider $\mathfrak{C} = \mathfrak{A}\mathfrak{B}^{-1}$.
Now $n(\mathfrak{C}) = n(\mathfrak{A}) - n(\mathfrak{B})$ and since
$\deg -\mathfrak{C} = - \deg \mathfrak{C}$, and
$\mathfrak{C}$ is integral (by supposition),
there are exactly
$n(\mathfrak{C})$ poles (counting multiplicities) in $-\mathfrak{C}$.
, and at least $\deg \mathfrak{p}$
coefficients with negative powers in the vectors corresponding
to the elements in $L(\mathfrak{p})$

.  We can impose

\endtheorem

Back to the Riemann-Roch Theorem...

It immediately follows (from $\mathfrak{b}={\bf 0}$) that
$l(\mathfrak{c})=g$, which can be taken as the definition of the
genus.

We can now pick $g$ independent differentials from $\mathfrak{c}$ and
use them (along with an arbitrary origin) to map into the torus
${\bf C}/\Lambda^g$.

Now, Abel's Theorem and the Jacobi inversion theorem ([Griffiths and
Harris], p. 235) shows that ${\rm Pic}^0$, the group of divisors of
degree zero modulo linear equivalence is isomorphic to ${\bf
C}/\Lambda^g$.

Alternately, ([Lang], II, \S1, Theorem 3), we can factor a mapping
of a product into an abelian variety into mappings on each factor.

Lang also characterizes Abel's theorem as follows:

\begin{quote}

Let $\omega_1, ..., \omega_g$ be a basis for the differential forms
on the first kind of V.  If $\mathfrak{a} = \sum n_i P_i$ is a
[divisor] of degree 0 on V, and P is a fixed point of V, then
the map into ${\bf C}/\Lambda^g$ given by:

$$\mathfrak{a} \to \sum n_i (\int_P^{P_i}\omega_1, ..., \int_P^{P_i}\omega_g)$$

is well defined modulo the periods... the kernel consists of those
divisors that are linearly equivalent to 0 (i.e, principle); this is
Abel's theorem.

\end{quote}


\section{Endomorphism Rings}

Any commutative group $G$ induces a (non-commutative) ring structure
on its endmorphisms, defined as follows (remember that an
endomorphism is a homomorphism from an object to itself):

Two endmorphisms $\phi(g): G \to G$ and $\gamma(g): G \to G$ are added
using $G$'s group operation on the images: $(\phi+\gamma)(g) =
\phi(g)\cdot\gamma(g)$, where $\cdot$ denotes the group operation.
The additive identify is the endmorphism that maps the entire group
onto its identity element.

Two endmorphisms $\phi(g): G \to G$ and $\gamma(g): G \to G$ are
multiplied using composition of mappings: $(\phi\gamma)(g) =
\phi(\gamma(g))$.  The multiplicative identity is the endmorphism that
maps every element in the group onto itself.

Let us now verify that these operations define a ring, the {\it endomorphism
ring} of G, which we shall denote ${\rm End}(G)$.  The properties
of the identity elements are fairly obvious, I think.  Almost as
obvious is that the associative and commutative properties of the
underlying group translate directly into additive associative and
commutative properties in the endmorphism ring.  The multiplicative
properties follow from composition of mappings being associative, but
not necessarily commutative.  The distributive law follows from the
easily verified identity $\phi(\gamma(g)\cdot\mu(g)) = \phi(\gamma(g)) \cdot
\phi(\mu(g))$, using the fact that $\phi$ is an endomorphism, and thus
a homomorphism, and therefore maps the group operator through.

The ring of integers ${\bf Z}$ can be mapped homomorphicaly\footnote{An easy
consequence of ${\bf Z}$'s repelling universal property in the category of
rings, see [Lang], p. ?} into any ring, and an endomorphism ring is no
exception.  We'll denote by $[m]$ the endmorphism mapped to by the
integer $m$. $[0]$ is clearly the additive identity mapping all
elements to the group identity.  $[1]$ is, of course, the
multiplicative identity mapping all elements to themselves.  $[2]$ is
$[1]+[1]$, the endmorphism that composes each element with itself
(using the group operator): $[2]: g \to g\cdot g$.  $[3]$ composes
each element with itself thrice: $[3]: g \to g\cdot g\cdot g$, etc.

Because ${\bf Z}$ is commutative, the subring $[m]$ it maps to is also
commutative, even though ${\rm End}(G)$ may not be.



\section{Good Reduction}

My notes from [Sh61].

{\small\begin{verbatim}

A function (Y -> k) is regular at a point on a variety if there exists
an open neighborhood of the point where the function is given by a
rational function of polynomials, the denominator never zero.
(relation to coordinate ring?)

A map (Y -> k) is regular on Y if it is regular at every point of Y.

A morphism (X -> Y) between varieties is a continuous map (in the
Zariski topology) such that pullbacks of regular functions are
regular.

A rational map is a morphism defined only on an open subset.

Given a rational map from affine varieties X to Y, if [x,y,z] is Y's
coordinate system, then we pullback to a regular function, which is a
rational function in X's function field.  So Y's coordinates are given
by rational functions in X's coordinates.

A group variety is an algebraic variety equipped with a group
structure, where the group operation and group inversion are rational
maps.  If the group operation is commutative, it's an abelian variety.

A homomorphism between abelian varieties is a rational map that
commutes with the group operation.

An endomorphism is a homomorphism from the variety to itself.

Endomorphisms of a group form a ring.  Addition is performed by
mapping through both endomorphisms, then applying the group operation,
which is commutative, so endomorphism addition is commutative.
Multiplication is performed by composition, and need not be
commutative.

Using just the addition structure, we get an abelian group that can be
structued as a Z-module.  Multiplication by an integer is just
repeated application of the endomorphism.

We can promote the endomorphism ring into an algebra by tensoring with
Q, call this EndQ(A).  Elements of EndQ(A) are basically endomorphisms
with an associated 1/n denominator (numerators can be sucked into the
endomorphism).

A lattice is a free Z-module of the same rank as the algebra over Q.

An order is a lattice that is also a subring and contains the identity.

The order of A, written t, is the image of the endomorphism ring in
the endomorphism algebra.

a is a lattice in the endomorphism algebra contained in t, so it's a
collection of actual endomorphisms.

g(a,A) is the set of points on A mapped to 0 by every element of a.

Prop 16. Let a be an integral ideal (p. 49) of F.  Reduction mod p
defines a homomorphism of g(a,A) onto g(a,A^).  If a is prime to the
characteristic of k^, this homomorphism is an isomorphism.


Div(C) is group of divisors
Div0(C) is group of degree 0 divisors
P(C) is group of principle divisors

P(C) in Div0(C) in Div(C)

define Pic(C) = Div(C)/P(C)
define Pic0(C) = Div0(C)/P(C)

Pic0(C) is isomorphic to the Jacobian variety with a point O.

Given a degree zero divisor in Div0(C), we can use the group law on
the Jacobian to construct a single point corresponding to the divisor.
Asking if the divisor is principle is asking if this point is O.
Asking if any multiple of the divisor is principle is asking if any
multiple of this point is O.

We can define an endomorphism to be addition by a point, using the
group law.  NO - doesn't take identity to identity.

Let's consider [n], the endomorphism defined by applying the group
operation n times (n is an integer).  This should generate a lattice
contained in t, so it's an integral ideal.  g([n], A) is the set of
points whose n-multiples are principle.

Given a divisor whose (k p^q)-multiple is principle, let's multiply by
p^q and get a divisor whose k-multiple is principle.  Endomorphism
ideal [k] is prime to p.  Then this divisor point will be in g(a,A)
and g(a,A^), where a is [k].

We can determine that the divisor point is in g(a,A^) for a=[k] by
determining that the divisor's k-multiple is principle on the module
curve.  Since this is an isomorphism, the divisor point is also in
g(a,A) for a=[k].




Given a non-singular algebraic curve C, we reduce mod p to get Cp.
Good reduction implies that Cp is non-singular with the same genus.  C
has an associated Jacobian J.  Cp also has an associated Jacobian Jp.

PROBLEM: (hopefully) Show that J mod p is Jp.

Construct J as follows.  Pick r > 2g-2.  Symmetric group J^(r).  Pick
r-g extra points and find an open covering of J^(r).  Within each
open set, construct J locally.

\end{verbatim}
}

\mysection{Examples}

\example Compute $\int \sqrt{4-x^2} \,dx$

A solution method from first year calculus might be to note that
this integrand forms one leg of a right triangle:

\begin{center}
\setlength{\unitlength}{1cm}
\begin{picture}(6,5)
\put(5,1){\line(0,1){3}}
\put(5,1){\line(-1,0){4}}
\put(1,1){\line(4,3){4}}
\put(2.5,0.5){$\sqrt{4-x^2}$}
\put(3,2.8){2}
\put(5.2,2.5){$x$}
\put(1.7,1.15){$\theta$}
\end{picture}
\end{center}

$$x=2\sin\theta \qquad \sqrt{4-x^2}=2\cos\theta \qquad dx=2\cos\theta\,d\theta$$


\begin{eqnarray*}
\int \sqrt{4-x^2} \, dx & = & \int 4 \cos^2\theta \, d\theta \\
& = & \int \left( 2 + 2\cos 2\theta \right) \, d\theta \\
& = & 2\theta + \sin 2\theta \\
& = & 2\theta + 2\sin\theta\cos\theta \\
& = & 2\arcsin\frac{x}{2} + \frac{x \sqrt{4-x^2}}{2} \\
\end{eqnarray*}

Now let's attack this integral using the methods of this chapter.
First, transform the problem into an algebraic curve:

$$\int y\,dx \qquad y^2 = 4-x^2$$

Since $\lim_{x\to\infty} y = \infty$, the integrand has poles at
infinity.  We want infinity to be an ordinary point of the curve (no
ramification; no singularities) with no poles in the integrand.  The
simplest transformation is to exchange zero with infinity, and in this
case zero is an ordinary point with places $(0,2)$ and $(0,-2)$,
neither of which is a pole of the integrand.  So we'll invert
$x$ and $y$ into $u$ and $v$:

$$x=\frac{1}{u} \qquad y=\frac{1}{v}$$
$$\left(\frac{1}{v}\right)^2 = 4 - \left(\frac{1}{u}\right)^2 \Longrightarrow 4u^2v^2 - v^2 - u^2=0$$
$$\int\frac{1}{v} \, d\left(\frac{1}{u}\right) \Longrightarrow -\int\frac{1}{vu^2}\,du$$

The only poles in this integrand occur when either $u=0$ or $v=0$.
Substituting these values into $4u^2v^2 - v^2 -u^2=0$, we see that
these condiutions only occur at $(u,v)=(0,0)$, so let's analyze our
curve at that point, starting with the Newton polygon:

\begin{center}
$4 u^2 v^2 - v^2 - u^2 = 0$ \\
\setlength{\unitlength}{1cm}
\begin{picture}(3,3)
\put(0,0){\line(0,1){2.5}}
\put(0,0){\line(1,0){3}}
\put(1.9,-0.1){x}
\put(1.9,1.9){x}
\put(-0.1,1.9){x}
\thicklines
\put(0,2){\line(1,-1){2}}
\end{picture}
\end{center}

The Newton polygon has a single line segment of span 2 and slope -1, so
we have two cycles, each with ramification index one: a singularity.
Since there is no ramification, $u$ is a uniformizing parameter
and we expect to expand $v$ as follows:

$$v = c_1 u + c_2 u^2 + c_3 u^3 + \cdots$$
$$v^2 = c_1^2 u^2 + 2 c_1 c_2 u^3 + (2 c_1 c_3 + c_2^2) u^4 + \cdots$$

Substituting these expansions into $4u^2v^2 - v^2 - u^2 = 0$, we obtain:

$$ 4 c_1^2 u^4 + 8 c_1 c_2 u^5 + (8 c_1 c_3 + 4 c_2^2) u^6 + \cdots $$
$$ - c_1^2 u^2 - 2 c_1 c_2 u^3 - (2 c_1 c_3 + c_2^2) u^4 + \cdots - u^2 = 0$$

Equating terms in $u^2$, we see that $c_1 = \pm i$.  Each of these
two values corresponds to one branch of the singularity.  There
is only a single term in $u^3$, which forces $c_2$ to be zero,
and equating terms in $u^4$ produces $c_3 = 2 c_1$, so

$$v = \pm (iu + 2iu^3 + \cdots) \qquad @(0,0)$$

Inverting $v$ and substituting into our 1-form, we obtain

$$\frac{1}{v} = \pm (-i \frac{1}{u} + 2i u + \cdots) \qquad @(0,0)$$

$$\frac{1}{vu^2}\, du = \pm \left[ -i \frac{1}{u^3} + 2i \frac{1}{u} + \cdots \right] \, du \qquad @(0,0)$$

The $u^{-1}$ terms will integrate into logarithms, so let's ignore
them for the moment and concentrate on the $u^{-3}$ terms, which will
integrate into $u^{-2}$ terms, so we're looking for a function with
second order poles at both places at the $(0,0)$ singularity.

Starting with our standard basis for all rational functions,
$\{1,\,v\}$, we seek to modify it into a basis for
${\rm P}^2(0,0)_a{\rm P}^2(0,0)_b$.  Note first that $v$ has
poles at $u=\pm\frac{1}{2}$.  Using $y=1/u$, we analyze
at $(\pm\frac{1}{2}, \infty)$ as follows:

\begin{center}
$y^2\left[(u-\frac12)^2+(u-\frac12)+\frac14\right]-4(u-\frac12)^2-4(u-\frac12)$
\\
\setlength{\unitlength}{1cm}
\begin{picture}(3,3)
\put(0,0){\line(0,1){2.5}}
\put(0,0){\line(1,0){3}}
\put(0.9,-0.1){x}
\put(1.9,-0.1){x}
\put(-0.1,1.9){x}
\put(0.9,1.9){x}
\put(1.9,1.9){x}
\thicklines
\put(0,2){\line(1,-2){1}}
\end{picture}
\end{center}

Our line segment has span 1 and slope -2, indicating a single place
with ramification 2, and $y$ as a uniformizing parameter.  Setting

$$(u-\frac12) = c_1 y + c_2 y^2 + \cdots$$
$$(u-\frac12)^2 = c_1^2 y^2 + \cdots$$

Substituting, we find that $c_1 = 0$ and $c_2 = \frac{1}{16}$, so

$$(u-\frac12) = \frac{1}{16} y^2 + \cdots \qquad v=y^{-1} \qquad @(\frac12, \infty)$$

$$(u+\frac12) = \frac{1}{16} y^2 + \cdots \qquad v=y^{-1} \qquad @(-\frac12, \infty)$$

In short, $v$ has first order poles at $(\pm\frac12,\infty)$ and
$(u\pm\frac12)$ has second order zeros, so we can adjust our basis
accordingly and obtain $\{1,\,(4u^2-1)v\}$ for a basis with no finite
poles.  We can also use a theorem of Trager to shortcut this calculation.

Returning to our analysis at $(0,0)$, we see that 1 has zero order
(obviously) and $(4u^2-1)v$ has a first order zero at both sheets
there, since $4u^2-1=-1$ is finite and $v$ has first order zeros.
We also know that $u$ is a uniformizing parameter, so it's easy
to modify our basis and obtain

$$\left\{\frac{1}{u^2},\,\frac{4u^2-1}{u^3}v\right\} {\rm is\, a\,} {\bf C}[x]{\rm -basis\, for\, P^2(0,0)_aP^2(0,0)_b}$$

Is this basis normal at infinity?  Well, the representation order of
$\frac{1}{u^2}$ is 2 and its $u^-2$ coefficients at $(\infty, \pm
\frac12)$ are both 1, while the representation order of $\frac{4u^2-1}{u^3}v$
is 1, and its $u^-1$ coefficients are 2 and -2.  Since

$$\det C = \begin{array}{|cc|} 1 & 2 \\ 1 & -2 \end{array} = -4$$

is non-zero, the basis is normal at infinity.

The Riemann-Roch theorem says that the dimension of ${\mathfrak l}(D)$ is 5,
$\frac{1}{u^2}$ can be multiplied by any polynomial up to second
degree without introducing poles at infinity, and $\frac{4u^2-1}{u^3}v$
can be multiplied by any polynomial up to first degree, so

$$\left\{\frac{1}{u^2},\, \frac{1}{u},\, 1,\, \frac{4u^2-1}{u^3}v,\, \frac{4u^2-1}{u^2}v\right\}$$

is a ${\cal C}$-module basis for ${\mathfrak l}(D)$.

Any linear combination of these functions is a multiple of the
divisor, but not all of them produce the correct residues.  Looking at
the residues, we see that only $\frac{4u^2-1}{u^3}v = \frac{1}{uv}$
has residues of $\pm i$ on the two sheets at the $(0,0)$ singularity.
Dividing by 2 to correct for the 2 that will be introduced by the
integration, we conclude that $\frac{1}{2uv} = \frac{xy}{2} =
\frac{x\sqrt{4-x^2}}{2}$ is the desired function.

Next, we have to deal with the logarithms.  Going back to the
series expansions of our 1-form, we see that we have residues
of $\pm 2i$ on our two sheets at $(0,0)$.  The objective
now is a bit different; we want a function with exactly
the divisor $Z(0,0)_a P(0,0)_b$.  Starting with an integral basis:

$$\{1, (4u^2-1)v\}$$

we want to modify these functions to make them multiples
of $Z(0,0)_a P(0,0)_b$.  The pole isn't a problem for
an integral basis, and looking at the series expansion
for $v$ at $(0,0)$ we see that it (and therefore $(4u^2-1)v$)
has a simple zero there, but $1$ needs to be replaced with $u$:

$$\{u, (4u^2-1)v\}$$

Now we construct a matrix with the coefficients in the series expansions:

$$\left[ \begin{array}{cc} 1 & -i \\ 0 & 0 \end{array} \right] \begin{array}{ll} \leftarrow (0,0)_a \\ \leftarrow (0,0)_b \end{array} $$

$$\left[ \begin{array}{cc} 1 & -i \\ 0 & 0 \end{array} \right] \left[ \begin{array}{c} i \\ 1 \end{array} \right] = 0$$

The solution shows us how to modify the basis:

$$\{u, \frac{iu + (4u^2-1)v}{u}\} = \{u, i + \frac{(4u^2-1)v}{u}\}$$

$$\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right] \begin{array}{ll} \leftarrow (0,0)_a \\ \leftarrow (0,0)_b \end{array} $$

$$\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right] \left[ \begin{array}{c} 0 \\ 1 \end{array} \right] = 0$$

$$\{u, i\frac{1}{u} + \frac{(4u^2-1)v}{u^2}\}$$

$$\left| \begin{array}{cc} 1 & -2i \\ 0 & 2i \end{array} \right| = 2i$$

At the last step, the determinant is non-zero, which shows that we
now have a basis for multiples of the divisor except at infinity.
Is it normal at infinity?  $u$'s expansion at both places at infinity
is $\left(\frac{1}{u}\right)^{-1}$, so its representation order is -1,
and the second element's expansion at infinity starts $\pm 2 + \cdots$,
so its representation order is 0 and:

$$\det C = \begin{array}{|cc|} 1 & 2 \\ 1 & -2 \end{array} = -4$$

So the basis is normal at infinity.  If an exact multiple of
the divisor exists, it is one of the basis elements.  It's not $u$,
since $u$ has a pole at infinity, but the second element is exact:

$$i\frac{1}{u} + \frac{(4u^2-1)v}{u^2} = i\frac{1}{u} - \frac{1}{v} = ix-y$$

The desired residues are $\pm 2i$, so the function we want is

$$2i \ln(ix-y) = 2i \ln(\frac{y}{2}-i\frac{x}{2}) + 2i \ln(-2) $$
$$= 2i \ln\left(\sqrt{1-\left(\frac{x}{2}\right)^2} - i\frac{x}{2}\right) = 2i (-i \arcsin \frac{x}{2}) = 2 \arcsin \frac{x}{2}$$

(the constant disappears into the constant of integration) and the final answer is:

$$ \int \sqrt{4-x^2} \, dx  = 2\arcsin\frac{x}{2} + \frac{x \sqrt{4-x^2}}{2}$$

\endexample


\vfill\eject
\mysection{arcsin}

\example Compute $\int {1\over{\sqrt{1-x^2}}} \,dx$

The obvious attempt is to use the algebraic extension $y^2=1-x^2$ and
integrate ${1\over y}\,dx$.

But we first need to determine if this differential has any poles at
infinity, by inverting the field and looking for poles at zero.
Setting $u={1\over x}$, we convert our minimal polynomial into
$u^2y^2=u^2-1$ (after multiplying through by $u^2$), and using
$v=uy$ we obtain our inverse field ${\bf C}(u,v); v^2=u^2-1$.

Since $x={1\over u}$ and $y={v\over u}$, we convert our differential as follows:

 $${1\over y}\,dx ={u\over v} (-{1\over{u^2}} \, du) = -{1\over{uv}} \, du$$

Now, $\{1, v\}$ is an integral basis for the inverse field, so we
multiply through by $v\over v$ to obtain:

 $$= -{v\over{uv^2}} du = -{1\over{u(u^2-1)}}v \, du $$

which is now in normal form and clearly has a pole at $u=0$, or $x=\infty$.  Note that

 $${1\over y} = {u\over v} = {{uv}\over{v^2}}
 = {u\over{u^2-1}} v$$

has no pole at $u=0$, a clear example of a differential having a pole
at a place where its constituent function has none.

In any event, we clearly can not use the original field to conduct the
integration, since it would require constructing a function with a
pole at infinity, and our algorithm can't handle this.  So we need to
transform into a field where the differential has no pole at infinity.

Actually, we've already done this!  Note that the integrand had no pole
at zero in the original field:

 $${1\over y}\,dx = {y\over y^2}\,dx = {1\over{1-x^2}}y \,dx $$

Since the inverse field swapped zero with infinity, it follows that
there is no pole at infinity in the inverse field, so we can proceed
to integrate $-{1\over{u(u^2-1)}}v \,du$ in ${\bf C}(u,v)$;
$v^2=u^2-1$.

Simple inspection of the integrand (already in normal form) shows that
its poles are at $(0, i)$, $(0, -i)$, $(1, 0)$, and $(-1, 0)$.
Remember that we're now working on the Riemann surface of an algebraic
extension, so we need to specify $\it both$ $u$ and $v$ to
specify a place.

The next step is to compute the residues at each of these places,
using Theorem \ref{Trager's residue theorem}:

\begin{center}
\begin{supertabular}{l l l}
  $(0, i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, i)$     & = $i$    \cr
  $(0, -i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, -i)$   & = $-i$    \cr
  $(1, 0)$  &  $\displaystyle -2{1\over{u(u+1)}}v$ @ $(1, 0)$      & = $0$    \cr
  $(-1, 0)$  &  $\displaystyle -2{1\over{u(u-1)}}v$ @ $(-1, 0)$    & = $0$    \cr
\end{supertabular}
\end{center}

The poles with zero residues can be ignored.  We're interested in the
other two, which exist in ${\bf Q}[i]$, which can be regarded as a
vector field over ${\bf Q}$ with basis $\{1, i\}$, and we want to
construct a function whose poles and zeros match the $i$-component of
the residues (the 1-component is uniformly zero).

We start by constructing an ${\cal I}$-module generator set for the divisor
with a simple zero at $(0,i)$ and a simple pole at $(0,-i)$.  Theorem
\ref{simple pole construction} shows that:

$$f = {{v^2+1}\over{u(v+i)}} = {{v-i}\over{u}} $$

has a simple pole at $(0,-i)$.  At $(0,i)$, L'H\^opital's rule gives:

$$ \lim_{(u,v)\to (0,i)} {{v-i}\over{u}}
   = {{(v-i)'}\over{u'}} {{dv}\over{du}} = {{dv}\over{du}} = {u\over v} = 0 $$

where the last transformation was accomplished by differentiating the
mimimal polynomial.  So $f$ has a zero at $(0,i)$, and I'll note that
we've just stumbled into the solution.  Theorem \ref{simple pole
construction} already assures us that $f$ has only a single finite
simple pole, and we can see that its only zeros occur when
$v-i=0$, which, according to the minimum polynomial, can only
occur at $u=0$, thus $(0,i)$ is its only finite zero, and it is
simple, as we can verify by showing that the corresponding pole in its
inverse is simple:

$$ {1\over f} = {u\over{v-i}} = {{u(v+i)}\over{v^2+1}}
  = {{u(v+i)}\over{u^2}} = {1\over u}v + {i\over u} $$


So we've found the function we're looking for by accident.  Let's save the
general case for the next example, and convert back to
our original field:

$${{v-i}\over{u}} = x({y\over x}-i) = y - ix $$

Remembering that our residues came multiplied by a factor of $i$, we
conclude that our solution is $i\,\ln(y-ix)$, or:

\begin{eqnarray*}
\int {1\over{\sqrt{1-x^2}}} \,dx &=& i\,\ln(\sqrt{1-x^2}-ix) \\
                                 &=& -i\,\ln({1\over{\sqrt{1-x^2}-ix}}) \\
                                 &=& -i\,\ln({{\sqrt{1-x^2}+ix}\over{1-x^2+x^2}}) \\
                                 &=& -i\,\ln({\sqrt{1-x^2}+ix}) \\
                                 &=& \arcsin x \\
\end{eqnarray*}

where I used the negative of a logarithm being the logarithm of the
inverse, and the last transformation came from section
\ref{sec:Root/Logarithm/Exponential Form}.


\endexample

\vfill\eject
\mysection{Geddes's example}

\example Compute $\int {1\over{x\sqrt{x^4+1}}} \, dx$

We'll use ${\bf C}(x,y); y^2=x^4+1$ and integrate ${1\over{xy}} =
{y\over{x^5+x}}$.  Inverting this field ($z={1\over x}$) shows that
this integrand has no poles at infinity, so we can proceed directly:

$$ {y\over{x^5+x}} = {y\over{x(x+\omega)(x-\omega)(x+i\omega)(x-i\omega)}} \qquad \omega = \sqrt{i} = {\sqrt{2}\over2} + {\sqrt{2}\over2} i $$

\bigskip
\begin{center}
\begin{supertabular}{l l l}
  $(0, 1)$  &  $\displaystyle {y\over{x^4+1}}$ @ $(0, 1)$     & = $1$    \cr
  $(0, -1)$  &  $\displaystyle {y\over{x^4+1}}$ @ $(0, -1)$     & = $-1$    \cr
  $(\omega, 0)$  &  $\displaystyle 2{y\over{x(x^2+i)(x+\omega)}}$ @ $(\omega, 1)$     & = $0$    \cr
  \multicolumn{2}{l}{$(-\omega, 0), (i\omega, 0), (-i\omega, 0)\qquad\cdots$}    & = $0$    \cr
\end{supertabular}
\end{center}

We now use theorem \ref{simple pole construction} to construct a
function with a simple pole at $(0,-1)$:

$${{f(0,y)}\over{x(y+1)}} = {{y^2-1}\over{x(y+1)}} = {{y-1}\over{x}} $$

This function has a zero at $(0,1)$, but, unfortunately, it is third order,
as can be seen from either L'H\^opital's rule:

$$y^2=x^4+1$$
$$2y\,dy=4x^3\,dx$$
$${{dy}\over{dx}} = 2{x^3\over y}$$

$${{y-1}\over{x}} @ (0,1) = \lim {{dy}\over{dx}} = 2 {{x^3}\over y} = 0$$
$${{y-1}\over{x^2}} @ (0,1) = \lim {1\over{2x}}{{dy}\over{dx}} = {{x^2}\over y} = 0$$
$${{y-1}\over{x^3}} @ (0,1) = \lim {1\over{3x^2}}{{dy}\over{dx}} = {2\over3}{{x}\over y} = 0$$
$${{y-1}\over{x^4}} @ (0,1) = \lim {1\over{4x^3}}{{dy}\over{dx}} = {1\over2}{1\over y} = {1\over2}$$

\vfil\eject

\ldots or from a series expansion of $y$ at (0,1):

$$y^2 = x^4 + 1 $$
$$(y-1)^2 = x^4 - 2(y-1)$$
$$(y-1) = {1\over2}x^4 - {1\over2}(y-1)^2$$
$$(y-1) = c_0 + c_1 x + c_2 x^2 + c_3 x^3 + \cdots$$
$$(y-1)^2 = c_0^2 + (2 c_0 c_1) x + (2 c_0 c_2 + c_1^2) x^2 + (2 c_0 c_3 + 2 c_1 c_2) x^3 + \cdots$$

$$ c_0, c_1, c_2, c_3 = 0 $$
$$ c_4 = {1\over2}$$
$$ c_5, c_6, c_7 = 0 $$
$$ c_8 = -{1\over8} $$

$$ (y-1) = {1\over2} x^4 - {1\over8} x^8 + \cdots$$

$$ {(y-1)\over x} = {1\over2} x^3 - {1\over8} x^7 + \cdots$$

\ldots or from the norm:

$$N({{y-1}\over{x}}) = {{y-1}\over{x}} \cdot {{-y-1}\over{x}} = - {{y^2-1}\over{x^2}} = - {{x^4}\over{x^2}} = - x^2$$

Since we know that the function has a simple pole at $(0,-1)$, so it
must have a third order zero at $(0,1)$ to form a norm with a second
order zero.

We can eliminate the inconvenient zero by adding a constant to the
function, say 1: ${{x+y-1}\over x}$.  We can now use theorem
\ref{simple zero construction} to create a simple zero at
(0,1) by multiplying by $x+y-1$:

$${{x+y-1}\over x} (x+y-1) = {{2x-2}\over x} y + {{x^4+x^2-2x+2}\over x} $$

This function has a simple pole at (0,-1) and a simple zero at (0,1),
but does it have other poles and zeros?  If so, can it be modified to
eliminate them?  To find out, we form the generators of an ${\cal I}$-module:

$$\{ {{2x-2}\over x} y + {{x^4+x^2-2x+2}\over x}, x \}$$

Noting that ${{x^4+x^2}\over{x}} = x(x^2+1)$ and $x^2+1 \in {\cal I}$,
we can simplify this:

$$\{ {{2x-2}\over x} y - {{2x-2}\over x}, x \}$$

\vfill\eject

Using the integral basis $\{1, y \}$, we convert this to a
${\bf C}[x]$-module:

$$\{ {{2x-2}\over x} y - {{2x-2}\over x}, {{2x-2}\over x} (x^4+1) - {{2x-2}\over x} y, x, xy \}$$

and since $(2x-2){{x^4}\over{x}} = x(2x^3-2x^2)$ and
$2x^3-2x^2 \in {\bf C}[x]$, we simplify:

$$\{ {{2x-2}\over x} y - {{2x-2}\over x}, {{2x-2}\over x} - {{2x-2}\over x} y, x, xy \}$$

and write it in matrix form:

% $$\pmatrix{-{{2x-2}\over{x}} & {{2x-2}\over{x}} \cr {{2x-2}\over{x}} & -{{2x-2}\over{x}} \cr x & 0 \cr 0 & x} \pmatrix{1 \cr y}$$

$${1\over x}\begin{pmatrix}-(2x-2) & 2x-2 \cr 2x-2 & -(2x-2) \cr x^2 & 0 \cr 0 & x^2\end{pmatrix} \begin{pmatrix}1 \cr y\end{pmatrix}$$

Elementary row operations\footnote{Read right to left; $R_{i,j,\lambda}$ adds $\lambda$ times row $j$ to row $i$; $R_{i,\alpha}$ multiplies row $i$ by $\alpha$ (a unit)} $R_{4,3,x^2} R_{1,3,(2x-2)} R_{3,4,-1} R_{3,1,{1\over2}(x+1)} R_{2,1,1} $ yield:

$${1\over x}\begin{pmatrix}1 & -1 \cr x^2 & 0\end{pmatrix} \begin{pmatrix}1 \cr y\end{pmatrix} = \begin{pmatrix}{{1-y}\over{x}} \cr x\end{pmatrix} $$

so $\{ {{1-y}\over{x}}, x \} $ forms a generator set for
the ${\bf C}[x]$-module of the finite multiples of $Z(0,1)P(0,-1)$.
We convert to a basis normal at infinity: $\{1, v\} = \{1, {y\over x^2}\}$:

$$\begin{pmatrix}{{1-y}\over{x}} \cr x\end{pmatrix} = \begin{pmatrix}{1\over x} & -x \cr x & 0\end{pmatrix} \begin{pmatrix}1 \cr {y\over x^2}\end{pmatrix}
= \begin{pmatrix}x & \cr & x\end{pmatrix} \begin{pmatrix}{1\over x^2} & -1 \cr 1 & 0\end{pmatrix} \begin{pmatrix}1 \cr {y\over x^2}\end{pmatrix}$$

$$\det_{@ \infty} \begin{pmatrix}{1\over x^2} & -1 \cr 1 & 0\end{pmatrix} = 1$$

so $\{ {{1-y}\over{x}}, x \} $ is normal at infinity.  $x$ clearly has
a pole at infinity, so it can't be the function we're looking for, but
what about ${{1-y}\over{x}}$?  Switching back to $\{u,v\}$
coordinates, we obtain ${{1-y}\over{x}} = {{u^2-v}\over u}$, which has
$\{u,v\}$ poles at both $\{0,1\}$ and $\{0,-1\}$, which translate into
poles at $x=\infty$.  Therefore, no rational function on this
algebraic curve has a simple pole at (0, -1), a simple zero at (0,1),
and no other poles or zeros.

\vfill\eject

So, let's try a double pole at (0,-1) and a double zero at (0,1).  We
can just square our previous generators:
$\{ {{1-y}\over{x}}, x \} $
to obtain: 
$\{ {(1-y)^2\over{x^2}}, x^2 \} = \{ {{1-2y+x^4+1}\over{x^2}}, x^2 \}$
which simplifies to
$\{ {{1-y}\over{x^2}}, x^2 \}$.  We again check for normalcy
at infinity:

$$\begin{pmatrix}
{1\over{x^2}} & -1 \cr
x^2 & 0\end{pmatrix} \begin{pmatrix}1 \cr {y\over{x^2}}\end{pmatrix} = \begin{pmatrix}1 & \cr & x^2\end{pmatrix}\begin{pmatrix}
{1\over{x^2}} & -1 \cr
1 & 0\end{pmatrix}
\begin{pmatrix}1 \cr {y\over{x^2}}\end{pmatrix} $$

$$\det_{@\infty}\begin{pmatrix}
{1\over{x^2}} & -1 \cr
1 & 0\end{pmatrix} = 1$$

So, $\{{{1-y}\over{x^2}}, x^2\}$ is a ${\bf C}[x]$-module, normal at
infinity, containing the finite multiples of $Z^2(0,1)P^2(0,-1)$.
$x^2$ has a pole at infinity, but does ${1-y}\over{x^2}$?
Switching to $x={1\over z}; y={u\over{z^2}}; u^2 = z^4 + 1$ and
Writing it as $z^2(1-{u\over{z^2}}) = z^2 - u$ shows that it has no
zero at $(z,u) = (0, \pm 1)$, and thus no pole at $x = \infty$.  It
is, therefore, the function we are looking for:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx
   = {1\over2} \ln{{1-\sqrt{x^4+1}}\over{x^2}}$$

I'll now point out to you what's been pointed out to me, and that is a
traditional solution technique for this integral:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx$$
$$x^2 = u \qquad 2 x\, dx = du$$
$$\int {1\over{2u\sqrt{u^2+1}}} \, du$$
$$u = \tan z \qquad du = \sec^2 z dz$$
$${1 \over 2} \int \csc z \, dz = {1\over 2} \ln \tan {z\over2}$$
$$={1\over2} \ln {{\sec z - 1}\over{\tan z}}$$
$$={1\over2} \ln {{\sqrt{u^2+1}-1}\over{u}}$$
$$={1\over2} \ln {{\sqrt{x^4+1}-1}\over{x^2}}$$

\vskip 0.5in

$${{1 - \cos z}\over{\sin z}} = {{1 - \cos^2 {z\over2} + \sin^2 {z\over2}}\over{2\sin{z\over2}\cos{z\over2}}} = {{2\sin^2 {z\over2}}\over{2\sin{z\over2}\cos{z\over2}}} = {{\sin {z\over2}}\over{\cos{z\over2}}} $$

\vfill\eject

We can also proceed like this:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx$$
$$x^4 = u \qquad 4 x^3\, dx = du$$
$$\int {1\over{4u\sqrt{u+1}}} \, du$$
$$v = u + 1 \qquad dv = du$$
$$\int {1\over{4(v-1)\sqrt{v}}} \, dv$$
$$z^2 = v \qquad 2 z dz = dv$$
$$\int {1\over{4(z^2-1)z}} \, 2z\, dz$$
$$\int {1\over{2(z^2-1)}} \, dz$$
$${1\over{z^2-1}} = {1\over2}{1\over{z-1}} - {1\over2}{1\over{z+1}}$$
$${1\over4} \int {1\over{z-1}} - {1\over{z+1}} \, dz$$
$${1\over4} \ln (z-1) - \ln (z+1)$$
$${1\over4} \ln {{z-1}\over{z+1}}$$
$${1\over4} \ln {{\sqrt{v}-1}\over{\sqrt{v}+1}}$$
$${1\over4} \ln {{\sqrt{u+1}-1}\over{\sqrt{u+1}+1}}$$
$${1\over4} \ln {{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}}$$

so from the previous page and this one, we conclude

$$\int {1\over{x\sqrt{x^4+1}}} \, dx
={1\over2} \ln {{\sqrt{x^4+1}-1}\over{x^2}}
={1\over4} \ln {{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}}$$

Is this last equality true?  Well, $\ln f^2 = 2\ln f$, so
${1\over4}\ln f^2 = {1\over2}\ln f$, and\ldots

$$\Big({{\sqrt{x^4+1}-1}\over{x^2}}\Big)^2
= {{(\sqrt{x^4+1}-1)^2}\over{x^4}}$$

$${{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}} \cdot
{{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}-1}}
= {{(\sqrt{x^4+1}-1)^2}\over{x^4+1-1}}$$




\endexample

\vfill\eject
\mysection{Chebyshev's Integral}

\example Compute:
\label{Chebyshev's Integral}
$$\int {{2x^6+4x^5+7x^4-3x^3-x^2-8x-8}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \,{\rm d}x$$

The polynomial under the square root is square-free:

\begin{maximablock}
num : 2*x^6 + 4*x^5 + 7*x^4-3*x^3-x^2-8*x-8;
den: 2*x^2-1;
root : x^4+4*x^3+2*x^2+1;
factor(root);
\end{maximablock}

\ldots so $y^2 = x^4+4 x^3+2 x^2+1$; $\{1, y\}$ is an integral basis;
and our normal form for this integral is:

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)y}\over{(2x^2-1)^2(x^4+4 x^3+2 x^2+1)}} \,{\rm d}x$$

Applying now Bronstein's Hermite reduction from
section 2.1 of his ``Symbolic Integration Tutorial'' with $v=2x^2-1$
to eliminate this square in the denominator:

\begin{maximablock}
gradef(y,x,ratsimp(D(root)/2*y/root));
D(y);
U: root;
V: den;
S2: ratsimp(U*V^2*D(y/V));
\end{maximablock}

Now we want to solve $f_2 S_2 = A_2 y$ where $A_2 y$ is our numerator.

\begin{maximablock}
kill(f);
f[2] ::: num*y/S2;
T[2] ::: num(f[2]);
/* Q::: S2/y; */
Q ::: denom(f[2]);

[A,R,g] : gcdex(V,Q,x);
/* T2: num; */
[Q2, B2]: divide(T[2]*R, V, x);
h ::: A*num*y/(V*U) - (D(V)*Q2+D(B2))*y/V + Q2*D(y);

simp: false$
'integrate(num*y/den,x) = ratsimp(B2*y/V) + 'integrate(h,x);
simp: true$
\end{maximablock}

\vfill\eject
\bigskip
\begin{center}
Non-zero residues

\begin{supertabular}{r @{} l | r @{} l | r @{} l}
\multicolumn{2}{c|}{x} & \multicolumn{2}{c|}{y} & \multicolumn{2}{c}{residue} \cr
\hline
&$\sqrt{2}\over 2$ & &${1\over 2} + \sqrt{2}$ & &${5\over2}$ \cr
&$\sqrt{2}\over 2$ & $-$&${1\over 2} - \sqrt{2}$ & $-$&${5\over2}$ \cr
$-$&${\sqrt{2}\over 2}$ & &${1\over 2} - \sqrt{2}$ & &${5\over2}$ \cr
$-$&${\sqrt{2}\over 2}$ & $-$&${1\over 2} + \sqrt{2}$ & $-$&${5\over2}$ \cr
\end{supertabular}
\end{center}


$$A(x) = 1023x^8+4104x^7+5048x^6+2182x^5+805x^4+624x^3+10x^2+28x$$
$$B(x) = 1025x^{10} + 6138x^9 + 12307x^8 + 10188x^7 + 4503x^6 + 3134x^5 + 1598x^4 + 140x^3 + 176x^2 +2$$
$$C(x) = 32x^{10}-80x^8+80x^6-40x^4+10x^2-1$$

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)y}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \,{\rm d}x
= {{(x+{1\over2})y}\over{2x^2-1}} + {1\over2}\ln{{A(x)y - B(x)}\over{C(x)}}
$$


\endexample

