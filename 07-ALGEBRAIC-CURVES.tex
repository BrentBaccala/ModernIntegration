
\mychapter{Algebraic Curves}

{\bf THIS CHAPTER IS INCOMPLETE.}

Having addressed logarithmic and exponential extensions, we now turn
to the algebraic extension, which at first appears to be completely
different in character from the two transcendental cases.  The
differences stem largely from the lack of unique factorization in the
algebraic case; algebraic extensions are not, in general, unique
factorization domains.

\begin{comment}

To justify that statement, let's begin by trying to attack algebraic
extensions in the same manner as the two transcendental cases.

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i (\theta^{i})')$$

Presumably, we know how to differentiate $\theta$, so let's write
$(\theta^i)' = \sum_j \psi_{i,j} \theta^j$:

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i \sum_j \psi_{i,j} \theta^j)$$

Writing the $a_i$'s as a column vector $A$, we'll end up with a matrix equation:

$$I = A' + \Psi A$$

This is superficially similar to a Risch equation; it's a matrix Risch
equation.  To proceed in the same manner as before, we'd now have to
construct partial fractions expansions of everything and begin looking
at how cancellation might occur between our various components.  The
variety of possible cancelations between our terms seems daunting.

\vfill\eject

$$\int y\,dx \qquad y^2 = 4-x^2$$

Let's assume that our solution has the form $S = ay+b+\ln (cy+d)$, where $a$
and $b$ are rational functions in $x$ and $c$ is a polynomial in $x$.

$$S' = a' y + a y' + b' + \frac{c'y+cy'+d'}{cy+d}$$
$$2 y y' = -2x \qquad y' = -\frac{x}{y} = - \frac{x}{4-x^2}y$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}\frac{cy-d}{cy-d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})cy^2+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2y^2-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})c(4-x^2)+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c'(4-x^2) - c x)c+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2} + \frac{cd-(c' - c \frac{x}{4-x^2})d}{c^2(4-x^2)-d^2})y + b' + \frac{(c'(4-x^2) - c x)c -d^2}{c^2(4-x^2)-d^2}$$


The correct answer is

$$S = 2i\ln(ix-y)+\frac{x}{2}y$$

Instead, we'll use a different approach...

\vfill\eject

\end{comment}

There are four basic operations we perform on a rational
function in order to integrate it:

\begin{enumerate}
\item We {\bf factor} its numerator and denominator
\begin{sagecode}[ch7-intro]

load("sagecommon.sage");
R.<x> = QQ[];
n = (x-1)^2*(x+3);
d = (x-2)*(x-3);
p = n/d;
def partfrac2(n,d):
    r = partfrac(n%d,d)
    a = latex(n//d)
    a += '&'
    for k,v in r.items():
        if v < 0:
            a += "-\\frac{"+latex(-v)+"}{"+latex(k[0])+"}"
        else:
            a += "+\\frac{"+latex(v)+"}{"+latex(k[0])+"}"
    return a
def partfrac3(n,d):
    r = partfrac(n%d,d)
    a = latex(n//d).replace("x", "\\frac{{1}}{{1/x}}")
    a += '&'
    for k,v in r.items():
        if v < 0:
            a += "-\\frac{"+latex(-v)+"}{"+latex(k[0])+"}"
        else:
            a += "+\\frac{"+latex(v)+"}{"+latex(k[0])+"}"
    return a
\end{sagecode}
$$\sage[ch7-intro]{p} = \frac{\sage[ch7-intro]{n.factor()}}{\sage[ch7-intro]{d.factor()}}$$
From the factorization, we can read off the locations of the function's
zeros and poles.  In this example, our zeros are at -3 and 1 (multiplicity 2),
and our poles are at 2 and 3.
\item We construct a {\bf partial fractions expansion}
\begin{IEEEeqnarray*}{rCCL}
\sage[ch7-intro]{p} & = & \sage[ch7-intro]{partfrac2(n, d)} \\
                    & = & \sage[ch7-intro]{partfrac3(n, d)}
\end{IEEEeqnarray*}
\item We reconstruct a function given a factorization of its numerator and denominator
(or equivalently, a list of poles and zeros along with
their multiplicities)
\item We reconstruct a function given a partial fractions expansion (or
equivalently, a set of principal parts expansions
at the function's poles)
\end{enumerate}

In this chapter, we'll develop a basic set of technical tools
for working in the simplest kind of algebraic extension, an
extension of ${\bf C}(x)$.  This will prepare us for the
next chapter, where we'll study {\it Abelian integrals}, which
are integrals whose integrands are formed from polynomials and roots
of polynomials.  In other words, integrands in an algebraic extension
of ${\bf C}(x)$.

How might we handle an algebraic extension of ${\bf C}(x)$?  A crucial
property of {\it algebraic functions}, as elements of an algebraic
extension are called, is that they admit series expansions everywhere,
including infinity, so long as we allow a finite number of negative
exponents.  Such functions are called {\it meromorphic}.  The
logarithm function fails to be meromorphic at the origin, and the
exponential function fails to be meromorphic at infinity, but
algebraic functions are meromorphic everywhere, including infinity.

This means that around any specific point, we can construct a series
expansion of the integrand and integrate termwise to obtain a series
expansion for the integral.  At first this doesn't seem terribly
useful, because series expansions are infinite and we're trying to
construct closed-form solutions, but it turns out that only a finite
number of places will have negative exponents in their series
expansions and that an algebraic function is completely specified, up to an
additive constant, by the coefficients of the negative powers.

Thus, the basic strategy is first to identify the function's {\it
poles}, the places where its value becomes infinite, and compute the
{\it principal part} of the series expansions there, which are the
negative exponents and their coefficients.  This is fairly
straightforward, though there are issues of computational complexity
that make it non-trivial.  Then we integrate termwise, which is
trivial, and obtain local series expansions at the poles of the
solution.  Next, we need to reassemble this local information into a
global function (if one exists), a {\it Mittag-Leffler problem}, for
which I will present a basic algorithm in this chapter, although more
efficient techniques have been developed.

What about the logarithmic terms?  This turns out to be the most
difficult part of the problem.  We can begin to analyze them using the
same techniques, by noting that the $t^{-1}$ terms in the principal
parts of the integrand lead directly to logarithms in the integral,
and furthermore that the coefficients of these terms give us the
locations and orders of the poles and zeros in the logarithms.  This
information specifies an algebraic function up to a multiplicative
constant\footnote{Of course.  Due to the presence of a constant of
integration, we expect to specify the main part of the integral up to
an {\it additive} constand, and the logarithmic parts of the integral
up to a {\it multiplicative} constant.}, and our algorithm can be
adapted without too much trouble to handle this case.

The problem is that no algebraic function might exist that match a
given set of zeros and poles, but increasing the order of the zeros
and poles might produce a solution.  This corresponds to raising the
logarithm term to powers, i.e, $\ln f$ is the same as $\frac{1}{2} \ln
f^2$, which is the same as $\frac{1}{3} \ln f^3$, except that in our
case the lower powers might not exist in our function field, even
though higher powers do.  What powers should we use?  We could go on
raising to higher and higher powers, hoping that something will work,
but the only known algorithm to limit this search requires reducing
modulo a prime, and that requires techniques that weren't developed
until the 1960s.  Before heading into such {\it modern algebraic
geometry}, however, let's see how far we can get with the classical
algebraic geometry of the nineteenth century.

\mysection{The Topology of an Algebraic Curve}

Recall that the graph of a bivariate polynomial, $\sum a_{ij} x^i y^j = 0$, is called an {\it algebraic curve},
and will be our main focus of attention in this chapter.

% , and a
% rational function in $x$ and $y$ is called an {\it algebraic
% function}.  These will
% Note that an algebraic curve's rational functions form a field, the
% {\it function field} of the curve.

One of the first problems we face when dealing with algebraic curves
is the multi-valued nature of their solutions.  Consider the algebraic
algebraic curve $y^2 = x^2 - 1$.  For almost any given $x$,
there are two seperate $y$ values that solve this equation.
Conventionally, we
express this by writing something like $y = \pm\sqrt{x-1}$, but for
higher degree curves this kind of notation becomes unsuitable.  How,
for example, do you express the three possible solutions to a cube
root, and how do you deal with the general case where $y$ appears
multiple times in the curve's defining polynomial, something
like $y^3 + x^2y^2 - x + 4y=0$?

% TODO: clarify algebraic geometry conventions w.r.t dimension

Our solution to this problem is to regard the entire algebraic curve
as a two-dimensional surface in a four-dimensional $x$-$y$ space.  Why four
dimensions?  Well, just as in the univariate case, we find it
convenient to work with complex numbers because of their property
of algebraic closure.
Most algebraic geometers talk about
dimension with respect to the coefficient field, so we can
consistently say that an algebraic curve is a one dimensional
curve in two dimensional space.
Regarding both $x$ and $y$ as complex
numbers (two dimensions each), and plotting them against each other,
we obtain a four dimensional space.  Just as in the real case, where
an equation like $x^2 + y^2 = 1$ defines a circle, an algebraic curve
defines a surface, the loci of $x$ and $y$ that satisfy the defining
polynomial.

Once we have defined an algebraic curve, we will want to consider
{\it algebraic functions} on the curve, which are simply rational
functions (ratios of polynomials) in the two variables $x$ and $y$.
We will only be interested the values of these functions on the
curve; the curve acts as a restriction on the possible values
of $x$ and $y$.  If this seems at all puzzling, consider an integrand involving both $x$ and $\sqrt{x^2+1}$.
We'll define $y^2 = x^2+1$ and replace all instances of the square root
with $y$.  Then we'll have an integrand involving both $x$ and $y$ (the algebraic function),
but $y$ is always the square root of $x^2+1$, so it makes no sense to consider values of the algebraic
function where $y^2 = x^2+1$ is not satisfied.

Given an algebraic curve and an algebraic function, we'd like to construct a series expansion
of the algebraic function at each point of the curve.
How can we construct series expansions of rational
functions that involve both $x$ and $y$?  At each point of the
curve, we seek to find a single {\it uniformizing variable} that is
suitable for constructing power series expansions that converge in
an open neighborhood of the point.  This is also
called a {\it local uniformizer}, since no one function is a suitable
uniformizing variable at all points of an algebraic curve.

% A {\it Riemann surface} is a one dimensional connected complex manifold.

Do local uniformizing variables always exist at every point of an algebraic curve?

The answer is a qualified ``yes''.  At most points on an algebraic
curve, the answer is an unqualified ``yes'' as a result of the
Implicit Function Theorem:

\theorem
{\bf Implicit Function Theorem}
\label{implicit function theorem}

The two-dimensional complex analytic version:

Let $f(x,y)$ be an analytic
mapping of an open set $E \in {\mathbb C}^2$ into ${\mathbb C}$, such
that $f(a,b)=0$ and $\frac{df}{dy}(a,b) \ne 0$, then open sets $U$ and $V$
exist in ${\mathbb C}$ such that $a \in U$, $b \in V$, and
an analytic function $g(x)$ exists that maps $U$ into $V$ such that $f(x,g(x))=0$.

What does it mean for a ${\mathbb C}^2 \to {\mathbb C}$ function
to be analytic?

\cite{baby rudin} Theorem 9.28 is a real version of the theorem
that is based on the inverse function theorem, but only
establishes that the implicit function is continuously differentiable,
whereas we want to show that it's analytic.  Of course, for a complex
version of this proof, differentiable would immediately imply analytic.

The Wikipedia article on the implicit function theorem includes a
proof for the two-dimensional case (they only case we care about here)
that derives a differential equation that the implicit function
must satisfy and uses the Picard-Lindel\"of theorem to show its existence,
but this also only establishes continuous differentiability.

\cite{guillemin} Lecture 7 starts with a complex version of the theorem.

See {\tt https://math.stackexchange.com/questions/489789}

{\tt https://math.stackexchange.com/a/289640/71520} discusses the
difficulty of moving along the transition $R \to R^n \to C \to C^n$.

\endtheorem

This theorem applies everywhere on the curve that $\frac{df}{dy} \ne 0$,
which is everywhere except a finite number of points.  Where does it
fail?  Those points at which both $f(x,y)=0$ and $\frac{df}{dy}$
equals zero.  In other words, points at which the defining function
and its derivative with respect to one of its variables share
a zero.  At these points, the curve exhibits a behavior
called {\it ramification}.

If the derivative with respect to one variable is zero, could we try
to apply the theorem using the derivative with respect to the other
variable?  The answer is often yes, but not always.  Ramification thus
occurs with respect to a specific variable, more generally with
respect to a specific mapping.

% The defining polynomial can be regarded as a polynomial in $y$, whose
% coefficients are polynomials in $x$, simply by collecting terms with
% like powers of $y$.  If we fix a given complex value of $x$, we have a
% polynomial in $y$ with complex coefficients that can be solved as a
% univariate polynomial and yields at most $n$ solutions for $y$.  We
% can be more specific.  For any given value of $x$, we have {\it
% exactly} $n$ solutions for $y$ {\it unless} one of two things happen.
% Either the leading ($y^n$) coefficient is zero, in which case we have
% less than $n$ solutions due to having a polynomial of degree less than
% $n$ and the curve has a point at infinity, or the polynomial has
% multiple identical roots at a {\it multiple point} of the curve.

% add a box of examples

% A curve has only a finite number of multiple points, as can be seen
% by considering the {\it discriminant} of the defining polynomial,
% which is the resultant of the polynomial with its partial derivative
% with respect to one of its variables.  The discriminant will be zero
% at multiple points, so these points can be located by computing the
% zeros of a univariate polynomial.

% Multiple points are further classified according to whether or not the
% curve is locally Euclidean in their neighborhood.  Geometrically, this
% corresponds to looping around the point until you return to your
% starting point.  If a single such {\it cycle} covers all the sheets of
% the curve, the curve is locally Euclidean, and we have an {\it
% ordinary point} of the curve, albeit one with {\it ramification}, the
% {\it ramification index} being how many times we had to circle the
% point.  Otherwise, multiple cycles are required to cover all of the
% sheets, and we have a {\it singular point} or {\it singularity}.

On some curves, however, there are points, called {\it singularities},
at which the derivative with respect to both variables is zero.
One might hope to pick a uniformizing variable different
from either coordinate variable that would allow the Implicit Function Theorem
to be applied, but this turns out to be impossible.  The
derivative with respect to any algebraic function could
be expanded using partial derivatives with respect to
the coordinate variables, and the resulting derivative
would necessarily be zero:

$$\frac{df}{dz} = \frac{df}{dx}\frac{dx}{dz} + \frac{df}{dy}\frac{dy}{dz} = 0\frac{dx}{dz} + 0\frac{dy}{dz} = 0$$

So there is no variable at a singularity that meets the requirements
of the Implicit Function Theorem.  Something more is required.

Since both $f$ and $\frac{df}{dy}$ share a zero at
a given value of $x$, $f$'s zero must be at least
second order, so singularities are necessarily
multiple points of the curve.

% For infinity and/or poles, substitute z=1/x or v=1/y.

Consider a specific multiple point where $n$ sheets of the curve meet
at a single point $P$.  A small circle at distance $\epsilon$ from $P$
will map to $n$ values of the curve.  Without loss of generality,
let's assume that $P$ is the origin.

[Silverman] Theorem 10.13: If a function is analytic on every point of
a open disk $K: |z-a| < R$, then it has a power series expansion
centered on $a$ that converges everywhere in $K$.

By this theorem, we see that we can surround $P$ with disks of radius
$\epsilon$; a finite number of them will circle around $P$.  These
disks don't have to cover every point in a neighborhood of $P$; they
need only circle $P$ so that we cleanly identify a permutation
of the sheets.

Label the $r$ values of $y$ at $(x+\epsilon)$
as $y_1(x),\ldots,y_n(x)$.  As we trace along the circle
defined by $x=\epsilon e^{i\theta}$ these values
deform continuously as $\theta$ goes from $0$ to $2\pi$.
Once we reach $2\pi$, we have come full circle
and the $y$ values match up, but permuted.
Call the permutation $\sigma$,
so $y_1(x),\ldots,y_n(x)$ map to $y_{\sigma 1}(x),\ldots,y_{\sigma n}(x)$.
The permutation $\sigma$ of the $n$
sheets can decomposed into $k$ disjoint cycles.  We seek to
show that a Riemann surface can be obtained by replacing the singular
point with $k$ distinct points.

Consider a single cycle of length $r$, the {\it ramification index},
and an open disk in the $t$-plane, $\Delta=\{t:|t|<\epsilon\}$.
We wish to exhibit an bihomomorphism from $\Delta$ to the
cycle.  Set $x(t)=t^r$, which is clearly holomorphic in $\Delta$
(in fact, everywhere on the $t$-plane).  The function

$$y(t) = y_{\sigma^{\lfloor (r \, \arg t)/(2 \pi) \rfloor}}(t^r)$$

where $\arg t$ is the complex argument, with range $[0, 2\pi)$,
$\lfloor \cdot \rfloor$ is the integer floor function, and powering
$\sigma$ by an integer applies the permutation that many times.  The
permutation ensures continuity of the function at each transition
between the various $y_n$ functions.  $y(t)$ is obviously holomorphic
away from the origin, but is it also holomorphic at the origin?  Yes,
according to Riemann's theorem on removable singularities.

% where $r$ is the {\it ramification index}, and
% construct a power series, not in $(x-\alpha)$, but in
% $t=(x-\alpha)^{1/r}$, a {\it Puiseux series}.

% As a function of $t$, both $x$ and $y$ are analytic in an open
% neighborhood of $t=0$.  $x$ is analytic as a function of $t$ because
% of the I.F.T. applied to $x=t^r+\alpha$.  $y$ is analytic as a
% function of $x$ because of the I.F.T. applied to the defining equation
% of the curve.  Composition of analytic functions are analytic,
% showing that $y$ is analytic as function of $t$.

% These arguments hold in an open neighborhood of $t=0$.  Continuity of
% the roots shows that $y$ is continuous at $t=0$.  Then use existence
% of the Laurent series (Silverman 11.2) and continuity of the roots
% (HOW?) to establish analyticity at the ramification point.


\begin{mdframed}[backgroundcolor=yellow!20]
\begin{center}
{\bf Riemann removable singularity theorem}
\end{center}
\theorem
\label{Riemann removable singularity theorem}
Let $D \subset \mathbb{C}$ be an open subset of the complex plane,
$a \in D$ a point of $D$ and $f$ a holomorphic function defined on
the set $D \setminus \{a\}$.  The following are equivalent:

\begin{enumerate}
\item $f$ is holomorphically extendable over $a$.
\item $f$ is continuously extendable over $a$.
\item There exists a neighborhood of $a$ on which $f$ is bounded.
\item $\lim_{z\to a}(z - a) f(z) = 0$.
\end{enumerate}

\proof

The implications $1 \Rightarrow 2 \Rightarrow 3 \Rightarrow 4$ are
trivial. To prove $4 \Rightarrow 1$, we first recall that the
holomorphy of a function at $a$ is equivalent to it being analytic at
$a$, i.e. having a power series representation. Define

$$
h(z) = \left\{
\begin{array}{rl}
(z - a)^2 f(z) &  z \ne a ,\\
0              &  z = a .
\end{array} \right.
$$

Clearly, $h$ is holomorphic on $D\backslash\{a\}$, and there exists
$h'(a)=\lim_{z\to a}\frac{(z - a)^2f(z)-0}{z-a}=\lim_{z\to a}(z - a) f(z)=0$
by 4, hence $h$ is holomorphic on $D$ and has a Taylor series about $a$:

$$h(z) = c_0 + c_1(z-a) + c_2 (z - a)^2 + c_3 (z - a)^3 + \cdots \, .$$

We have $c_0 = h(a) = 0$ and $c_1 = h'(a) = 0$; therefore

$$h(z) = c_2 (z - a)^2 + c_3 (z - a)^3 + \cdots \, .$$

Hence, where $z \ne a$, we have:

$$f(z) = \frac{h(z)}{(z - a)^2} = c_2 + c_3 (z - a) + \cdots \, .$$

However,

$$g(z) = c_2 + c_3 (z - a) + \cdots \, .$$

is holomorphic on $D$, thus an extension of $f$.

\endtheorem
Source: copied verbatim from Wikipedia
\end{mdframed}

To see that function $y(t)$ is bounded on $\Delta$, pick a $\delta$ so
that $f(0,\delta)$ is $G$, then ensure that $\epsilon$ is small enough
to ensure that $f(x,\delta) \ne 0 \forall x \in \Delta_\epsilon$.
Since $y(t)$ is continuous on $\Delta_\epsilon$, if it were not
bounded, then $f(t^r,y(t)) = \delta$ for some value $t$, contridicting
our assumption.  Thus, $y(t)$ is bounded on $\Delta$, so it can be
holomorphically extended to the origin by the previous theorem.

{\bf Lemma.}  Given a polynomial $f(x,y)$,
then at any given point $(x_0,y_0)$ and any given real number $\delta>0$,
there exists a real number $\epsilon$ such that
$$f(x,y)\ne f(x_0,y_0) \quad\forall x,y; |x-x_0| < \epsilon; |y-y_0| = \delta$$

{\bf Proof.}  Consider $g(y) = f(x_0,y) - f(x_0,y_0)$, a polynomial in $y$ with
a zero at $y_0$.  Pick a number $a$ such that no other zero is within $\pm a$
of $y_0$.  Consider the complex circle of radius $a$ centered at $y_0$.
$g(y)$ on this circle must have a minimum value that is not zero;
call this minimum value $m$.  Now, at any point $y_0+a$,
$f(x_0, y_0+a)$ by {\bf complex} continuity will have a value $\epsilon$ such that
$x_0$ can be varied up to $\epsilon$ without changing the function by
more than $m$.  But do all of these values have a minimum
greater than zero?

In short, to obtain a complex manifold,
we need to modify our curve slightly by adding additional
points at singularities.
Theorem \ref{Riemann removable singularity theorem}
tells us that no additional information is need to specify
the behavior of holomorphic functions at those additional
points -- their behavior at an isolated point is completely determined by
their behavior in an open neighborhood surrounding that point.

What about meromorphic functions?  They can just be promoted
to holomorphic functions by multiplying them by a suitable
power of the uniformizing variable ($t$, in the above treatment),
and Theorem \ref{Riemann removable singularity theorem} again
tells us that their behavior is completely specified.

Interestingly enough, a rational function can have different values
at the same points over a singularity.

\example
Example: $y^2 = x^3 + x^2$ has a singularity at the origin,
since $f(x,y) = y^2 - x^3 - x^2$, $\frac{df}{dx} = -3x^2-2x$,
$\frac{df}{dy} = 2y$, and $\frac{df}{dx}(0,0) = 0$
and $\frac{df}{dy}(0,0) = 0$.

\begin{figure}[H]
\begin{center}
\begin{sagecode}
gray20 = (0.8,0.8,0.8);
var('x,y');
implicit_pdfplot(y^2 == x^3 + x^2, (-2,2), (-2,2), color='red', frame=True, axes=False, gridlines=True, gridlinesstyle={'color':gray20})
\end{sagecode}
\end{center}
\caption{$y^2 = x^3 + x^2$}
\end{figure}

The function $y/x$ has the value $1$ on one branch and $-1$
on the other.  It is also possible, straightforward even, to construct functions
with a zero on only one branch ($y/x - 1$) or the other
($y/x + 1$), or a pole on only one branch ($x/(y-x)$),
or a pole on one branch and zero on the other ($x/(y-x) + 1/2$).
\endexample


Analytically, both partial derivatives of the curve's polynomial are
zero at a singular point, while at least one is non-zero at ordinary
points. (PROOF)



Resolving our singularities in this manner creates a complex
manifold, but it lacks a crucial property: topological compactness.
In order to apply Theorem \ref{holomorphic functions on compact
manifolds are constant}, we need a {\it compact} manifold.
We fix this problem by embedding our algebraic curve
in projective space, using a standard construction.


Now, the coefficient of $y^n$ in the defining polynomial will be a
polynomial in $x$, which has a finite number of roots at which it is
zero, so there are only a finite number of points where the defining
polynomial is of degree less than $n$ in $y$.  As $x$ approaches one
of these points, the value of the $y^n$ coefficient approaches zero,
which causes at least one of the roots to approach infinity.  We'll
deal with these points by introducing a line at infinity, forming
{\it projective space} and creating a {\it compact} surface.

\definition
A topological space $X$ is {\it compact} if any open covering
of $X$ admits a finite subset that covers $X$.
\enddefinition

There are several crucial theorems that depend on the topological
property of {\it compactness}.  The complex plane is not compact; we
remedy this by adding a point at infinity to obtain the {\it Riemann
sphere}.  Likewise, two-dimensional complex space is not compact,
either; we remedy this by adding a line at infinity and obtaining {\it
projective space}.

Theorem: Projective space is compact

Munkres Theorem 26.2: Every closed subspace of a compact space is compact.

Theorem: The Zariski topology is the coursest topology in which
singletons are closed.

Varieties are closed in the Zariski topology.

Standard topology is finer than Zariksi, so all open sets in Zariski
are open in standard, and all Zariki-closed sets are closed in standard.

So, varieties are closed in the standard topology, and are therefore
compact in projective space.



Projective space.  Compactness.

Another highly desirable property is to be locally isomorphic to
Euclidean space.  A differentiable surface that is everywhere locally
Euclidean is called a {\it manifold}.

By adding a line at infinity and resolving our singularities, we can
coax our algebraic curve into a compact, connected, complex manifold.
The primary utility of this construction is embodied in the following
theorems.

Regular functions.  Holomorphic functions.  Identical on complex
projective varieties.

\theorem
\label{holomorphic functions on compact manifolds are constant}

Every holomorphic function $M \to C$ on a compact, connected, complex manifold $M$ is constant.
\footnote{
In fact, we don't need the complex structure and can make the stronger
statement that the only regular functions on any projective variety are the constants.
See Proposition 4.2 in
{\tt https://www.math.utah.edu/~bertram/6030/Projective.pdf},
or Hartshorne Theorem (I, 3.4a),
or {\tt https://math.stackexchange.com/questions/56236}.
If the base field is not algebraically closed, however,
new constants may appear as algebraic functions.
For example, the polynomial $(x+y)^2+1$ is
irreducible in the ring $\QQ[x,y]$, so we can use it to construct
an algebraic curve, but $x+y$ is a constant on this
curve, a square root of $-1$, in fact.
}

\proof

{\tt https://math.stackexchange.com/questions/881742}

\cite{guillemin} Lecture 2 contains a proof of the Maximum Modulus Priciple.

\endtheorem

\definition
The {\it principal part} of an algebraic function at a pole is the
portion of its corresponding Laurent series with negative exponents.
\enddefinition

\theorem
\label{algebraic functions are characterized by their principal parts}

An algebraic function on an algebraic curve is completely characterized, up to an additive
constant, by its principal parts.

\proof

Consider two algebraic functions $f$ and $g$ with identical principal
parts.  Taking the difference between them, we obtain a function $f-g$
with no principal parts, i.e, a holomorphic function.  By
Theorem \ref{holomorphic functions on compact manifolds are constant},
$f-g$ must be constant.

\endtheorem

\theorem
\label{algebraic functions are characterized by their divisors}

An algebraic function on an algebraic curve is completely characterized, up to a multiplicative
constant, by its divisor.

\proof

Consider two algebraic functions $f$ and $g$ with identical divisors.
Dividing $f/g$ we obtain a function with no poles (or zeros),
a holomorphic function.  By
Theorem \ref{holomorphic functions on compact manifolds are constant},
$f/g$ must be constant.

\endtheorem

Related: Hartshorne Corollary 6.10. A principle divisor on a complete
nonsingular curve has degree zero.

Given the importance of an algebraic function's principal parts, we will
now develop tools to calculate them.

\vfill\eject

\mysection{Puiseux Expansions}

The previous section showed that, in complex projective space,
a covering surface can be constructed for an algebraic curve,
which is isomorphic to the curve except at singularities
(where additional points must be introduced), such that:
\begin{itemize}
\item the covering surface is a complex manifold (a {\it Riemann surface}),
\item the curve's rational functions are meromorphic functions
on that manifold, and
\item the rational functions admit series expansions at
every point of the manifold.
\end{itemize}

Our next task is to compute those series expansions.

Since the rational functions in the curve's function field are
formed as rational functions in $x$ and $y$ (or whatever our
variables are named), our primary goal is to compute
series expansions for $x$ and $y$ at arbitrary points on the curve.  With such
expansions in hand, it is straightforward to construct expansions for
any algebraic function, simply by substituting in the $x$ and $y$ expansions.

% Given an algebraic function on an algebraic curve, we wish to compute
% its principal parts by locating its poles and computing series
% expansions there.

At any point where the discriminant is non-zero and $y$ obtains a
finite value, a series expansion for $y$ exists as a power series in
$(x-\alpha)$, which, as we have seen, is a straightforward application
of the Implicit Function Theorem.  At these {\it ordinary} points, we
need only postulate a Taylor series for $y$ in powers of $(x-\alpha)$,
substitute this into the curve's defining polynomial, and equate like
powers to obtain a set of equations to be solved simultaneously.
Multiple solutions will typically be found, corresponding to multiple
branches of the curve.

% EXAMPLE

At ramification points, the series expansion exists in terms
of fractional powers of $(x-\alpha)$, where the denominator
of the fractions is the ramification index.  Issac Newton,
in 1676, first proposed a method of computing the ramification index
using what are now called {\it Newton polygons}.

% Uniformizing elements.  Show that they exist, and uniquely
% define the order of a function, and the residue of a differential.

% Singular points will admit multiple Puiseux series, each one
% corresponding to a single cycle.

Let's assume that we're expanding around the point $(0,0)$, as this
simplifies the analysis with no loss of generality.  Consider
factoring the defining polynomial of the algebraic curve:

$$p_n y^n + p_{n-1}y^{n-1} + \cdots + p_0 = (y-r_1)(y-r_2)\cdots(y-r_n)$$

How might we do this, if the polynomial is irreducible?  We need to
extend to a larger field where the polynomial's roots exist.  The
analysis above shows that Puiseux series form a suitable extension.

For each root $r_i$, define its {\it order} as the lowest power of $t$
that appears in its Puiseux expansion, divided by its ramification
index.  Multiplying factors together adds their orders, so
$p_0$'s order will be the sum of all of the $r_i$'s orders.

% $p_i$ is a sum of terms, each term with $y^i$ and $n-i$ of the $r$'s
% multiplied together.  The term with the lowest order will be formed by
% multiplying the $n-i$ lowest order roots, but there may be
% cancellation between multiple sets of $n-i$ such roots.


Now let's consider increasing $i$ by one.  How does $p_0$'s order
change?  $p_1$ is formed by adding together all products of $n-1$
roots, so $p_1$'s order will be lower than $p_0$'s order by the
largest of $r_i$'s orders, unless there are multiple $r_i$'s with the
same order.  In this case, cancellation between these multiple terms
could result in $p_1$ having a larger order than otherwise expected.

If there are $j$ $r_i$'s with the same largest order, increasing $i$
by $j$ will lower $p_i$'s order by $j$ times that largest order.

The Newton polygon is formed by plotting the orders of the $p_i$
coefficients, with $i$ varying along the horizonal axis and the order
plotted vertically.  The easiest way to do this is to plot the powers
of the monomials that appear in the equation, and construct the
polygon's lower convex hull.

Thus, a segment on the lower convex hull of the Newton polygon will
correspond to as many solutions as the width of the line segment, each
with order equal to the change in height divided by the width, i.e,
the negative slope of the line segment.  The denominator of the slope
will be the ramification index, and the numerator of the slope will be
the lowest exponent expected in the expansion of $y$.

Consider a Puiseux series
corresponding to a single line segment of the Newton polygon.
Letting
$\alpha$ be the $x$ exponent and $\beta$ be the $y$ exponent, so the
monomials in $f$ have the form $x^\alpha y^\beta$, then the equation
of the line segment is $r\alpha + s\beta = p$, where $r$, $s$, and $p$
are integers and $r$ and $s$ are relatively prime.  Making the
substitution $x=t^r$ and $y=t^s u(t)$, we obtain:

$$f(x,y) = \sum A_{\alpha\beta} x^\alpha y^\beta$$
$$ = \sum A_{\alpha\beta} t^{r \alpha} t^{s \beta} u(t)^\beta$$
$$ = t^p \underbrace{\sum A_{\alpha\beta} t^{r \alpha + s \beta - p} u(t)^\beta}_{g(t,u)}$$

At least two of the $(r \alpha + s \beta - p)$ exponents will be zero
(those monomials corresponding to the endpoints of the line segment on
the Newton polygon); all of the remaining exponents will be positive.
This means that if we expand $u(t)$ in a power series in $t$:

$$u(t) = u_0 + u_1 t + u_2 t^2 + u_3 t^3 + \cdots$$

then any power of $u(t)$ will have the form:

$$u(t)^\beta = U_0(u_0) + U_1(u_0, u_1) t + U_2(u_0, u_1, u_2) t^2 + U_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

and $g(t,u)$ will also have the form:

$$g(t,u) = G_0(u_0) + G_1(u_0, u_1) t + G_2(u_0, u_1, u_2) t^2 + G_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

In order for $g(t,u)=0$ at $t=0$, $G_0(u_0)$ must be zero, and since $G_0(u_0)$ is a polynomial
in $u_0$, this gives us a finite number of values for $u_0$ that can solve our equation.

Now, by setting $g(t,u)=0$, can we obtain $u(t)$ as a function of $t$?

The Implicit Function Theorem states that we can, if $\frac{\delta g}{\delta u}$ is not zero
at the point we wish to expand around.

$$\frac{\delta g}{\delta u}(0,u_0) = \frac{\delta}{\delta u} G_0(u_0)$$

In short, the roots of $G_0(u_0)$ give us the starting values for our series expansion,
and if the roots are simple, then the Implicit Function Theorem guarantees that we'll
have a unique series expansion for $u(t)$ as a function of $t$.  If any of the roots
are not simple, then we can repeat this procedure for $g(t,u)$.  It can be shown
(\cite{bliss} \S 15) that this procedure always terminates.

\vfill\eject

\begin{sagecode}
from sage.geometry.newton_polygon import NewtonPolygon
from sage.geometry.polyhedron.constructor import Polyhedron

def newton_polygon(f, x, y, x0, y0):
    vertices = [(b,a) for a,b in f.subs({x:x-x0, y:y-y0}).exponents()]
    vertices.sort(key=lambda a: (a[0], -a[1]))
    polyhedron = Polyhedron(base_ring=QQ, vertices=vertices)
    return NewtonPolygon(polyhedron)

R.<x,y> = QQ[];
\end{sagecode}

\example\label{y^2 = 1 - x^2}
Construct Puiseux expansions of $y$ at the multiple points of the
curve $y^2 = 1 - x^2$

We normalize the defining polynomial by writing it as
$y^2 + x^2 - 1 = 0$.  Where does it have multiple points?
We compute the discriminant:

\begin{sageblock}
R.<x,y> = QQbar[];
(y^2 + x^2 - 1).discriminant(y).factor()
\end{sageblock}

The multiple points of $y^2 = 1 - x^2$ lie at the roots of the
discriminant, which are $x = \pm 1$.  In both cases, $y=0$ is the only
solution, so the curve has multiple points at $(x,y)=(\pm 1, 0)$.  The
partial derivative of the defining polynomial with respect to $x$
is $2x$, which is non-zero, so neither of these multiple
points are singular; we'll get ramification instead.
The analysis is almost the same in both cases, so I'll just do $(1,0)$.

First, construction of the Newton polygon requires recasting the
curve's polynomial into a form centered about the point being
analyzed, i.e, $y^2 + (x-1)^2 + 2(x-1) = 0$.  Next, we construct the Newton
polygon by plotting the monomial powers, putting the $y$ exponents on the horizontal axis and the
$(x-1)$ exponents on the vertical:

\begin{figure}[H]
\begin{center}
\begin{sagecode}
f = y^2+x^2-1;
x0 = 1;
y0 = 0;
np_vertices = [(b,a) for a,b in f.subs({x:x-x0, y:y-y0}).exponents()];
np_vertices.sort(key=lambda a: (a[0], -a[1]));
np = NewtonPolygon(np_vertices);
#np = newton_polygon(f, x, y, x0, y0)
p1 = plot(np, thickness=2, aspect_ratio=1, frame=True);
p2 = points(np_vertices, size=50);
pdf(p1+p2, axes=False, gridlines=[[], []], xmax=3, ymax=3, ticks=[[0,1,2,3],[0,1,2,3]])
\end{sagecode}
\end{center}
\end{figure}

The only segment on the Newton polygon's lower convex hull has slope
$-1/2$ and width 2, telling us that two of our roots (the width of the
segment) will require a single Puiseux series with ramification index
2 (the denominator of the slope):

$$x=t^2+1$$

We know that y can be expressed as a power series in $t$ with
initial exponent 1 (the numerator of the slope):

$$y= a_1 t + a_2 t^2 + a_3 t^3 + \cdots$$

Now, substituting these expressions for $x^2$ and $y^2$ into the
curve's defining equation $y^2 + x^2 - 1 = 0$ and setting all
coefficients of $t$ to zero, we find:

\begin{sageblock}
var('x, y, t, a1, a2, a3');
f = y^2 + x^2 - 1;
exp = f.subs({x: t^2+1,
              y: a1*t + a2*t^2 + a3*t^3});
exp.collect(t)
\end{sageblock}

$$2 + a_1^2 = 0 \qquad 2 a_1 a_2 = 0 \qquad 1 + 2 a_1 a_3 + a_2^2 = 0$$

The first equation tells us that $a_1 = \pm\sqrt{2}i$,
the second equation tells us that $a_2=0$ and the
third equation tells us that $a_3 = \pm \frac{\sqrt{2}}{4} i$, so

% $$a_0=0; \qquad a_1 = \pm\sqrt{2}i; \qquad a_2 = 0; \qquad a_3 = \pm \frac{\sqrt{2}}{4} i$$

\begin{equation}
\label{(1,0) expansion}
x = t^2 +1; \qquad y = \pm\left[ \sqrt{2}it + \frac{\sqrt{2}}{4} it^3 + \cdots \right]
\end{equation}

It would seem that we have two different series to chose from.  This
is not really the case, as they differ by only a $180^\circ$ rotation
in the t-plane, as can been seen by substituting $t=-t$, which
transforms one of the y-series into the other, while leaving the
x-series unchanged.


Now, let's analyze the point at infinity.  We move infinity to a
finite point (0) with the substitution $x=u^{-1}$, then combine all of
our terms over a common denominator and discard the denominator.  Our
curve becomes:

$$y^2 u^2 + 1 - u^2 = 0$$

\begin{figure}[H]
\begin{center}
\begin{sagecode}
R.<u,v> = QQ[];
f = v^2 * u^2 + 1 - u^2;
u0 = 0;
v0 = 0;
np_vertices = [(b,a) for a,b in f.subs({u:u-u0, v:v-v0}).exponents()];
np = NewtonPolygon(np_vertices);
p1 = plot(np, thickness=2, aspect_ratio=1, frame=True);
p2 = points(np_vertices, size=50);
pdf(p1+p2, axes=False, gridlines=[[], []], xmax=3, ymax=3, ticks=[[0,1,2,3],[0,1,2,3]])
\end{sagecode}
\end{center}
\end{figure}

The Newton polygon's lower convex hull has a single line segment,
slope $1$, length $2$, telling us that we'll have two separate
poles, each with ramification index 1.  Thus, $u$ can be used
directly as a uniformizing variable, and we postulate an expansion for
$y$ in the form:

$$y = a_{-1} \frac{1}{u} + a_0 + a_1 u + a_2 u^2 + a_3 u^3 + \cdots$$

Plugging this into $y^2 u^2 + 1 - u^2$ and setting all the resulting
coefficients to zero, we conclude:

\begin{sageblock}
var('u, y, t, a0, a1, a2, a3');
var('an1', latex_name='a_{-1}');
f = y^2*u^2 + 1 - u^2
exp = f.subs({y: an1*(1/u) + a0 + a1*u + a2*u^2 + a3*u^3});
exp.collect(u)
\end{sageblock}

%% $$a_{-1}^2 + 1 = 0; \qquad 2 a_{-1} a_0 = 0 \qquad (2a_{-1}a_1+a_0^2-1)=0$$

$$a_{-1} = \pm i; \qquad a_0 = 0; \qquad a_1 = \mp \frac{1}{2}i; \qquad a_2 = 0; \qquad a_3 = \mp \frac{1}{8}i$$

$$y = \pm i \frac{1}{u} \mp \frac{1}{2} i u \mp \frac{1}{8} i u^3 + \cdots$$

This time, there is no ramification, since $u$, and not a power of
$u$, is $\frac{1}{x}$.  We actually have two distinct series that will
yield two different values of $y$ for each value of $u$.
Geometrically, we have two sheets that approach each other and touch
at a singular point where the curve is not locally Euclidean,
in a manner somewhat like this:

\begin{center}
\includegraphics[width=0.5\textwidth]{07-ALGEBRAIC-CURVES-EXAMPLE1.pdf}
\end{center}

\endexample

We've seen how to construct Puiseux expansions at arbitrary points of
an algebraic curve, but some points have multiple expansions,
corresponding to multiple cycles on their corresponding surfaces.
These points are the curve's {\it singularities}, which are the
only points where the curve is not locally Euclidean, and behaves
instead like in the graphic above, where several Euclidean sheets
touch at a single point.

We now seek some mechanisms for distinguishing between multiple cycles
at a single point.  To this end, we introduce the concept of a {\it
place}.  Intuitively speaking, a place is a cycle, and places are in
one-to-one correspondence with cycles.  Therefore, we
can handle singularities by thinking in terms of places.
Non-singular points have a unique place associated
with them, while there are multiple places (and multiple cycles)
associated with a singular point.

\vfill\eject
\mysection{Valuation Rings and Orders}

Driven in no small part by the difficulty in visualizing higher dimensional geometric shapes,
mathematicians have developed increasingly algebraic techniques to manipulate geometric objects.
In particular, the techniques of the previous section were presented largely for educational purposes,
as they are now considered obsolete.
The current state of the art is to use the tools of abstract algebra developed in the early
twentieth century, such as rings, ideals, and fields.

We can't use ideals directly in a function field, or any field for that matter, because
there are only two ideals in any field -- the zero ideal, which is the ideal containing
only the zero element, and the unit ideal, which is the entire field.  This
follows directly from the invertability of field elements.  As all field element possess
inverses, any non-zero ideal generator can be multiplied by its multiplicative inverse
to generate 1, which then generates the entire field.

The obvious choice would be to use ideals of the {\it coordinate ring}, which is simply
the polynomial ring modulo the defining polynomial of the curve, but these ideals
are in one-to-one correspondence with the {\it points} of the curve, and we need
some way to represent {\it places}.

To solve this problem, we introduce the concept of {\it valuation rings}
and decompose the function field into {\it orders}, of which
there are two of primary interest: the maximal finite order and the maximal infinite order.
The ideals of the maximal finite order correspond to
finite places, and the ideals of the maximal infinite order correspond to infinite places.
These maximal orders can be constructed
using the algebraic process of {\it normalization}.
Using one or the other, we obtain ideals that represent all places in the function field.

\begin{definition}
The {\bf coordinate ring} of the algebraic curve is the ring $R[x,y] \mod p(x,y)$.
If $p(x,y)$ is irreducible, then the coordinate
ring is an integral domain.  (what is $R$?)
\end{definition}

What do we do if $p(x,y)$ is not irreducible?  In this case, we're trying something
like computing an integral with a root of a polynomial that can be factored.  We factor the
polynomial, producing two separate roots.  We then use the theorem of the
primitive element (NEED TO PUT IT IN CHAPTER 2) to construct a single algebraic
extension defined by a single irreducible polynomial in which we can construct
both roots.  So, for the remainder of this discussion, we need only consider the case
where $p(x,y)$ is irreducible.

\begin{definition}
The fraction field of the coordinate ring is called the curve's {\bf function field},
and its elements are called {\bf algebraic functions}.
\end{definition}

A curve's function field contains all rational functions in $x$ and $y$, grouped together into equivalence
classes by the relation $p(x,y)=0$.

\begin{definition}
A {\bf valuation ring} of a function field $F$ is a subring $O \in F$ such that
   %% we don't need this in the definition (I think)
   %% $K \in O \in F$ (both proper inclusions)
   for every $z \in F$, either $z \in O$ or $z^{-1} \in O$
\end{definition}

{\bf Note:} A valuation ring might not be {\bf discrete valuation ring}, which is
a valuation ring with value group isomorphic to the integers under addition.

Both ideals and valuation rings are subrings, but there are two different conditions
that they must satisfy.  Fields admit only two ideals (the unit ideal and the trivial ideal),
but there are typically an infinite number of valuation rings within a given function field.

We now wish to show that valuation rings are in one-to-one correspondence with places.
The key observation is that an element $z$ in is a valuation ring $O$ only if $z$ has
non-negative valuation (i.e, is finite) at the corresponding place.

The condition that either $z$ or $z^{-1}$ must be in the valuation ring does not preclude
the possibility that both will be in the valuation ring.  In fact, these are precisely
the elements of valuation zero, that have neither a zero nor a pole at the place
in question.  The elements of $O$ whose inverses are not in $O$ are the elements of positive
valuation, that have a zero at the place in question.  They form an ideal $P$ of the
valuation ring.  $O$ is, in fact, a {\it local ring}, and $P$ is its unique maximal ideal.

\begin{theorem}
The valuation rings of a curve's function field are in one-to-one correspondence with its places.

\proof
\begin{enumerate}
\item Every cycle induces a valuation ring

To do this, we just use the Puiseux expansion of a function field element to compute its valuation.
If it is finite, it is in the valuation ring; if it is a pole, it is not in the valuation ring.

Show that valuation of the Puiseux expansion is independent of the choice of uniformizing variable

Show compatibility of the Puiseux expansions -- inverting one inverts the valuation

\item Every valuation ring induces a cycle

How to compute a Puiseux expansion from a place?  Show that a valuation ring $O$ is a local ring
with a unique maximal ideal $P$.  Every element $t \in P$ such that $P=tO$ is a uniformizing parameter.

This is where we need to have a {\it discrete} valuation ring.  \cite{Swanson Huneke} proposition 6.3.4
shows that a DVR is a principal ideal domain (and other equivalences).
\cite{Stichtenoth} Theorem 1.1.6 is relevant, as it shows that the valuation ring of a function
field is a DVR.  \cite{Swanson Huneke} Theorem 6.4.3 suggests that the Noetherian property
(of the coordinate ring?) is what we need.

Show that $O \mod P$ is isomorphic to the constants.  $x \mod P$ and $y \mod P$ give us the coordinates
of the point corresponding to the place.

Given an element in $O$, mod out by $P$ to find its value.  Subtract this value to get an element in $P$.
Divide by a uniformizing parameter.  Repeat to construct a Puiseux expansion.

Show that all elements have a well-defined valuation ($P^v$ contains them but $P^{v+1}$ does not).

If the element is not in $O$, its inverse must be, show that its inverse respects valuation and we can
multiply by $t^{-v}$, where $t$ is a uniformizing parameter, to move the inverse into $O$.

\item Need to show that different cycles yield different valuation rings

If the coordinates of the cycles are different, $x-x_0$ (or $y-y_0$) will have valuation zero at
one cycle, but positive valuation at the other.

Otherwise, if the coordinate of the cycle is the same, then at least one of $x$ or $y$ must have
different Puiseux expansions.  Subtract the common prefix and the first differing term from $y$,
i.e, $y-y_0-c_1 t-c^2 t^2-\cdots-c_v t^v$ to obtain a function that has positive valuation
on the one cycle but zero valuation on the other.

\end{enumerate}
\end{theorem}

Since there are an infinite number of valuation rings, it is most convenient to separate them
into two sets (the finite and the infinite), each of which can be represented as ideals
of a particular subring (an order).

\begin{definition}
A valuation ring $O$ is {\bf finite} if both $x \in O$ and $y \in O$.  Otherwise, it is {\bf infinite}.
\end{definition}

\begin{definition}
An {\bf order} $\mathcal {O}$ of a ring $R$ (also called an $R$-order) is a subring of $R$ such that
\begin{enumerate}
\item $\mathcal {O}$ is a finite-dimensional algebra over the field $\mathbb {Q}$ of rational numbers
\item $\mathcal {O}$ spans $R$ over $\mathbb {Q}$, and
\item $\mathcal {O}$ is a $\mathbb {Z}$-lattice in $R$.
\end{enumerate}
[wikipedia]
\end{definition}

\begin{definition}
The {\bf maximal finite order} of a function field $F$ is the intersection of all its finite valuation rings.
\end{definition}

Show that it's an order.

Show that valuation rings are maximal ideals of the order.  (are there other maximal ideals?)

Show that it's identical to the integral closure of the coordinate ring.

\begin{definition}
The {\bf maximal infinite order} of a function field $F$ is the intersection of all its infinite valuation rings.
\end{definition}

Show that it's an order.

\begin{definition}
The {\bf integral closure} of an integral domain $R$ in its field of fractions $F$
are the elements of $F$ with no finite poles, or that admit monic defining polynomials.
\end{definition}

Theorem: Every element of an $R$-order is integral over $R$.

Theorem: The integral closure of an integral domain $R$ in its field of fractions is an $R$-order,
and in fact is maximal (due to the last theorem).

\begin{mdframed}[backgroundcolor=yellow!20]
We might need all this stuff to show that integral closure is the same as the intersection
of all the finite valuation rings.

\cite{Swanson Huneke} Proposition 6.8.14 Let $R$ be an integral domain. Then
the integral closure of the ring $R$ equals $\cap_V V$ , where $V$
varies over all the valuation domains between $R$ and its field of
fractions. If $R$ is Noetherian, all the $V$ may be taken to be
Noetherian.

\cite{kollar} Definition 1.23
Let $S$ be an integral domain with quotient field $Q(S)$.
The {\it normalization} of $S$ in $Q(S)$, denoted by $\overline{S}$,
is the unique largest subring $\overline{S} \subset Q(S)$
such that every homomorphism $\phi: S \to R$ to a DVR
extends to a homomorphism $\overline{\phi}: \overline{S} \to R$.

\cite{kollar} Lemma 1.24
A unique factorization domain is normal.  In particular,
any polynomial ring over a field is normal.

\cite{kollar} Lemma 1.25.
Assume that $t \in Q(S)$ satisfies a monic equation
with coefficients in $S$ (i.e, $t$ is integral).
Then $t \in \overline{S}$.

\cite{kollar} Defintion 1.27.
Let $S$ be an integral domain.  The {\it normalization} of $S$
is its integral closure in its quotient field

\cite{kollar}: ``The easy argument that every normal integral domain $S$
is the intersection of all the valuation rings sitting between
it and its quotient field is given in [AM69, 5.22].  Working
only with discrete valuation rings is a bit harder.  The strongest
theorem in this direction is Serre's condition for normality;
see [Mat70, 17.1] or [Mat89, 23.8].''

[Mat89]:

$R_i$ condition: $A_p$ is regular for all $P \in {\rm Spec } A$ with ${\rm ht } P \le i$

$S_i$ condition: depth $A_p \ge {\rm min}({\rm ht } P, i)$ for all $P \in {\rm Spec } A$

$(S_0)$ always holds. $(S_1)$ says that all the associated primes of $A$ are minimal.
$(R_0)+(S_1)$ is n.a.s.c. for $A$ to be reduced.

Theorem 23.8: $(R_1)+(S_2)$ are n.a.s.c. for a Notherian ring $A$ to be normal.
\end{mdframed}

\begin{mdframed}[backgroundcolor=cyan!20]
What do we need this for?

A {\bf Dedekind domain} is a integral domain in which all ideals factor into a product of prime ideals.

Theorem: such a factorization is necessarily unique.

[Wiki Dedekind domain] Theorem: Let R be a Dedekind domain with
fraction field K. Let L be a finite degree field extension of K and
denote by S the integral closure of R in L. Then S is itself a
Dedekind domain.

Theorem (Kummer): If $\{1,y,...,y^{n-1}\}$ is a local integral basis
at some prime polynomial $\mathfrak{p}$ in $x$, then we can factor
the ideal $\mathfrak{p}^e$ by factoring the field's defining
polynomial mod $\mathfrak{p}$. [Stichtenoch Theorem 3.3.7]

An {\bf Artinian ring} satisfied the descending chain condition on ideals.

Theorem: the modulo ring constructed from a Dedekind domain and a proper ideal is an Artinian ring.
\end{mdframed}

Show that the maximal finite order is a finitely generated $R[x]$-module.

To work with the maximal finite order of an algebraic curve, or the integral closure of the coordinate
ring (it's same thing), we'd like to compute a basis for the order as an $R[x]$-module, since
the basis is finite.  Computing the integral closure of a curve's coordinate ring is not
trivial, but algorithms to do this have been known for over a hundred years.  Chapter 5
in \cite{alvanos} describes Trager's algorithm, which was originally published
in \cite{trager}.

Once we have a basis for the maximal finite order, we can represent places as ideals within the order
using their ideal generators, of which there are also a finite number.  This gives us the most convenient
way of working with places on an algebraic curve.

All places correspond to a maximal ideal in the ring of integral elements
That ideal consists of all algebraic functions which are zero at that place and have no finite poles.
Equivalently, it consists of all integral elements (that satisify a monic polynomial) which are
zero at that place.
Two different places, even if they're at the same point, correspond to two distinct maximal ideals of the order.

Given a valuation ring $O$ and its maximal prime ideal $P$, we define the {\bf residue class field} of
the valuation as $F_P := O/P$.  Given any element of $O$, its value at the corresponding place is its
image in $F_P$.  The easy case in when the field of constants is algebraically closed.  Otherwise,
we can have non-rational places whose residue fields have degree (over the constants) greater than one.

As a basic (and important) example of the technique, I'll show how to evaluate an algebraic function
at a place, given a representation of the place as an ideal of an order.

\begin{mdframed}[backgroundcolor=yellow!20]
Given: a rational function $f$ and a prime ideal $I$ in a maximal order $O$.

Goal: compute the value of $f \bmod I$.

Step 0: Precompute a $k[x]$-module basis for $I$ in Hermite normal form (doable since $I$ is finitely generated over $O$ and $O$ is finitely generated over $k[x]$) and $\alpha$, a rational function with a simple pole at the place corresponding to $I$ and no other finite poles.

Step 1: Compute the valuation of $f$'s denominator; call it $\nu$.  Multiply both $f$'s numerator and denominator by $\alpha^\nu$.  Now both the numerator and denominator are in $O$, and the denominator is not in $I$ (i.e, it has a finite value).  So we've reduced to the case of computing the residue of an element of $O$, as we now can divide by the denominator's residue, since we know it's not zero.

Step 2: Find a primitive element of the residue field; call it $g$.

Step 3: Compute $g$'s minimal polynomial and use it to construct the residue field as an algebraic extension of the constant base field.

Reducing an element $f \in O$ modulo the HNF basis of $I$ gives a $k[x]$-vector that represents $f \bmod I$, i.e, when this $k[x]$-vector is multiplied by the HNF basis vector, we obtain an element of $O$ equivalent (mod $I$) to $f$.

We can represent $g$ and its powers in this form, and construct a matrix that converts from $g$-basis to HNF basis, then invert it to obtain a matrix that converts from HNF basis to $g$-basis.

Use reduction mod $I$'s HNF basis, then multiply by this inverse matrix to obtain an element in the residue field.
\end{mdframed}

At a non-singular point, the (x-a, y-b) ideal exists in the coordinate ring.  How can we
show that it is prime in the integral closure?

All points correspond to maximal ideals in the coordinate ring.
For points, our ideal generators are just $x-x_0$ and $y-y_0$, and this ideal is maximal in the {\it coordinate ring}.
If $(x_0, y_0)$ was {\it not} on the curve, then the ideal would be $(x-x_0, y-y_0, p(x,y))$, which
would simplify to $p(x,y)$ being a non-zero constant, and this would generate the unit ideal.

At a singularity, we can construct a function that takes on different values on each cycle.
How does this lead to multiple ideals in the integral closure?

\cite{Stichtenoth} Corollary 1.2.3 shows that
all places of $C(x)$ (the rational function field) are in 1-to-1 correspondence with $C \cup \inf$ (as places).

We have the function field $C(x)$ and its places, and also the function field defined by the curve,
and its places.  Since all the rational functions in $C(x)$ are also rational
functions in $C(x,y) \mod p(x,y)$ (the coordinate ring), it follows that
the function field defined by the curve is a field extension of the rational function field.

\begin{enumerate}
\item All places of an extension are ``over'' a place of the underlying field
\item $v_{P'}(x) = e\cdot v_P(x)$, so $e$ is the ramification index (and is a finite integer)
\end{enumerate}

How do we relate this to the geometric picture?  We've already seen ramification in the previous section.

\begin{mdframed}[backgroundcolor=cyan!20]
Do we need this discussion here?  Only if we need to compute ramification indices, I think.

[Wiki Ideal] Theorem: if $f: A \to B$ is surjective and $ \mathfrak {a}\supseteq \ker f$ then:

\begin{itemize}
\item $\mathfrak {a}^{ec}={\mathfrak {a}}$ and ${\mathfrak {b}}^{ce}={\mathfrak {b}}$
\item ${\mathfrak {a}}$ is a prime ideal in $A \Leftrightarrow {\mathfrak  {a}}^{e}$ is a prime ideal in $B$.
\item ${\mathfrak {a}}$ is a maximal ideal in $A \Leftrightarrow  {\mathfrak  {a}}^{e}$ is a maximal ideal in $B$.
\end{itemize}

So, to factor $\mathfrak{p}$, we need to find all of the prime/maximal ideals
in $R \mod \mathfrak{p}$.

Constructing $R \mod \mathfrak{p}$ as a finite dimensional algebra,
we have an algorithm to enumerate all of the algebra's maximal ideals.
This is the same as the ideal decomposition algorithm.

We also want to find each ideal's ramification index and relative degree.
I wrote a paper on this.

The ramification indices are the powers of the prime ideals.  Cohen Theorem 4.8.3:

$$p {\mathbb Z}_K = \prod_{i=1}^{g} {\mathfrak p}_i^{e_i}$$

Stichnoch defines the ramification index as the integer such
that $\nu_{P'}(x) = e \nu_P(x)$ for all $x$ in the base field.
($P'$ lies over $P$)

We can find the ramification index in the modulo ring by raising each maximal
ideal to successive powers and determining when it stabilizes.  The Artinian
condition guarantees that it will eventually stabalize.

The relative (or residual) degree of ${\mathfrak p}$ is defined (Cohen Definition 4.8.4):

$$f_i = [{\mathbb Z}_K/{\mathfrak p}_i : {\mathbb Z}/p{\mathbb Z}]$$

This seems to be the nullity of the matrix that defines the ideal in the algebra.

Why?  The dimension of the algebra (over ${\mathbb Z}/p{\mathbb Z}$)
is the dimension of ${\mathbb Z}_K$.  The number of basis elements in
the ideal is the dimension of ${\mathfrak p}_i$.  The dimension of
${\mathbb Z}_K/{\mathfrak p}_i$ is the difference of these two numbers
(the dimension of the algebra minus the dimension of the ideal), which
is the dimension of the vector space minus the dimension of the
ideal's basis matrix's kernel, or the nullity of this matrix.
\end{mdframed}


%\mysection{meromorphic functions are analytic}
%
%first, trace of a meromorphic function is meromorphic on C(x), and is
%thus a rational function
%
%Liouville's theorem: a bounded entire function is constant
%
%Proof A: (Silverman) use a Taylor series expansion around z=0, which
%is valid in the entire plane (since the function is entire).  Cauchy's
%inequality $|f| \le M ==> |c_n| \le M/{R^n}$ (eq. 10.8') as R->infty
%implies that the function is constant.
%
%Lemma: A entire function with no singularities, even at infinity, is
%constant.
%
%Proof: We can do a Taylor series expansion at the origin, whose
%non-zero terms will correspond to the principal part of the expansion
%at infinity, which must therefore be zero.
%
%Next: A entire function with only a pole at infinity is a polynomial.
%The principal part at infinity will be a polynomial.  Subtract it out
%to get a function with no singularities, which must be constant.
%
%Next: Given a function with only a finite number of finite poles,
%multiply it by a polynomial (the denominator) matching the poles with
%zeros.  Now we've got a function with only a pole at infinity, which
%must be a polynomial (the numerator).

\begin{comment}

\example
Compute expansions at all multiple points of
the exercises in \cite{bliss} \S 68.

To facilitate this example, let's define an
auxiliary function to perform the analysis:

\begin{sageblocksmall}
def analyze_multiple_points(f, pr=False):
   P = f.parent()
   Base = P.base_ring()
   x,y = P.gens()
   # This next code is here to avoid Trac #25271, though it assumes
   # that the curve's polynomial has only rational coefficients.
   # (which is true for all of our test cases)
   if False:
      disc = f.discriminant(y)
   else:
      QQR = QQ[x,y]
      disc = P(QQR(f).discriminant(QQR(y)))
   if pr: print '$$', latex(disc.factor()), '$$'

   for x0 in Base[x](disc).roots(multiplicities=False):
      sheets = 0
      for y0 in Base[y](f.subs({x: x0})).roots(multiplicities=False):
          p = puiseux(f, x0, y0, 3)
          for cycle in p:
             assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
             if cycle[0] == x0:
                sheets = sheets + 1
             else:
                sheets = sheets + (cycle[0] - x0).valuation()
          if pr: print '$$', latex((x0,y0)), '$$'
          if pr: print '$$', latex(p), '$$'
      if f.subs({x: x0}).degree(y) != f.degree(y):
          p = puiseux(f, x0, oo, 3)
          for cycle in p:
             assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
             if cycle[0] == x0:
                sheets = sheets + 1
             else:
                sheets = sheets + (cycle[0] - x0).valuation()
          if pr: print '$$', latex((x0,oo)), '$$'
          if pr: print '$$', latex(p), '$$'
      assert(sheets == f.degree(y))

   finf = Base[y](f.subs({x: 1/x}).numerator().subs({x:0}))
   for y0 in finf.roots(multiplicities=False):
      p = puiseux(f, oo, y0, 3)
      for cycle in p:
         assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
      if pr: print '$$', latex((oo,y0)), '$$'
      if pr: print '$$', latex(p), '$$'
   if finf.degree() < f.degree(y):
      p = puiseux(f, oo, oo, 3)
      for cycle in p:
         assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
      if pr: print '$$', latex((oo,oo)), '$$'
      if pr: print '$$', latex(p), '$$'
\end{sageblocksmall}

\begin{sageblock}
test_curves = [
    y^3 - 3*y + 2*x,
    y^3 + 3*y - x,
    y^3 - 3*y^2 - x,
    y^4 - 4*y - x,
    y^4+2*(1-2*x)*y^2+1,
    y^3-3*y^2+x^6,
    y^3-3*y+2*x^2*(2-x^2),
    y^3-3*y+2*x^3*(2-x^3),
    3*x*(x-1)*y^4 -4*(x-1)*(x-2)*y^3 + (4/27)*(x-2)^4,
    y^5 + (x^2-1)*y^4 - (4^4)/(5^5)*x^2*(x^2-1),
    y^3 - x*y - x^2,
    y^3 - 3*x^2*y + 2*x,
    y^3 - 3*x*y + 2*x^2,
    y^3 - 3*y + x^6];

# for f in test_curves:
#     analyze_multiple_points(f)
\end{sageblock}

%\begin{sageblocksmall}
%analyze_multiple_points(y^3-3*y+2*x^3*(2-x^3), True);
%\end{sageblocksmall}

$$y^3 - 3axy + x^3$$

This function contains an extra variable and is, in fact, a family of
algebraic curves.

\endexample

\end{comment}

% A {\it Riemann surface}, more precisely, is a two-dimensional manifold
% with a {\it complex analytic structure}.

\vfill\eject
\mysection{Sage's FunctionField code}

Sage has built-in routines to compute Puiseux expansions without having to
construct Newton polygons and substitute trial expansions.
In fact, there are three different
constructions in Sage for algebraic curves -- polynomial rings
modulo an ideal, function fields, and curves in projective space.
We'll mostly use the {\tt FunctionField} code.

Instead of constructing a polynomial ring the usual way (which is?), we'll
construct a univariate {\tt FunctionField} over a single variable, specifying its
constant field, then form its {\tt extension} modulo the curve's defining polynomial,
which will yield another, bivariate, {\tt FunctionField}.

A Sage {\tt FunctionField} has methods to construct both the maximal finite order
and the maximal infinite order.
We can construct ideals of these orders, and from them
construct the corresponding places.  Given a place in the underlying univariate
{\tt FunctionField}, we have a method that will extend the corresponding ideal
to the bivariate {\tt FunctionField}, factor (decompose) it, and return a list
of all {\tt places\_above} the original place.

The standard way to construct Puiseux expansions in Sage is the {\tt completion} method,
defined on {\tt FunctionField}'s,
but this method has some limitations.  It doesn't work on
differentials (only on functions), doesn't allow a uniformizing parameter to be
specified, and doesn't allow absolute precision to be specified (only relative precision).
To overcome this problems, I've written the following function that's basically just
a wrapper around {\tt completion}.  Its default behavior is to compute either
the principal part of a Puiseux expansion for poles, or a single term of the Puiseux
expansion for finite values.
The main thing I don't like about it is that
when you pass it a differential, it returns a {\tt LaurentSeries}, when it should actually
return a {\tt LaurentSeries} times $ds$, where $s$ is the uniformizing variable.

If a uniformizing variable is specified, either as a function field element
or as a {\tt LaurentSeries}, we reverse the series, which only works if the series has valuation 1,
giving us a new series that expresses $s$ in powers of the uniformizing variable, then
then substitute the reversed series into the original $s$-expansion of the function field element,
which gives us an expansion of the element in powers of the uniformizing variable.

\begin{sagecommonsmall}
from sage.rings.function_field.differential import FunctionFieldDifferential
def puiseux(F, pl, uvar=None, absprec=0):
    def puiseux2(f):
       if isinstance(f, FunctionFieldDifferential):
          base_differential = f.parent()._gen_base_differential
          f = f._f
          is_differential = True
          valuation = f.valuation(pl) + base_differential.valuation(pl) - 1
       else:
          is_differential = False
          valuation = f.valuation(pl)
       series = F.completion(pl, prec=max(1, absprec-valuation))
       if uvar:
          if isinstance(uvar, LaurentSeries):
             uvar_series = uvar.reverse()
          else:
             uvar_series = series(uvar).reverse()
          f_series = series(f)(uvar_series)
       else:
          f_series = series(f)
       if is_differential:
          if uvar:
             f_series *= series(base_differential).derivative()(uvar_series)
          else:
             f_series *= series(base_differential).derivative()
       return f_series
    return puiseux2
\end{sagecommonsmall}

%\renewcommand{\theexample}{\ref{y^2 = 1 - x^2} cont}
\newtheorem*{examplecont}{Example \ref{y^2 = 1 - x^2} cont}
\begin{examplecont}
\begin{quote}\rm
%\hfil\break
Construct Puiseux expansions of $y$ at the multiple points of the
curve $y^2 = 1 - x^2$

This is Example \ref{y^2 = 1 - x^2}, redone now using Sage {\tt FunctionField}.

First we create a function field
in one variable with coefficients in $\QQbar$:

\begin{sageblock}[ch7]
R.<x> = FunctionField(QQbar)
\end{sageblock}

Next we create a ring of polynomials in $y$ with coefficients in the function field,
which allows us to write the minimal polynomial of the algebraic curve.  We then
create a new function field that is an extension of the rational function field:

\begin{sageblock}[ch7]
L.<y> = R[]
F.<y> = R.extension(y^2 + x^2 - 1)
\end{sageblock}

Recall from Example \ref{y^2 = 1 - x^2} that our multiple points lie at $x = \pm 1$.
Since we wish to construct our series expansion at a point with finite coordinates
(remember that this is projective space, so we also have points at infinity),
we use the finite maximal order, construct the ideal corresponding the desired
point, then construct the unique place at that point:

\begin{sageblock}[ch7]
O = F.maximal_order()
pl = O.ideal(x-1, y).place()
\end{sageblock}

Finally, we construct the Puiseux expansion of $y$ at the place, specifying the desired precision of the expansion.
Notice that {\tt puiseux} actually returns a function, to which we pass
the function field element ($y$) that we wish to expand:

\begin{sageblock}[ch7]
puiseux(F, pl, absprec=4)(y)
\end{sageblock}

This answer differs from the one we computed in Example \ref{y^2 = 1 - x^2} because the
choice of uniformizing variable is not unique, and the
computer made a different choice than we did.  Our ``$t$'' variable in Example \ref{y^2 = 1 - x^2}
was roughly $\sqrt{x-1}$.  Even though square roots don't exist
in this field, we can construct a series expansion of $x-1$,
and construct the square root of the series expansion as another series expansion.

\begin{sageblock}[ch7]
xminus1 = puiseux(F, pl, absprec=5)(x-1)
t = xminus1^(1/2)
\end{sageblock}

Using this series as a uniformizing variables gives us $y$ in powers of $t$,
even though the computer still prints the results using $s$.

\begin{sageblock}[ch7]
puiseux(F, pl, absprec=4, uvar=t)(y)
\end{sageblock}

Comparing this to equation \ref{(1,0) expansion}, we see that they're the same.

Now let us turn to the point at infinity.  We begin by constructing the
maximal infinite order of the underlying rational function field $C(x)$,
because its structure is quite simple and we know that it will only have a single point,
and a single place, at infinity.

\begin{sageblock}[ch7]
ROinf = R.maximal_order_infinite();
Rinf = ROinf.ideal(1/x).place()
\end{sageblock}

Now we'll use the {\tt places\_above} method
to obtain a list of all places in an extension field that lie
over a given place in the underlying function field.
Recall from Example \ref{y^2 = 1 - x^2} that this curve has a
singular point at infinity with two separate cycles
and therefore two separate places.

\begin{sageblock}[ch7]
Pinf = F.places_above(Rinf)
\end{sageblock}

Having obtained
these places, represented as ideals in the maximal infinite order,
it is straightforward to use the {\tt puiseux} function
to construct Puiseux series for $x$ and $y$.

\begin{sageblock}[ch7]
[puiseux(F, pl, absprec=5)(x) for pl in Pinf]
[puiseux(F, pl, absprec=5)(y) for pl in Pinf]
\end{sageblock}

Comparing this to Example \ref{y^2 = 1 - x^2}, we see that the results are the same.

\hfill$\Box$\end{quote}
\end{examplecont}

%\renewcommand{\theexample}{\thesection.\arabic{example}}

\example \cite{bliss} \S 68
Compute expansions at all multiple points of

$$y^3+x^3y+x=0$$

We begin by computing the discriminant of the
equation, which gives us the locations of the multiple points.

\begin{sageblock}
R.<x,y> = QQbar[];
f = y^3 + x^3*y + x
f.discriminant(y).factor()
\end{sageblock}

That result is rather confusing.  Let's try factoring over $\QQ$
instead of $\QQbar$:

\begin{sageblock}
f.discriminant(y).change_ring(QQ).factor()
\end{sageblock}

The multiple points lie over the roots of this equation: $x=0$ and
the seven roots of $4x^7+27=0$.  Infinity also needs to be
examined.  We begin with $x=0$:

\begin{sageblock}[ch7-2]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^3 + x^3*y + x)
O = F.maximal_order()
F.places_above(R.maximal_order().ideal(x).place())
O.ideal(x).factor()
pl = O.ideal(x, y).place()
xseries = puiseux(F, pl)(x)
yseries = puiseux(F, pl)(y)
yseries((xseries^(1/3)).reverse())
\end{sageblock}

This result shows that we have a single cycle at $(x,y)=(0,0)$ with
three sheets.  Now, let's look at a specimen root
of $4x^7+27=0$:

%% This was from my old Maxima puiseux routine
\begin{comment}
puiseux(y^3 + x^3*y +x, x, y, g, -3/(2*g^2), 1);
puiseux(y^3 + x^3*y +x, x, y, g, 3/g^2, 1);
puiseux(y^3 + x^3*y +x, x, y, g, -(3/8)^(1/7), 1);
\end{comment}

%% 7min43s with this
%% 8min14s with it commented out

\begin{sageblock}[ch7-2]
g = QQbar(-27/4)^(1/7)
pl = O.ideal(x-g, y+3/(2*g^2)).place()
xseries = F.completion(pl, prec=3)(x)
yseries = F.completion(pl, prec=2)(y)
# This is the algebraic number that takes forever to print
# yseries(((xseries-g)^(1/2)).reverse())
pl = O.ideal(x-g, y-3/g^2).place()
xseries = F.completion(pl, prec=3)(x)
yseries = F.completion(pl, prec=2)(y)
yseries((xseries-g).reverse())
\end{sageblock}

\begin{sageblock}[ch7-3]
R1.<g> = QQ[]
S.<g> = NumberField(4*g^7+27)

R.<x> = FunctionField(S)
L.<y> = R[]
F.<y> = R.extension(y^3 + x^3*y + x)

O = F.maximal_order()

O.ideal(x-g).factor()

pl = O.ideal(x-g, y+3/(2*g^2)).place()
xseries = F.completion(pl, prec=3)(x)
yseries = F.completion(pl, prec=2)(y)
xseries((yseries+3/(2*g^2)).reverse())

pl = O.ideal(x-g, y-3/g^2).place()
xseries = F.completion(pl, prec=2)(x)
yseries = F.completion(pl, prec=2)(y)
yseries((xseries-g).reverse())
\end{sageblock}

We have one sheet of two cycles at $(g,-3/(2g^2))$
and an ordinary point at $(g,3/g^2)$.

Finally, let's look at what happens when $x$ goes to infinity:

%% 12s with both this and the previous ch7-2 block commented out
%% 31s with this commented out; 7min43s with it in
%%\begin{sageblock}[ch7-2]
%%Rinf = R.maximal_order_infinite().ideal(1/x).place()
%%Pinf = F.places_above(Rinf)
%%xseries = F.completion(Pinf[0], prec=3)(x)
%%yseries = F.completion(Pinf[0], prec=10)(y)
%%
%%xseries = F.completion(Pinf[1], prec=3)(x)
%%yseries = F.completion(Pinf[1], prec=10)(y)
%%\end{sageblock}

%% 42s with this reduced precision version of it
\begin{sageblock}[ch7-2]

Rinf = R.maximal_order_infinite().ideal(1/x).place()
Pinf = F.places_above(Rinf)
xseries = puiseux(F, Pinf[0])(x)
yseries = puiseux(F, Pinf[0])(y)

xseries = puiseux(F, Pinf[1])(x)
yseries = puiseux(F, Pinf[1])(y)
\end{sageblock}

Here we have an ordinary point at $(\infty,0)$ and
a single cycle of two sheets at $(\infty,\infty)$.

We have examined all of this curve's ramification points,
including those at infinity (since we analyzed all of its
points at infinity), and found that all of them admitted
a single Puiseux expansion.

Therefore, this curve is {\it non-singular}, and according to the
genus-degree formula (MORE INFO), its geometric and arithmetic genus
are the same.  Its arithmetic genus is $\frac{1}{2}(d-1)(d-2) = 3$,
where $d=4$ is the degree of the defining polynomial.  Computing
the geometric genus is more difficult\footnote{
{\tt https://www.singular.uni-kl.de/Overview/Examples/Genus/genus1.html}

{\tt https://en.wikipedia.org/wiki/Algebraic_curve\#Classification_of_singularities}

{\tt https://math.stackexchange.com/questions/150840}

{\tt http://mathforum.org/library/drmath/view/71229.html}
}, but we can verify our
information with Sage, being careful to work in {\it projective} space:

\begin{sageblock}
PP.<x,y,z> = ProjectiveSpace(QQ, 2)
C = Curve(y^3*z + x^3*y + x*z^3)
C.is_singular()
C.arithmetic_genus()
C.geometric_genus()
\end{sageblock}


\endexample

\example Find the principal parts of $\frac{1}{y}$ on the curve
$y^2 = 1 - x^2$

The {\it principal part} of an algebraic function is the part
of its series expansion with negative exponents.  Theorem
\ref{algebraic functions are characterized by their principal parts}
states that an algebraic function is completely determined,
up to adding a constant, by its principal parts.

The first step is to locate the function's poles, which in this case is
simply the places where the denominator is zero, and that's just
$x=\pm 1$.  Now, if we use {\tt puiseux}, we can just request a series
truncated at the $-1$ term:

\begin{sageblock}[ch7-4]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 + x^2 - 1)
D = (1/y).divisor()
# there's nothing in Sage function field code to set absolute precision, unfortunately
table([[p, F.completion(p, prec=1)(1/y)] \
    for p,m in D.list() if m < 0])
\end{sageblock}

\endexample

\example Find the principal parts of $\frac{x}{y} \ud x$ on the curve
$y^2 = 1 - x^2$

Differential forms are not functions, and have different series
expansions, due to the presence of the
differential, which must be adjusted at ramification points.

Let's expand $\frac{x}{y}$ at $x=1$:

\begin{sageblock}[ch7-5]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 + x^2 - 1)
O = F.maximal_order()
pl = O.ideal(x-1, y).place()
F.completion(pl, prec=6)(x)
F.completion(pl, prec=6)(x/y)
\end{sageblock}

Now $x=t^2+1$, so $\ud x=2t\ud t$.  Thus, multiplying $\frac{x}{y}$
by $\ud x$ and changing our variable to $t$ will multiply
all of the terms in our expansion by $2t$:

\begin{sageblock}[ch7-5]
# Completion of a differential is not implemented in production Sage code
# F.completion(pl, prec=6)(x/y*x.differential())
dx = F.completion(pl, prec=6)(x).derivative()
F.completion(pl, prec=6)(x/y) * dx
\end{sageblock}

Even though $\frac{x}{y}$ has a pole
at $x=1$, $\frac{x}{y} \ud x$ does not!

Its behavior at infinity also requires analysis.

\begin{sageblock}[ch7-5]
# this is here to manually introduce i into our number field
F.maximal_order().ideal(x - sqrt(QQbar(-1)));

Rinf = R.maximal_order_infinite().ideal(1/x).place()
Pinf = F.places_above(Rinf)

F.completion(Pinf[0], prec=2)(x)
F.completion(Pinf[0], prec=2)(y)
F.completion(Pinf[0], prec=2)(x/y)
dxinf = F.completion(Pinf[0], prec=2)(x).derivative()
F.completion(Pinf[0], prec=2)(x/y) * dxinf
\end{sageblock}

$\frac{x}{y}$ has no poles at infinity, and approaches
the limiting values $\pm i$ as $x$ and $y$ approach
infinity.  The differential $\frac{x}{y} \ud x$,
on the other hand, requires us to multiply by $\ud x$,
and since $x=\frac{1}{t}$, $\ud x = - \frac{1}{t^2} \ud t$.

In short, while $\frac{x}{y}$ has poles only at $(\pm 1,0)$,
$\frac{x}{y} \ud x$ has poles only at infinity.

\begin{sageblock}[ch7-5]
# F(x) and not x because otherwise we get a divisor in the underlying rational function field
D = (x/y).divisor() + F(x).differential().divisor()
table([[p, F.completion(p, prec=2)(x/y) * F.completion(p)(x).derivative()] for p,m in D.list() if m < 0])
\end{sageblock}

\endexample

Consider, for example the lemniscate of Bernoulli, defined by the equation

$$ (x^2+y^2)^2 - (x^2-y^2) = 0$$

\begin{sageblock}[lemniscate]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension((x^2+y^2)^2 - (x^2-y^2))

O = F.maximal_order()
I = O.ideal(x,y)
I.factor()
\end{sageblock}

\begin{sageblock}[lemniscate]
O.basis()
\end{sageblock}

One characterization of the maximal finite order $O$ is that it contains
all functions with no poles at finite places.  The first three elements
in the basis are obvious, but why is the fourth so complicated?
Isn't $\frac{y}{x}$ in $O$?

\begin{sageblock}[lemniscate]
y/x in O
\end{sageblock}

Doesn't $\frac{y}{x}$ approach either $1$ or $-1$ as it approaches the
origin?  Remember that we're working in complex space.  We've got
four roots, not two.  Let's look at some examples of limiting values
using some numerical examples:

\begin{sageblock}[lemniscate]
set_verbose(-1)
R.<u,v> = CC[]
ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.0001))
ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.0001)).variety()
[d[v] for d in ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.0001)).variety()]
[d[v]/.0001 for d in ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.0001)).variety()]
[d[v]/.00001 for d in ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.00001)).variety()]
[(d[v]^3+d[v])/.00001 for d in ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.00001)).variety()]
\end{sageblock}

\begin{sageblock}[lemniscate]
Fs = O.ideal(x).factor()
F.completion(Fs[0][0].place(), prec=7)(y)
F.completion(Fs[1][0].place(), prec=7)(y)
F.completion(Fs[2][0].place(), prec=7)(y)
F.completion(Fs[3][0].place(), prec=7)(y)
\end{sageblock}

How does one of these ideal generators factor?

\begin{sageblock}[lemniscate]
A.<a> = QQbar[]
(1/2*a^2+1/2*sqrt(QQbar(-1))*a+1).factor()
\end{sageblock}

\vfill\eject
\mysection{Divisors}

Given a function on an algebraic curve, we can ask at which places it
has poles and zeros.  The location and strengths of a function's
poles and zeros are called its {\it divisor}.

For non-singular curves, the points and places are in one-to-one
correspondence, and a function's divisor can be described in terms
of the points where its poles and zeros lie.

Thus, one way of defining a divisor is to associate integers (positive
for zeros, negative for poles) with each point of the curve, subject
to the stipulation that all but a finite number of those integers is
zero.  Such a description is called a {\it Weil divisor}, and is most
suitable for working in {\it intersection theory}.

For singular curves, the situation is more complicated.  A divisor
needs to be associated with places, not points.  Such a divisor is
called a {\it Cartier divisor}, and is more suitable for our purposes.

\vfill\eject
\mysection{Riemann-Roch spaces}

A {\it Riemann-Roch space} is a subspace of an algebraic curve's
function field characterized by specifying a minimum order that the
function must obtain at all of the curve's points.  Aside from having
great theoretical significance, Riemann-Roch spaces are practically
useful because they are finite dimensional, and algorithms exist for
constructing Riemann-Roch bases.  Finding a basis for a Riemman-Roch
space in a crucial first step in solving a Mittag-Leffler problem.

Numerous algorithms have been developed for computing bases of
Riemann-Roch spaces.  Sage uses an implementation of Hess's algorithm
from \cite{hess}.

\begin{comment}
I've implemented in Maxima one of the oldest,
from \cite{bliss}, though it probably dates back
to \cite{dedekind-weber}.

We begin the process with a ${\mathrm C}(x)$-basis for the entire
function field, namely $\{1, y, \ldots, y^{n-1}\}$.

Next, we want to convert this into a ${\mathrm C}[x]$-basis for the
finite portion of the divisor.  First, we multiple the basis by
whatever polynomials in $x$ are required to place the basis elements
into the divisor's function space, then for each value of $x$ form
a matrix of coefficients, and keep reducing until its determinant is zero.

Finally, we need to adjust this basis to match the divisor's requirements at infinity.

A divisor's basis can be transformed to another basis for the same
divisor by multiplying by a matrix in ${\mathrm C}[x]$ with
determinant a constant not equal to zero. (Bliss Th. 21.1)

If we have a cycle at infinity, multiplying by x will multiply
the expansions by (1/t^r).

{\tt riemannroch(f,x,y,divisor)} computes a basis for the Riemann-Roch
space $L(D)$.  {\tt divisor} is a list of elements, each in the form
{\tt [[$x_i$, $y_i$], $\nu_i$]}, where $(x_i, y_i)$ is a point on the
curve, and $\nu_i$ is the order of the divisor at that point.  For
singular points, either the standard syntax can be used, indicating
that the order of the divisor is the same at all points of the
singularity, or $\nu_i$ can be replaced with a list of values, one for
each sheet at the singularity.  The order of sheets is the same
returned by {\tt puiseux}.  Specifying multiple orders at
singularities with cycles is currently not supported.

\end{comment}

Here's a simple example\footnote{From
{\tt https://math.stackexchange.com/questions/294644}}
of a Riemann-Roch space calculation:

\begin{sageblock}[riemannroch]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 - x^3 + x)


O = F.maximal_order()
P = O.ideal(x,y)
D = P.divisor()

D.basis_function_space()
(2*D).basis_function_space()
(3*D).basis_function_space()
(4*D).basis_function_space()
\end{sageblock}

Here are the examples from \cite{alvanos} \S6.3:

\begin{sageblock}[riemannroch]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 - x^3 - 1)

O = F.maximal_order()
P1 = O.ideal(x-2,y-3)
P2 = O.ideal(x-2,y+3)

Rinf = R.maximal_order_infinite().ideal(1/x).place()
Pinf = F.places_above(Rinf)[0]

D1 = P1.divisor()
D2 = P2.divisor()
Dinf = Pinf.divisor()

(Dinf-D1).basis_function_space()
(2*Dinf-D1).basis_function_space()
(3*Dinf-D1).basis_function_space()
(4*Dinf-D1).basis_function_space()
(Dinf).basis_function_space()
(2*Dinf).basis_function_space()
(3*Dinf).basis_function_space()
\end{sageblock}

\vfill\eject
\mysection{Mittag-Leffler Problems}

Theorem \ref{algebraic functions are characterized by their principal parts}
tells us that a rational function on an algebraic
curve is completely characterized, up to an additive constant,
by the principal parts of the Puiseux expansions at its poles.
Note that Theorem \ref{algebraic functions are characterized by their principal parts}
does not guarantee the existence of a function with
specified principal parts.  It only shows that any
two such functions, {\it if they exist}, differ
by at most a constant.

A {\it Mittag-Leffler problem} is the practical application of this
theorem -- given a set of principal parts, find a function that
matches them all, or prove that no such function exists.

The first step in solving a Mittag-Leffler problem is to identify the
maximum strengths of the poles, and construct a basis for a
Riemann-Roch space that includes all functions with poles of such
strength.  We now have a finite basis for a vector space that must
include the function we are looking for.  We construct Puiseux
expansions for the basis functions, and use them to construct
a matrix equation that, when solved, gives the coefficients
needed to form the function we seek from the basis functions.

The input data is a set of principal parts or, alternately, a divisor
combined with a vector of coefficients.

Let's assume that we've got our data in the latter form, so we can run
{\tt riemannroch} on the divisor and obtain a set of basis functions.
Now let's construct a Sage function to extract the principal parts
of the basis functions and form them into a matrix:

\begin{sagecommon}
def principal_parts_matrix(div, basis):
    F = div.parent().function_field()
    coeffs = [(puiseux(F,p), i) for p,m in div.list() for i in range(-m,0)]
    return matrix([[c[0](b)[c[1]] for c in coeffs] for b in basis]).transpose()
\end{sagecommon}

Given a vector {\tt b} of coefficients, we now want to
solve a matrix equation:

$$m \cdot v = b$$

This will typically be an overspecified system -- a non-square matrix
that may or may not have a solution.  That's fine; since some
integrals have no elementary form, this doesn't represent a limitation
in our theory.  Failure to solve this matrix equation would only show
that no function exists on this curve with the coefficients {\tt b}.

\example
Let's say that we've identified a divisor on an algebraic
curve (example \ref{an integral Maxima can't solve}):

We now compute its principal parts matrix:

\begin{sageblock}[riemannroch]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 - x^8 - 1)

# do this to force the extension code to run
y.divisor();

O = F.maximal_order()
Oinf = F.maximal_order_infinite()

Dfinite = add([O.ideal(x-a*QQbar(-1).sqrt(), y-b*QQbar(2).sqrt()).place().divisor() for a in [-1, 1] for b in [-1, 1]])

Rinf = R.maximal_order_infinite().ideal(1/x).place()
Dinf = add([pl.divisor() for pl in F.places_above(Rinf)])

D1 = Dfinite + 2*Dinf

basis = Dfinite.basis_function_space()

D1.basis_function_space()

principal_parts_matrix(D1, basis)
\end{sageblock}

{\bf TODO}

Introduce a sample vector {\tt b} and show how to proceed.

\endexample

\mysection{Parallels with the Transcendental Cases}

At this point, it may seem that we've spent this entire chapter
developing a suite of technical tools that appear completely different
from everything that came before them.  Why should the algebraic case
be so much different from the transcendental cases?  What would happen
if we used here the same kind of techniques from earlier in the book?

First, the key difference in the algebraic case is the lack of unique
factorization.  Algebraic extensions are not, in general, unique
factorization domains, a classic example being the factorization of
$6$ into either $3\cdot 2$ or $(1+\sqrt{-5})\cdot(1-\sqrt{-5})$ in the
ring $\ZZ[\sqrt{-5}]$.  You can show that all four numbers $3$, $2$,
$(1+\sqrt{-5})$ and $(1-\sqrt{-5})$ are all prime in $\ZZ[\sqrt{-5}]$,
so we have two distinct factorizations in this ring.

{\bf Show an example in a function field.}

The main problem with our earlier tools is the difficulty in defining
factorization.  How, for example, do you construct a partial fractions
expansion?  A review of Theorems \ref{logarithmic integration theorem}
and \ref{exponential integration theorem} reveals that both depend
not merely on the construction of a partial fractions expansions, but
also on its {\it uniqueness}.  Without unique factorization, how can
you possibly have a unique partial fractions expansion?

The primary goal of this chapter is to develop techniques to carry
out the same kinds of operations we did earlier, but without relying
on unique factorization.

For example, a principle parts expansion of a function on an algebraic
curve is exactly analogous to a partial fractions expansion of a
rational function.

{\bf Demonstrate}

Although we began our development using infinite series expansions, we
ultimately concluded that we can completely specify a function (up to
an additive constant), using only a finite number of constants -- the
principle parts coefficients, which turn out to align precisely with
the coefficients in a partial fractions expansion.

Reassembling a partial fractions expansion into a rational function is
easy -- you just promote all the fractions to a common denominator,
add up the terms, and cancel any common factors that remain between
the numerator and the denominator.  Solving a Mittag-Lefler problem is
considerably more difficult, but is in principle the same operation --
given the principle parts coefficients (resp. the partial fractions
expansion), construct a single rational function that matches.  The
major caveat here is that, unlike reassembling a partial fractions
expansion, there might be not solution.  Not every principle parts
expansion has a matching algebraic function.

Likewise, finding an algebraic function's divisor is exactly analogous
to factoring the numerator and denominator of a rational function.
You get a finite set of poles and zeros with their locations and
multiplicities.  Again, in the algebraic case, it's more complicated --
you might have singularities with multiple places lying over a single
point; the ``coordinate'' is more complicated that a simple $(x,y)$
coordinate pair, but the principle is the same.

And finding a function with a specified set of poles and zeros is the
same as taking a rational function in factored form and multiplying
the factors together again.  Again, there's a caveat -- in the
algebraic curve case there might be no solution.

So, if the tools we've developed in this chapter parallel neatly with
the tools we used in Chapter \ref{chap:Integration of Rational Functions} to solve integrals of rational
functions, can we generalize these tools to handle more complicated
transcendental fields, like we did in Chapters \ref{chap:The Logarithmic Extension} and \ref{chap:The Exponential Extension}?
And do we have anything like the Hermite reduction procedure we developed at the
end of Chapter \ref{chap:Integration of Rational Functions}?

The answers to both of these questions is 'yes'.  For the purpose of a
clear exposition, I've developed this theory so far in its simplest
form, and if you're seeing it for the first time, I suspect that you
already appreciate not having met it in its full generality!  We can
drop the assumption of an algebraically closed coefficient field and
lose very little except simplicity; this will be the subject of
Chapter \ref{chap:Algebraic Extensions}.  Barry Trager showed in \cite{trager} how the Hermite
reduction can be performed in an algebraic extension; it's now called
{\it Hermite-Trager reduction} and I'll present it at the end of
Chapter \ref{chap:Abelian Integrals}.

However, continuing with the intent of presenting the theory in its
simplest form first, we'll begin the next chapter by looking at how to
use these tools to integrate Abelian integrals, much like we first met
partial fractions expansion when learning to integrate in first year
calculus, and only later generalized it into a form suitable for
integrating in arbitrary transcendental extensions.  We'll find that
completely solving integrals in even this simplest of algebraic
extensions will require a significant excursion into modern algebraic
geometry, so much so that the entirety of Chapter \ref{chap:The Risch Theorem}
will be devoted to proving the book's most exotic theorem.

If there's a lesson to be learned from Chapter \ref{chap:Algebraic Curves}, though, it's this:

\begin{key point}
Divisors, principle parts expansions, Riemann-Roch spaces, and Mittag
Leffler problems are how we do factorization, partial fractions
expansions, and their inverse operations in algebraic extensions where
we've lost unique factorization.
\end{key point}
