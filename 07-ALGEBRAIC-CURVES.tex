
\mychapter{Algebraic Curves}

{\bf THIS CHAPTER IS INCOMPLETE.}

Having addressed logarithmic and exponential extensions, we now turn
to the algebraic extension, which at first appears to be completely
different in character from the two transcendental cases.  The
differences stem largely from the lack of unique factorization in the
algebraic case; algebraic extensions are not, in general, unique
factorization domains.

\begin{comment}

To justify that statement, let's begin by trying to attack algebraic
extensions in the same manner as the two transcendental cases.

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i (\theta^{i})')$$

Presumably, we know how to differentiate $\theta$, so let's write
$(\theta^i)' = \sum_j \psi_{i,j} \theta^j$:

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i \sum_j \psi_{i,j} \theta^j)$$

Writing the $a_i$'s as a column vector $A$, we'll end up with a matrix equation:

$$I = A' + \Psi A$$

This is superficially similar to a Risch equation; it's a matrix Risch
equation.  To proceed in the same manner as before, we'd now have to
construct partial fractions expansions of everything and begin looking
at how cancellation might occur between our various components.  The
variety of possible cancelations between our terms seems daunting.

\vfill\eject

$$\int y\,dx \qquad y^2 = 4-x^2$$

Let's assume that our solution has the form $S = ay+b+\ln (cy+d)$, where $a$
and $b$ are rational functions in $x$ and $c$ is a polynomial in $x$.

$$S' = a' y + a y' + b' + \frac{c'y+cy'+d'}{cy+d}$$
$$2 y y' = -2x \qquad y' = -\frac{x}{y} = - \frac{x}{4-x^2}y$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}\frac{cy-d}{cy-d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})cy^2+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2y^2-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})c(4-x^2)+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c'(4-x^2) - c x)c+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2} + \frac{cd-(c' - c \frac{x}{4-x^2})d}{c^2(4-x^2)-d^2})y + b' + \frac{(c'(4-x^2) - c x)c -d^2}{c^2(4-x^2)-d^2}$$


The correct answer is

$$S = 2i\ln(ix-y)+\frac{x}{2}y$$

Instead, we'll use a different approach...

\vfill\eject

\end{comment}

In this chapter, we'll develop a basic set of technical tools
for working in the simplest kind of algebraic extension, an
extension of ${\bf C}(x)$.  This will prepare us for the
next chapter, where we'lll study {\it Abelian integrals}, which
are integrals whose integrands are formed from polynomials and roots
of polynomials.  In other words, integrands in an algebraic extension
of ${\bf C}(x)$.

How might we handle an algebraic extension of ${\bf C}(x)$?  A crucial
property of {\it algebraic functions}, as elements of an algebraic
extension are called, is that they admit series expansions everywhere,
including infinity, so long as we allow a finite number of negative
exponents.  Such functions are called {\it meromorphic}.  The
logarithm function fails to be meromorphic at the origin, and the
exponential function fails to be meromorphic at infinity, but
algebraic functions are meromorphic everywhere, including infinity.

This means that around any specific point, we can construct a series
expansion of the integrand and integrate termwise to obtain a series
expansion for the integral.  At first this doesn't seem terribly
useful, because series expansions are infinite and we're trying to
construct closed-form solutions, but it turns out that only a finite
number of places will have negative exponents in their series
expansions and that the function is completely specified, up to an
additive constant, by the coefficients of the negative powers.

Thus, the basic strategy is first to identify the function's {\it
poles}, the places where its value becomes infinite, and compute the
{\it principal part} of the series expansions there, which are the
negative exponents and their coefficients.  This is fairly
straightforward, though there are issues of computational complexity
that make it non-trivial.  Then we integrate termwise, which is
trivial, and obtain local series expansions at the poles of the
solution.  Next, we need to reassemble this local information into a
global function (if one exists), a {\it Mittag-Leffler problem}, for
which I will present a basic algorithm in this chapter, although more
efficient techniques have been developed.

What about the logarithmic terms?  This turns out to be the most
difficult part of the problem.  We can begin to analyze them using the
same techniques, by noting that the $t^{-1}$ terms in the principal
parts of the integrand lead directly to logarithms in the integral,
and furthermore that the coefficients of these terms give us the
locations and orders of the poles and zeros in the logarithms.  This
information specifies an algebraic function up to a multiplicative
constant\footnote{Of course.  Due to the presence of a constant of
integration, we expect to specify the main part of the integral up to
an {\it additive} constand, and the logarithmic parts of the integral
up to a {\it multiplicative} constant.}, and our algorithm can be
adapted without too much trouble to handle this case.

The problem is that no algebraic function might exist that match a
given set of zeros and poles, but increasing the order of the zeros
and poles might produce a solution.  This corresponds to raising the
logarithm term to powers, i.e, $\ln f$ is the same as $\frac{1}{2} \ln
f^2$, which is the same as $\frac{1}{3} \ln f^3$, except that in our
case the lower powers might not exist in our function field, even
though higher powers do.  What powers should we use?  We could go on
raising to higher and higher powers, hoping that something will work,
but the only known algorithm to limit this search requires reducing
modulo a prime, and that requires techniques that weren't developed
until the 1960s.  Before heading into such {\it modern algebraic
geometry}, however, let's see how far we can get with the classical
algebraic geometry of the nineteenth century.

\mysection{Classical Algebraic Geometry}

The roots of algebraic geometry lie in studying the zeros of
polynomial equations.  We began with a single polynomial in a single
variable, and have learnt a great deal about it.  We know how to solve
it (at least in terms of radicals) if its degree is less than 5.
Galois proved that no such solution (in radicals) exists (in the
general case) for larger degree, though abstract algebra provides us
with a suitable general theory to handle this case.  Simple long
division tells us that it can have no more roots than its degree, and
Gauss showed that all of the roots exist as complex numbers --- the
Fundamental Theorem of Algebra.

The next logical step is to consider zeros of a single polynomial in
two variables, and this equation has also received a great deal of
attention from mathematicians.  Like the univariate case, we have
theories devoted to low-order special cases --- {\it linear equations}
(all terms first degree or constant), the {\it conic sections} (all
terms second degree or less), and the {\it elliptic curves} (one term
third degree; all others second degree or less).  In the general case,
$\sum a_{ij} x^i y^j = 0$ is called an {\it algebraic curve}, and a
rational function in $x$ and $y$ is called an {\it algebraic
function}.  These will be our main focus of attention in this chapter.
Note that an algebraic curve's rational functions form a field, the
{\it function field} of the curve.

One of the first problems we face when dealing with algebraic curves
is the multi-valued nature of their solutions.  Consider the algebraic
function $y$ defined on the algebraic curve $y^2 = x^2 - 1$.  There
are, in fact, two seperate algebraic functions that solve this
equation --- both $y$ and $-y$ are solutions.  Conventionally, we
express this by writing something like $y = \pm\sqrt{x-1}$, but for
higher degree curves this kind of notation becomes unsuitable.  How,
for example, do you express the three possible solutions to a cube
root, and how do you deal with the general case where $y$ appears
multiple times in the curve's defining polynomial?

Our solution to this problem is to regard the entire algebraic curve
as a two-dimensional surface in a four-dimensional space.  Why four
dimensions?  Well, just as in the univariate case, we find it
convenient to work with complex numbers, so as to deal easily with
roots of negative numbers.  Regarding both $x$ and $y$ as complex
numbers (two dimensions each), and plotting them against each other,
we obtain a four dimensional space.  Just as in the real case, where
an equation like $x^2 + y^2 = 1$ defines a circle, an algebraic curve
defines a surface, the loci of $x$ and $y$ that satisfy the defining
polynomial.

The defining polynomial can be regarded as a polynomial in $y$, whose
coefficients are polynomials in $x$, simply by collecting terms with
like powers of $y$.  If we fix a given complex value of $x$, we have a
polynomial in $y$ with complex coefficients that can be solved as a
univariate polynomial and yields at most $n$ solutions for $y$.  We
can be more specific.  For any given value of $x$, we have {\it
exactly} $n$ solutions for $y$ {\it unless} one of two things happen.
Either the leading ($y^n$) coefficient is zero, in which case we have
less than $n$ solutions due to having a polynomial of degree less than
$n$, or the polynomial has multiple identical roots.

There are only a finite number of these special points, as can be seen
by considering the {\it discriminant} of the defining polynomial,
which is the resultant of the polynomial with its partial derivative
with respect to one of its variables.  The discriminant will be zero
at special points, so these points can be located by computing the
zeros of a univariate polynomial.

These points are further classified according to whether or not the
curve is locally Euclidean in their neighborhood.  Geometrically, this
corresponds to looping around the point until you return to your
starting point.  If a single such {\it cycle} covers all the sheets of
the curve, the curve is locally Euclidean, and we have an {\it
ordinary point} of the curve, albeit one with {\it ramification}, the
{\it ramification index} being how many times we had to circle the
point.  Otherwise, multiple cycles are required to cover all of the
sheets, and we have a {\it singular point}.  Analytically, both
partial derivatives of the curve's polynomial are zero at a singular
point, while at least one is non-zero at ordinary points.

Now, the coefficient of $y^n$ in the defining polynomial will be a
polynomial in $x$, which has a finite number of roots at which it is
zero, so there are only a finite number of points where the defining
polynomial is of degree less than $n$ in $y$.  As $x$ approaches one
of these points, the value of the $y^n$ coefficient approaches zero,
which causes at least one of the roots to approach infinity.  We'll
deal with these points by introducing a line at infinity, forming
{\it projective space} and creating a {\it compact} surface.

There are several crucial theorems that depend on the topological
property of {\it compactness}.  The complex plane is not compact; we
remedy this by adding a point at infinity.  Likewise, two-dimensional
complex space is not compact, either; we remedy this by adding a
line at infinity and obtaining {\it projective space}.

Projective space.  Compactness.

Another highly desirable property is to be locally isomorphic to
Euclidean space.  A differentiable surface that is everywhere locally
Euclidean is called a {\it manifold}.

By adding a line at infinity and resolving our singularities, we can
coax our algebraic curve into a compact, connected, complex manifold.
The primary utility of this construction is embodied in the following
theorems.

\theorem
\label{holomorphic functions on compact manifolds are constant}

Every holomorphic function $M \to C$ on a compact, connected, complex manifold $M$ is constant.

\proof

{\tt https://math.stackexchange.com/questions/881742}

\cite{guillemin} Lecture 2 contains a proof of the Maximum Modulus Priciple.

\endtheorem

\theorem
\label{algebraic functions are characterized by their principal parts}

An algebraic function on an algebraic curve is completely characterized, up to an additive
constant, by its principal parts.

\proof

Consider two algebraic functions $f$ and $g$ with identical principal
parts.  Taking the difference between them, we obtain a function $f-g$
with no principal parts, i.e, a holomorphic function.  By
Theorem \ref{holomorphic functions on compact manifolds are constant},
$f-g$ must be constant.

\endtheorem

Given the importance of an algebraic function's principal parts, we will
now develop tools to calculate them.

\vfill\eject

\mysection{Puiseux Expansions}

Given an algebraic function on an algebraic curve, we wish to compute
its principal parts by locating its poles and computing series
expansions there.  Since the powers of $y$ form a ${\cal C}(x)$ basis
for the curve's function field (PROOF), our primary goal is to compute
series expansions for $y$ at arbitrary points on the curve.  With such
expansions in hand, it is straightforward to construct expansions for
any algebraic function.

At any point where the discriminant is non-zero, a series expansion
for $y$ exists as a power series in $(x-\alpha)$, which is a
straightforward application of the Implicit Function Theorem.

\theorem
{\bf Implicit Function Theorem}
\label{implicit function theorem}

\cite{baby rudin} Theorem 9.28 is a real version of the theorem.

\cite{guillemin} Lecture 7 starts with a complex version of the theorem.

See {\tt https://math.stackexchange.com/questions/489789}

The two-dimensional complex analytic version:

Let f be an analytic
mapping of an open set $E \in {\mathbb C}^2$ into ${\mathbb C}$, such
that $f(x,y)=0$ and $\frac{df}{dy} \ne 0$, then an analytic
function $g(x)$ exists such that $f(x,g(x))=0$.
\endtheorem

% For infinity and/or poles, substitute z=1/x or v=1/y.

For ramification points, we'll use a substitution of the form
$x=t^r+\alpha$, where $r$ is the {\it ramification index}, and
construct a power series, not in $(x-\alpha)$, but in
$t=(x-\alpha)^{1/r}$, a {\it Puiseux series}.

As a function of $t$, both $x$ and $y$ are analytic in an open
neighborhood of $t=0$.  $x$ is analytic as a function of $t$ because
of the I.F.T. applied to $x=t^r+\alpha$.  $y$ is analytic as a
function of $x$ because of the I.F.T. applied to the defining equation
of the curve.  Composition of analytic functions are analytic,
showing that $y$ is analytic as function of $t$.

These arguments hold in an open neighborhood of $t=0$.  Continuity of
the roots shows that $y$ is continuous at $t=0$.  Then use existence
of the Laurent series (Silverman 11.2) and continuity of the roots
(HOW?) to establish analyticity at the ramification point.

Singular points will admit multiple Puiseux series, each one
corresponding to a single cycle.

% This analysis is facilitated by Newton polygons.

The most straightforward way to compute Puiseux series is to use {\it Newton
polygons} to determine ramification, then setup a trial series with
the correct ramification and substitute it into the curve's defining
equation.

Let's assume that we're expanding around the point $(0,0)$, as this
simplifies the analysis with no loss of generality.  Consider
factoring the defining polynomial of the algebraic curve:

$$p_n y^n + p_{n-1}y^{n-1} + \cdots + p_0 = (y-r_1)(y-r_2)\cdots(y-r_n)$$

How might we do this, if the polynomial is irreducible?  We need to
extend to a larger field where the polynomial's roots exist.  The
analysis above shows that Puiseux series form a suitable extension.

For each root $r_i$, define its {\it order} as the lowest power of $t$
that appears in its Puiseux expansion, divided by its ramification
index.  Multiplying factors together adds their orders, so
$p_0$'s order will be the sum of all of the $r_i$'s orders.

% $p_i$ is a sum of terms, each term with $y^i$ and $n-i$ of the $r$'s
% multiplied together.  The term with the lowest order will be formed by
% multiplying the $n-i$ lowest order roots, but there may be
% cancellation between multiple sets of $n-i$ such roots.


Now let's consider increasing $i$ by one.  How does $p_0$'s order
change?  $p_1$ is formed by adding together all products of $n-1$
roots, so $p_1$'s order will be lower than $p_0$'s order by the
largest of $r_i$'s orders, unless there are multiple $r_i$'s with the
same order.  In this case, cancellation between these multiple terms
could result in $p_1$ having a larger order than otherwise expected.

If there are $j$ $r_i$'s with the same largest order, increasing $i$
by $j$ will lower $p_i$'s order by $j$ times that largest order.

The Newton polygon is formed by plotting the orders of the $p_i$
coefficients, with $i$ varying along the horizonal axis and the order
plotted vertically.  The easiest way to do this is to plot the powers
of the monomials that appear in the equation, and construct the
polygon's lower convex hull.

Thus, a segment on the lower convex hull of the Newton polygon will
correspond to as many solutions as the width of the line segment, each
with order equal to the change in height divided by the width, i.e,
the negative slope of the line segment.  The denominator of the slope
will be the ramification index, and the numerator of the slope will be
the lowest exponent expected in the expansion of $y$.

Consider a Puiseux series
corresponding to a single line segment of the Newton polygon.
Letting
$\alpha$ be the $x$ exponent and $\beta$ be the $y$ exponent, so the
monomials in $f$ have the form $x^\alpha y^\beta$, then the equation
of the line segment is $r\alpha + s\beta = p$, where $r$, $s$, and $p$
are integers and $r$ and $s$ are relatively prime.  Making the
substitution $x=t^r$ and $y=t^s u(t)$, we obtain:

$$f(x,y) = \sum A_{\alpha\beta} x^\alpha y^\beta$$
$$ = \sum A_{\alpha\beta} t^{r \alpha} t^{s \beta} u(t)^\beta$$
$$ = t^p \underbrace{\sum A_{\alpha\beta} t^{r \alpha + s \beta - p} u(t)^\beta}_{g(t,u)}$$

At least two of the $(r \alpha + s \beta - p)$ exponents will be zero
(those monomials corresponding to the endpoints of the line segment on
the Newton polygon); all of the remaining exponents will be positive.
This means that if we expand $u(t)$ in a power series in $t$:

$$u(t) = u_0 + u_1 t + u_2 t^2 + u_3 t^3 + \cdots$$

then any power of $u(t)$ will have the form:

$$u(t)^\beta = U_0(u_0) + U_1(u_0, u_1) t + U_2(u_0, u_1, u_2) t^2 + U_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

and $g(t,u)$ will also have the form:

$$g(t,u) = G_0(u_0) + G_1(u_0, u_1) t + G_2(u_0, u_1, u_2) t^2 + G_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

In order for $g(t,u)=0$ at $t=0$, $G_0(u_0)$ must be zero, and since $G_0(u_0)$ is a polynomial
in $u_0$, this gives us a finite number of values for $u_0$ that can solve our equation.

Now, by setting $g(t,u)=0$, can we obtain $u(t)$ as a function of $t$?

The Implicit Function Theorem states that we can, if $\frac{\delta g}{\delta u}$ is not zero
at the point we wish to expand around.

$$\frac{\delta g}{\delta u}(0,u_0) = \frac{\delta}{\delta u} G_0(u_0)$$

In short, the roots of $G_0(u_0)$ give us the starting values for our series expansion,
and if the roots are simple, then the Implicit Function Theorem guarantees that we'll
have a unique series expansion for $u(t)$ as a function of $t$.  If any of the roots
are not simple, then we can repeat this procedure for $g(t,u)$.  It can be shown
(\cite{bliss} \S 15) that this procedure always terminates.

\vfill\eject

\begin{comment}

{\tt puiseux(f,x,y,x0,y0,deg,[ratfunc])}, adopted from {\tt
implicit_taylor} in the Maxima distribution, computes the Puiseux
series of a function {\tt f} in variables $x$ and $y$, centered around
place {\tt (x0, y0)}, to degree {\tt deg}.  An optional seventh
argument {\tt ratfunc} specifies a rational function in $x$ and $y$ to
be expanded; the default is $y$.  {\tt ratfunc} may also be a
differential.  Either {\tt x0} or {\tt y0}, or both, may be {\tt inf}
to request expansions at infinity.  {\tt y0} may be {\tt false}
to request expansions at all points of the curve lying over {\tt x0}.

{\bf Limitations.}  {\tt puiseux()} constructs the Puiseux series
coefficients using Maxima's built-in {\tt solve()} routine, which is
limited to solving polynomials using radicals.  Therefore, {\tt
puiseux()} can fail on algebraic curves of fifth degree and higher.
Also, {\tt puiseux()} does not yet implement the recursion case
described above (when $\frac{\delta g}{\delta u}$ is zero).

{\tt puiseux_fracexp} controls whether {\tt puiseux}'s results
should be returned as fractional powers in $x$, or using
an auxilary variable $t$.

{\tt newton_polygon} graphs the Newton polygon corresponding to
an algebraic curve.

Several auxilary functions are also used; these are internal to
the namespace {\tt puiseux}:

{\tt puiseux4} computes a Puiseux series corresponding to a single
place on the algebraic curve; it is called multiple times by
{\tt puiseux} if {\tt y0} is {\tt false}.

{\tt puiseux3} computes a Puiseux series corresponding to a single
line segment of the Newton polygon.

{\tt puiseux2} computes a Puiseux series of $y$ corresponding to a
single line segment of the Newton polygon.

{\tt distribute_denominator(frac, var, lim)} distributes the numerator
of a fraction over its denominator, limiting the powers of {\tt var}
to be no higher than {\tt lim}.

{\tt recenter_curve(f,x,y,x0,y0)} changes variables on an algebraic curve
so that a given point {\tt (x0,y0)} is moved to {\tt (0,0)}.

{\tt monomial_powers(f,x,y,x0,y0)} returns a list of the powers that
appear in the monomials of a recentered curve.

{\tt lower_convex_points(list)} takes a {\tt monomial_powers} list as
input and returns the subset corresponding to the lower convex hull
of the Newton polygon.

{\tt lower_convex_hull(list, x)} is used by {\tt newton_polygon}
to graph the lower convex hull.

\vfill\eject

\end{comment}

\begin{maximacode}
/* in_namespace(puiseux)$ */

recenter_curve(f,x,y,x0,y0) := block([xcoeff, newf:0],
   if x0 = inf then (f:subst(x=1/x, f), x0:0),
   if y0 = inf then (f:subst(y=1/y, f), y0:0),
   f : num(ratsimp(f)),
   f : expand(subst([y=y+y0, x=x+x0], f)),
   for xpow: 0 thru hipow(f, x) do (
      xcoeff : coeff(f, x, xpow),
      for ypow: 0 thru hipow(xcoeff, y) do
         newf : newf + radcan(ratsimp(coeff(xcoeff, y, ypow)))*x^xpow*y^ypow
   ),
   newf
);
\end{maximacode}

\begin{maximacode}
/* why doesn't this produce cdots? */
taylor(sin(x),x,0,3);
\end{maximacode}

\begin{maximacode}
monomial_powers(f,x,y,x0,y0) := block(
  [result: [], xcoeff],
  f : recenter_curve(f,x,y,x0,y0),
  for xpow: 0 thru hipow(f, x) do (
    xcoeff : coeff(f, x, xpow),
    if xcoeff # 0 then
       for ypow: 0 thru hipow(xcoeff, y) do (
          if coeff(xcoeff, y, ypow) # 0 then
             result: endcons([ypow,xpow], result)
       )
  ),
  result
);
\end{maximacode}

\begin{maximacode}
/* list is a list of pairs, like [[0,1], [0,2], [2,0]] */
/* this function returns the points on the lower convex hull */

lower_convex_points(list) := block(
   [result, nxt, slopes, lowest_slope],

   /* step 1 - sort the list first by increasing x values, */
   /*    then by increasing y values */

   list: sort(list, lambda([u,v],
      u[1] < v[1] or (u[1] = v[1] and u[2] < v[2]))),

   /* step 2 - discard all points directly above other points */

   list: lreduce(lambda([U,v],
        if last(U)[1] = v[1] then U else endcons(v,U)),
      list, [list[1]]),

   /* step 3 - start with the point on the y-axis, */
   /*    then rotate around the lower convex hull, */
   /*    adding points as we go */

   result: [pop(list)],

   while length(list) > 0 do (
      nxt: last(result),
      slopes: map(lambda([u],
         ((nxt - u)[2] / (nxt - u)[1])), list),
      lowest_slope: sort(slopes)[1],
      while (length(slopes) > 0 and slopes[1] > lowest_slope) do
         (pop(slopes), pop(list)),
      while (length(slopes) > 0 and slopes[1] = lowest_slope) do
         (pop(slopes), result:endcons(pop(list), result))
   ),
   result
);

/* this function graphs a line around the lower convex hull */

lower_convex_hull(inlist, x) := block([list],
  list: sort(inlist, lambda([u,v], u[1] < v[1])),
  while length(list) >= 2 and list[2][1] < x do pop(list),
  if length(list) >= 2 then
     list[1][2] + (x - list[1][1])
                  * (list[2][2] - list[1][2])
                  / (list[2][1] - list[1][1])
  else
     %nan
);

newton_polygon(f, x, y, x0, y0) := block(
   [newton_points, convex_hull_points, maxx, maxy, maxxy, filename],

   newton_points : monomial_powers(f,x,y,x0,y0),
   convex_hull_points : lower_convex_points(newton_points),
   maxx: lreduce(max, map(first, convex_hull_points)),
   maxy: lreduce(max, map(second, convex_hull_points)),
   maxxy: max(maxx, maxy),
   filename: next_pdf_filename(),

   plot2d([[discrete, newton_points],
           'lower_convex_hull(convex_hull_points, l)],
       [l,0,maxxy+1], [y,0,maxxy+1],
       [style, points, [lines, 5]],
       [color, red, blue],
       [point_type, bullet],
       [legend, false],
       [xlabel, false],
       [xtics, 1], [ytics, 1],
       [ylabel, false],
       [pdf_file, filename]),
   embed_latex_graphic(filename)
);

\end{maximacode}

\vfill\eject

\begin{comment}
\begin{sageblock}
from sage.geometry.newton_polygon import NewtonPolygon

def newton_polygon(f):
    return NewtonPolygon(f.exponents())
\end{sageblock}
\end{comment}

\example Construct Puiseux expansions of $y$ at the multiple points of the
curve $y^2 = 1 - x^2$

We normalize the defining polynomial by writing it as
$y^2 + x^2 - 1 = 0$.  Where does it have multiple points?
We compute the discriminant:

\begin{sageblock}
R.<x,y> = QQbar[];
(y^2 + x^2 - 1).discriminant(y).factor()
\end{sageblock}

The multiple points of $y^2 = 1 - x^2$ lie at the roots of the
discriminant, which are $x = \pm 1$.  In both cases, $y=0$ is the only
solution, so the curve has multiple points at $(x,y)=(\pm 1, 0)$.  The
partial derivative of the defining polynomial with respect to $x$
is $2x$, which is non-zero, so neither of these multiple
points are singular; we'll get ramification instead.
The analysis is almost the same in both cases, so I'll just do $(1,0)$.

First, construction of the Newton polygon requires recasting the
curve's polynomial into a form centered about the point being
analyzed, i.e, $y^2 + (x-1)^2 + 2(x-1) = 0$.  Next, we construct the Newton
polygon by plotting the monomial powers, putting the $y$ exponents on the horizontal axis and the
$(x-1)$ exponents on the vertical:

\begin{figure}[H]
\begin{center}
\maximac{newton_polygon(y^2+x^2-1, x, y, 1, 0)$}
\end{center}
\end{figure}

The only segment on the Newton polygon's lower convex hull has slope
$-1/2$ and width 2, telling us that two of our roots (the width of the
segment) will require a single Puiseux series with ramification index
2 (the denominator of the slope):

$$x=t^2+1$$

We know that y can be expressed as a power series in $t$ with
initial exponent 1 (the numerator of the slope):

$$y= a_1 t + a_2 t^2 + a_3 t^3 + \cdots$$

Now, substituting these expressions for $x^2$ and $y^2$ into the
curve's defining equation $y^2 + x^2 - 1 = 0$ and setting all
coefficients of $t$ to zero, we find:

\begin{sageblock}
var('x, y, t, a1, a2, a3');
f = y^2 + x^2 - 1;
exp = f.subs({x: t^2+1,
              y: a1*t + a2*t^2 + a3*t^3});
exp.collect(t)
\end{sageblock}

$$2 + a_1^2 = 0 \qquad 2 a_1 a_2 = 0 \qquad 1 + 2 a_1 a_3 + a_2^2 = 0$$

The first equation tells us that $a_1 = \pm\sqrt{2}i$,
the second equation tells us that $a_2=0$ and the
third equation tells us that $a_3 = \pm \frac{\sqrt{2}}{4} i$, so

% $$a_0=0; \qquad a_1 = \pm\sqrt{2}i; \qquad a_2 = 0; \qquad a_3 = \pm \frac{\sqrt{2}}{4} i$$

$$x = t^2 +1; \qquad y = \pm\left[ \sqrt{2}it + \frac{\sqrt{2}}{4} it^3 + \cdots \right]$$

It would seem that we have two different series to chose from.  This
is not really the case, as they differ by only a $180^\circ$ rotation
in the t-plane, as can been seen by substituting $t=-t$, which
transforms one of the y-series into the other, while leaving the
x-series unchanged.  Let's confirm this result with Sage:

\begin{sageblock}[ch7]
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 + x^2 - 1)
O = F.maximal_order()
pl = O.ideal(x-1, y).place()
F.completion(pl, prec=4)(y)
\end{sageblock}

This answer differs from the one we computed by hand because the
choice of uniformizing variable is not unique, and because the
computer made a different choice than we did.  Our ``$t$'' variable
was roughly $\sqrt{x-1}$.  Even though square roots don't exist {\it
per se} in this field, we can use this notation to express
the desired uniformizing variable and confirm our calculation:

\begin{sageblock}[ch7]
F.completion(pl, uvar=sqrt(x-1), prec=4)(y)
\end{sageblock}

Now, let's analyze the point at infinity.  We move infinity to a
finite point (0) with the substitution $x=u^{-1}$, then combine all of
our terms over a common denominator and discard the denominator.  Our
curve becomes:

$$y^2 u^2 + 1 - u^2 = 0$$

\begin{figure}[H]
\begin{center}
\maximac{newton_polygon(y^2+x^2-1, x, y, inf, 0)$}
\end{center}
\end{figure}

The Newton polygon's lower convex hull has a single line segment,
slope $1$, length $2$, telling us that we'll have two separate
poles, each with ramification index 1.  Thus, $u$ can be used
directly as a uniformizing variable, and we postulate an expansion for
$y$ in the form:

$$y = a_{-1} \frac{1}{u} + a_0 + a_1 u + a_2 u^2 + a_3 u^3 + \cdots$$

Plugging this into $y^2 u^2 + 1 - u^2$ and setting all the resulting
coefficients to zero, we conclude:

\begin{sageblock}
var('u, y, t, a0, a1, a2, a3');
var('an1', latex_name='a_{-1}');
f = y^2*u^2 + 1 - u^2
exp = f.subs({y: an1*(1/u) + a0 + a1*u + a2*u^2 + a3*u^3});
exp.collect(u)
\end{sageblock}

%% $$a_{-1}^2 + 1 = 0; \qquad 2 a_{-1} a_0 = 0 \qquad (2a_{-1}a_1+a_0^2-1)=0$$

$$a_{-1} = \pm i; \qquad a_0 = 0; \qquad a_1 = \mp \frac{1}{2}i; \qquad a_2 = 0; \qquad a_3 = \mp \frac{1}{8}i$$

$$y = \pm i \frac{1}{u} \mp \frac{1}{2} i u \mp \frac{1}{8} i u^3 + \cdots$$

This time, there is no ramification, since $u$, and not a power of
$u$, is $\frac{1}{x}$.  We actually have two distinct series that will
yield two different values of $y$ for each value of $u$.
Geometrically, we have two sheets that approach each other and touch
at a singular point where the curve is not locally Euclidean.

\begin{sageblock}[ch7]
Pinf = F.places_infinite()
[F.completion(pl, prec=4, uvar=1/x)(y) for pl in Pinf]
\end{sageblock}

\endexample

\example \cite{bliss} \S 68
Compute expansions at all multiple points of

$$y^3+x^3y+x=0$$

We begin by computing the discriminant of the
equation, which gives us the locations of the multiple points.

\begin{sageblock}
R.<x,y> = QQbar[];
f = y^3 + x^3*y + x
f.discriminant(y).factor()
\end{sageblock}

That result is rather confusing.  Let's try factoring over $\QQ$
instead of $\QQbar$:

\begin{sageblock}
f.discriminant(y).change_ring(QQ).factor()
\end{sageblock}

The multiple points lie over the roots of this equation: $x=0$ and
the seven roots of $4x^7+27=0$.  Infinity also needs to be
examined.  We begin with $x=0$:

\begin{sageblock}[ch7-2]
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^3 + x^3*y + x)
O = F.maximal_order()
O.ideal(x).factor()
pl = O.ideal(x, y).place()
xseries = F.completion(pl, prec=30)(x)
yseries = F.completion(pl, prec=10)(y)
yseries((xseries^(1/3)).reverse())
\end{sageblock}

This result shows that we have a single cycle at $(x,y)=(0,0)$ with
three sheets.  Now, let's look at a specimen root
of $4x^7+27=0$:

\begin{comment}
puiseux(y^3 + x^3*y +x, x, y, g, -3/(2*g^2), 1);
puiseux(y^3 + x^3*y +x, x, y, g, 3/g^2, 1);
puiseux(y^3 + x^3*y +x, x, y, g, -(3/8)^(1/7), 1);
\end{comment}

\begin{sageblock}[ch7-2]
g = QQbar(-27/4)^(1/7)
pl = O.ideal(x-g, y+3/(2*g^2)).place()
xseries = F.completion(pl, prec=3)(x)
yseries = F.completion(pl, prec=2)(y)
# This is the algebraic number that takes forever to print
# yseries(((xseries-g)^(1/2)).reverse())
pl = O.ideal(x-g, y-3/g^2).place()
xseries = F.completion(pl, prec=3)(x)
yseries = F.completion(pl, prec=2)(y)
yseries((xseries-g).reverse())
\end{sageblock}

\begin{sageblock}[ch7-3]
R1.<g> = QQ[]
S.<g> = NumberField(4*g^7+27)

R.<x> = FunctionField(S, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^3 + x^3*y + x)

O = F.maximal_order()

O.ideal(x-g).factor()

pl = O.ideal(x-g, y+3/(2*g^2)).place()
xseries = F.completion(pl, prec=3)(x)
yseries = F.completion(pl, prec=2)(y)
xseries((yseries+3/(2*g^2)).reverse())

pl = O.ideal(x-g, y-3/g^2).place()
xseries = F.completion(pl, prec=2)(x)
yseries = F.completion(pl, prec=2)(y)
yseries((xseries-g).reverse())
\end{sageblock}

We have one sheet of two cycles at $(g,-3/(2g^2))$
and an ordinary point at $(g,3/g^2)$.

Finally, let's look at what happens when $x$ goes to infinity:

\begin{sageblock}[ch7-2]
Pinf = F.places_infinite()
xseries = F.completion(Pinf[0], prec=3)(x)
yseries = F.completion(Pinf[0], prec=10)(y)

xseries = F.completion(Pinf[1], prec=3)(x)
yseries = F.completion(Pinf[1], prec=10)(y)
\end{sageblock}

Here we have an ordinary point at $(\infty,0)$ and
a single cycle of two sheets at $(\infty,\infty)$.

We have examined all of this curve's ramification points,
including those at infinity (since we analyzed all of its
points at infinity), and found that all of them admitted
a single Puiseux expansion.

Therefore, this curve is {\it non-singular}, and according to the
genus-degree formula (MORE INFO), its geometric and arithmetic genus
are the same.  Its arithmetic genus is $\frac{1}{2}(d-1)(d-2) = 3$,
where $d=4$ is the degree of the defining polynomial.  Computing
the geometric genus is more difficult\footnote{
{\tt https://www.singular.uni-kl.de/Overview/Examples/Genus/genus1.html}

{\tt https://en.wikipedia.org/wiki/Algebraic_curve\#Classification_of_singularities}

{\tt https://math.stackexchange.com/questions/150840}

{\tt http://mathforum.org/library/drmath/view/71229.html}
}, but we can verify our
information with Sage, being careful to work in {\it projective} space:

\begin{sageblock}
PP.<x,y,z> = ProjectiveSpace(QQ, 2)
C = Curve(y^3*z + x^3*y + x*z^3)
C.is_singular()
C.arithmetic_genus()
C.geometric_genus()
\end{sageblock}


\endexample

%\mysection{meromorphic functions are analytic}
%
%first, trace of a meromorphic function is meromorphic on C(x), and is
%thus a rational function
%
%Liouville's theorem: a bounded entire function is constant
%
%Proof A: (Silverman) use a Taylor series expansion around z=0, which
%is valid in the entire plane (since the function is entire).  Cauchy's
%inequality $|f| \le M ==> |c_n| \le M/{R^n}$ (eq. 10.8') as R->infty
%implies that the function is constant.
%
%Lemma: A entire function with no singularities, even at infinity, is
%constant.
%
%Proof: We can do a Taylor series expansion at the origin, whose
%non-zero terms will correspond to the principal part of the expansion
%at infinity, which must therefore be zero.
%
%Next: A entire function with only a pole at infinity is a polynomial.
%The principal part at infinity will be a polynomial.  Subtract it out
%to get a function with no singularities, which must be constant.
%
%Next: Given a function with only a finite number of finite poles,
%multiply it by a polynomial (the denominator) matching the poles with
%zeros.  Now we've got a function with only a pole at infinity, which
%must be a polynomial (the numerator).

\example Find the principal parts of $\frac{1}{y}$ on the curve
$y^2 = 1 - x^2$

The {\it principal part} of an algebraic function is the part
of its series expansion with negative exponents.  Theorem
\ref{algebraic functions are characterized by their principal parts}
states that an algebraic function is completely determined,
up to adding a constant, by its principal parts.

The first step is to locate the function's poles, which in this case is
simply the places where the denominator is zero, and that's just
$x=\pm 1$.  Now, if we use {\tt puiseux}, we can just request a series
truncated at the $-1$ term:

\begin{sageblock}[ch7-4]
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 + x^2 - 1)
D = (1/y).divisor()
table([[p, F.completion(p, prec=0)(1/y)] for p,m in D.list() if m < 0])
\end{sageblock}

\endexample

\example Find the principal parts of $\frac{x}{y} \ud x$ on the curve
$y^2 = 1 - x^2$

Differential forms are not functions, and have different series
expansions, due to the presence of the
differential, which must be adjusted at ramification points.

Let's expand $\frac{x}{y}$ at $x=1$:

\begin{sageblock}[ch7-5]
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 + x^2 - 1)
O = F.maximal_order()
pl = O.ideal(x-1, y).place()
F.completion(pl, prec=6)(x)
F.completion(pl, prec=6)(x/y)
\end{sageblock}

Now $x=t^2+1$, so $\ud x=2t\ud t$.  Thus, multiplying $\frac{x}{y}$
by $\ud x$ and changing our variable to $t$ will multiply
all of the terms in our expansion by $2t$:

\begin{sageblock}[ch7-5]
F.completion(pl, prec=6)(x/y*x.differential())
\end{sageblock}

Even though $\frac{x}{y}$ has a pole
at $x=1$, $\frac{x}{y} \ud x$ does not!

Its behavior at infinity also requires analysis.

\begin{sageblock}[ch7-5]
Pinf = F.places_infinite()
F.completion(Pinf[0], prec=2)(x)
F.completion(Pinf[0], prec=2)(y)
F.completion(Pinf[0], prec=2)(x/y)
F.completion(Pinf[0], prec=2)(x/y*x.differential())
\end{sageblock}

$\frac{x}{y}$ has no poles at infinity, and approaches
the limiting values $\pm i$ as $x$ and $y$ approach
infinity.  The differential $\frac{x}{y} \ud x$,
on the other hand, requires us to multiply by $\ud x$,
and since $x=\frac{1}{t}$, $\ud x = - \frac{1}{t^2} \ud t$.

In short, while $\frac{x}{y}$ has poles only at $(\pm 1,0)$,
$\frac{x}{y} \ud x$ has poles only at infinity.

\begin{sageblock}[ch7-5]
v = x/y*x.differential()
D = v.divisor()
table([[p, F.completion(p, prec=0)(v)] for p,m in D.list() if m < 0])
\end{sageblock}

\endexample

\begin{comment}

\example
Compute expansions at all multiple points of
the exercises in \cite{bliss} \S 68.

To facilitate this example, let's define an
auxiliary function to perform the analysis:

\begin{sageblocksmall}
def analyze_multiple_points(f, pr=False):
   P = f.parent()
   Base = P.base_ring()
   x,y = P.gens()
   # This next code is here to avoid Trac #25271, though it assumes
   # that the curve's polynomial has only rational coefficients.
   # (which is true for all of our test cases)
   if False:
      disc = f.discriminant(y)
   else:
      QQR = QQ[x,y]
      disc = P(QQR(f).discriminant(QQR(y)))
   if pr: print '$$', latex(disc.factor()), '$$'

   for x0 in Base[x](disc).roots(multiplicities=False):
      sheets = 0
      for y0 in Base[y](f.subs({x: x0})).roots(multiplicities=False):
          p = puiseux(f, x0, y0, 3)
          for cycle in p:
             assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
             if cycle[0] == x0:
                sheets = sheets + 1
             else:
                sheets = sheets + (cycle[0] - x0).valuation()
          if pr: print '$$', latex((x0,y0)), '$$'
          if pr: print '$$', latex(p), '$$'
      if f.subs({x: x0}).degree(y) != f.degree(y):
          p = puiseux(f, x0, oo, 3)
          for cycle in p:
             assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
             if cycle[0] == x0:
                sheets = sheets + 1
             else:
                sheets = sheets + (cycle[0] - x0).valuation()
          if pr: print '$$', latex((x0,oo)), '$$'
          if pr: print '$$', latex(p), '$$'
      assert(sheets == f.degree(y))

   finf = Base[y](f.subs({x: 1/x}).numerator().subs({x:0}))
   for y0 in finf.roots(multiplicities=False):
      p = puiseux(f, oo, y0, 3)
      for cycle in p:
         assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
      if pr: print '$$', latex((oo,y0)), '$$'
      if pr: print '$$', latex(p), '$$'
   if finf.degree() < f.degree(y):
      p = puiseux(f, oo, oo, 3)
      for cycle in p:
         assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
      if pr: print '$$', latex((oo,oo)), '$$'
      if pr: print '$$', latex(p), '$$'
\end{sageblocksmall}

\begin{sageblock}
test_curves = [
    y^3 - 3*y + 2*x,
    y^3 + 3*y - x,
    y^3 - 3*y^2 - x,
    y^4 - 4*y - x,
    y^4+2*(1-2*x)*y^2+1,
    y^3-3*y^2+x^6,
    y^3-3*y+2*x^2*(2-x^2),
    y^3-3*y+2*x^3*(2-x^3),
    3*x*(x-1)*y^4 -4*(x-1)*(x-2)*y^3 + (4/27)*(x-2)^4,
    y^5 + (x^2-1)*y^4 - (4^4)/(5^5)*x^2*(x^2-1),
    y^3 - x*y - x^2,
    y^3 - 3*x^2*y + 2*x,
    y^3 - 3*x*y + 2*x^2,
    y^3 - 3*y + x^6];

# for f in test_curves:
#     analyze_multiple_points(f)
\end{sageblock}

%\begin{sageblocksmall}
%analyze_multiple_points(y^3-3*y+2*x^3*(2-x^3), True);
%\end{sageblocksmall}

$$y^3 - 3axy + x^3$$

This function contains an extra variable and is, in fact, a family of
algebraic curves.

\endexample

\end{comment}

% A {\it Riemann surface}, more precisely, is a two-dimensional manifold
% with a {\it complex analytic structure}.

\vfill\eject
\mysection{Places and Divisors}

We've seen how to construct Puiseux expansions at arbitrary points of
an algebraic curve, but some points have multiple expansions,
corresponding to multiple cycles on their corresponding surfaces.

We now seek some mechanism for distinguishing between multiple cycles
at a single point.  To this end, we introduce the concept of a {\it
place}.  Intuitively speaking, a place is a cycle, and places are in
one-to-one correspondence with Puiseux expansions.  Therefore, we
can handle singularities by thinking in terms of Puiseux expansions
at cycles.  Non-singular points have a unique place associated
with them, while there are multiple places (and multiple cycles)
associated with a singular point.

We can characterize places algebraically, both to formalize this
concept and also to analyze them in a manner that will generalize
in Chapter 10 to arbitrary algebraic extensions.

Places correspond to ideals of maximal orders of the curve's
function field.

Given a function on an algebraic curve, we can ask at which places it
has poles and zeros.  The location and strengths of a function's
poles and zeros are called its {\it divisor}.

For non-singular curves, the points and places are in one-to-one
correspondence, and a function's divisor can be described in terms
of the points where its poles and zeros lie.

Thus, one way of defining a divisor is to associate integers (positive
for zeros, negative for poles) with each point of the curve, subject
to the stipulation that all but a finite number of those integers is
zero.  Such a description is called a {\it Weil divisor}, and is most
suitable for working in {\it intersection theory}.

For singular curves, the situation is more complicated.  A divisor
needs to be associated with places, not points.  Such a divisor is
called a {\it Cartier divisor}, and is more suitable for our purposes.


\vfill\eject
\mysection{Riemann-Roch spaces}

A {\it Riemann-Roch space} is a subspace of an algebraic curve's
function field characterized by specifying a minimum order that the
function must obtain at all of the curve's points.  Aside from having
great theoretical significance, Riemann-Roch spaces are practically
useful because they are finite dimensional, and algorithms exist for
constructing Riemann-Roch bases.  Finding a basis for a Riemman-Roch
space in a crucial first step in solving a Mittag-Leffler problem.

Numerous algorithms have been developed for computing bases of
Riemann-Roch spaces.  Sage uses an implementation of Hess's algorithm
from \cite{hess}.

\begin{comment}
I've implemented in Maxima one of the oldest,
from \cite{bliss}, though it probably dates back
to \cite{dedekind-weber}.

We begin the process with a ${\mathrm C}(x)$-basis for the entire
function field, namely $\{1, y, \ldots, y^{n-1}\}$.

Next, we want to convert this into a ${\mathrm C}[x]$-basis for the
finite portion of the divisor.  First, we multiple the basis by
whatever polynomials in $x$ are required to place the basis elements
into the divisor's function space, then for each value of $x$ form
a matrix of coefficients, and keep reducing until its determinant is zero.

Finally, we need to adjust this basis to match the divisor's requirements at infinity.

A divisor's basis can be transformed to another basis for the same
divisor by multiplying by a matrix in ${\mathrm C}[x]$ with
determinant a constant not equal to zero. (Bliss Th. 21.1)

If we have a cycle at infinity, multiplying by x will multiply
the expansions by (1/t^r).

{\tt riemannroch(f,x,y,divisor)} computes a basis for the Riemann-Roch
space $L(D)$.  {\tt divisor} is a list of elements, each in the form
{\tt [[$x_i$, $y_i$], $\nu_i$]}, where $(x_i, y_i)$ is a point on the
curve, and $\nu_i$ is the order of the divisor at that point.  For
singular points, either the standard syntax can be used, indicating
that the order of the divisor is the same at all points of the
singularity, or $\nu_i$ can be replaced with a list of values, one for
each sheet at the singularity.  The order of sheets is the same
returned by {\tt puiseux}.  Specifying multiple orders at
singularities with cycles is currently not supported.

\end{comment}

Here's a simple example\footnote{From
{\tt https://math.stackexchange.com/questions/294644}}
of a Riemann-Roch space calculation:

\begin{sageblock}[riemannroch]
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 - x^3 + x)

O = F.maximal_order()
P = O.ideal(x,y)
D = P.divisor()

D.basis_function_space()
(2*D).basis_function_space()
(3*D).basis_function_space()
(4*D).basis_function_space()
\end{sageblock}

Here are the examples from \cite{alvanos} \S6.3:

\begin{sageblock}[riemannroch]
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 - x^3 - 1)

O = F.maximal_order()
P1 = O.ideal(x-2,y-3)
P2 = O.ideal(x-2,y+3)
Pinf = F.places_infinite()[0]

D1 = P1.divisor()
D2 = P2.divisor()
Dinf = Pinf.divisor()

(Dinf-D1).basis_function_space()
(2*Dinf-D1).basis_function_space()
(3*Dinf-D1).basis_function_space()
(4*Dinf-D1).basis_function_space()
(Dinf).basis_function_space()
(2*Dinf).basis_function_space()
(3*Dinf).basis_function_space()
\end{sageblock}

\vfill\eject
\mysection{Mittag-Leffler Problems}

Theorem ? tells us that a rational function on an algebraic
curve is completely characterized, up to an additive constant,
by the principal parts of the Puiseux expansions at its poles.

A {\it Mittag-Leffler problem} is the practical application of this
theorem -- given a set of principal parts, find a function that
matches them all, or prove that no such function exists.

The first step in solving a Mittag-Leffler problem is to identify the
maximum strengths of the poles, and construct a basis for a
Riemann-Roch space that includes all functions with poles of such
strength.  We now have a finite basis for a vector space that must
include the function we are looking for.  We construct Puiseux
expansions for the basis functions, and use them to construct
a matrix equation that, when solved, gives the coefficients
needed to form the function we seek from the basis functions.

The input data is a set of principal parts or, alternately, a divisor
combined with a vector of coefficients.

Let's assume that we've got our data in the latter form, so we can run
{\tt riemannroch} on the divisor and obtain a set of basis functions.
Now let's construct a Sage function to extract the principal parts
of the basis functions and form them into a matrix:

\begin{sageblock}[riemannroch]
def principal_parts_matrix(div, basis):
    F = div.parent().function_field()
    coeffs = [(F.completion(p, prec=0), i) for p,m in div.list() for i in range(-m,0)]
    return matrix([[c[0](b)[c[1]] for c in coeffs] for b in basis]).transpose()
\end{sageblock}

Given a vector {\tt b} of coefficients, we now want to
solve a matrix equation:

$$m \cdot v = b$$

This will typically be an overspecified system -- a non-square matrix
that may or may not have a solution.  That's fine; since some
integrals have no elementary form, this doesn't represent a limitation
in our theory.  Failure to solve this matrix equation would only show
that no function exists on this curve with the coefficients {\tt b}.

To proceed, we'll use the
{\it Moore-Penrose pseudoinverse}\footnote{\tt https://en.wikipedia.org/wiki/System_of_linear_equations\#Matrix_solution},
which has the property that if a solution exists, it can
be found by multiplying the $b$ vector by the pseudoinverse.

To find out if there actually is a solution, we first compute a trial
solution by multiplying the pseudoinverse by {\tt b}, then checking
to see if the trial solution actual solves the original equation.

\example
Let's say that we've identified a divisor on an algebraic
curve (example \ref{an integral Maxima can't solve}):

We now compute its principal parts matrix:

\begin{sageblock}[riemannroch]
R.<x> = FunctionField(QQbar, implementation='kash')
L.<y> = R[]
F.<y> = R.extension(y^2 - x^8 - 1)

# do this to force the extension code to run
y.divisor();

O = F.maximal_order()
Oinf = F.maximal_order_infinite()

Dfinite = add([O.ideal(x-a*QQbar(-1).sqrt(), y-b*QQbar(2).sqrt()).place().divisor() for a in [-1, 1] for b in [-1, 1]])
Dinf = add([pl.divisor() for pl in F.places_infinite()])

D1 = Dfinite + 2*Dinf

basis = Dfinite.basis_function_space()

D1.basis_function_space()

principal_parts_matrix(D1, basis)
\end{sageblock}

{\bf TODO}

Introduce a sample vector {\tt b} and show how to proceed.

\endexample

\mysection{Parallels with the Transcendental Cases}

At this point, it may seem that we've spent this entire chapter
developing a suite of technical tools that appear completely different
from everything that came before them.  Why should the algebraic case
be so much different from the transcendental cases?  What would happen
if we used here the same kind of techniques from earlier in the book?

First, the key difference in the algebraic case is the lack of unique
factorization.  Algebraic extensions are not, in general, unique
factorization domains, a classic example being the factorization of
$6$ into either $3\cdot 2$ or $(1+\sqrt{-5})\cdot(1-\sqrt{-5})$ in the
ring $\ZZ[\sqrt{-5}]$.  You can show that all four numbers $3$, $2$,
$(1+\sqrt{-5})$ and $(1-\sqrt{-5})$ are all prime in $\ZZ[\sqrt{-5}]$,
so we have two distinct factorizations in this ring.

{\bf Show an example in a function field.}

The main problem with our earlier tools is the difficulty in defining
factorization.  How, for example, do you construct a partial fractions
expansion?  A review of Theorems \ref{logarithmic integration theorem}
and \label{exponential integration theorem} reveals that both depend
not merely on the construction of a partial fractions expansions, but
also on its {\it uniqueness}.  Without unique factorization, how can
you possibly have a unique partial fractions expansion?

The primary goal of this chapter is to develop techniques to carry
out the same kinds of operations we did earlier, but without relying
on unique factorization.

For example, a principle parts expansion of a function on an algebraic
curve is exactly analogous to a partial fractions expansion of a
rational function.

{\bf Demonstrate}

Although we began our development using infinite series expansions, we
ultimately concluded that we can completely specify a function (up to
an additive constant), using only a finite number of constants -- the
principle parts coefficients, which turn out to align precisely with
the coefficients in a partial fractions expansion.

Reassembling a partial fractions expansion into a rational function is
easy -- you just promote all the fractions to a common denominator,
add up the terms, and cancel any common factors that remain between
the numerator and the denominator.  Solving a Mittag-Lefler problem is
considerably more difficult, but is in principle the same operation --
given the principle parts coefficients (resp. the partial fractions
expansion), construct a single rational function that matches.  The
major caveat here is that, unlike reassembling a partial fractions
expansion, there might be not solution.  Not every principle parts
expansion has a matching algebraic function.

Likewise, finding an algebraic function's divisor is exactly analogous
to factoring the numerator and denominator of a rational function.
You get a finite set of poles and zeros with their locations and
multiplicities.  Again, in the algebraic case, it's more complicated --
you might have singularities with multiple places lying over a single
point; the ``coordinate'' is more complicated that a simple $(x,y)$
coordinate pair, but the principle is the same.

And finding a function with a specified set of poles and zeros is the
same as taking a rational function in factored form and multiplying
the factors together again.  Again, there's a caveat -- in the
algebraic curve case there might be no solution.

So, if the tools we've developed in this chapter parallel neatly with
the tools we used in Chapter 4 to solve integrals of rational
functions, can we generalize these tools to handle more complicated
transcendental fields, like we did in Chapters 5 and 6?  And do we
have anything like the Hermite reduction procedure we developed at the
end of Chapter 4?

The answers to both of these questions is 'yes'.  For the purpose of a
clear exposition, I've developed this theory so far in its simplest
form, and if you're seeing it for the first time, I suspect that you
already appreciate not having met it in its full generality!  We can
drop the assumption of an algebraically closed coefficient field and
lose very little except simplicity; this will be the subject of
Chapter 10.  Barry Trager showed in \cite{trager} how the Hermite
reduction can be performed in an algebraic extension; it's now called
{\it Hermite-Trager reduction} and I'll present it at the end of
Chapter 8.

However, continuing with the intent of presenting the theory in its
simplest form first, we'll begin the next chapter by looking at how to
use these tools to integrate Abelian integrals, much like we first met
partial fractions expansion when learning to integrate in first year
calculus, and only later generalized it into a form suitable for
integrating in arbitrary transcendental extensions.  We'll find that
completely solving integrals in even this simplest of algebraic
extensions will require a significant excursion into modern algebraic
geometry, so much so that the entirety of Chapter 9 will be devoted to
proving the book's most exotic theorem.

If there's a lesson to be learned from Chapter 7, though, it's this:

\begin{key point}
Divisors, principle parts expansions, Riemann-Roch spaces, and Mittag
Leffler problems are how we do factorization, partial fractions
expansions, and their inverse operations in algebraic extensions where
we've lost unique factorization.
\end{key point}
