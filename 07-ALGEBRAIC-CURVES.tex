
\mychapter{Algebraic Curves}

Having addressed logarithmic and exponential extensions, we now turn
to the algebraic extension, which turns out to be completely different
in character from the two transcendental cases.

\begin{comment}

To justify that statement, let's begin by trying to attack algebraic
extensions in the same manner as the two transcendental cases.

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i (\theta^{i})')$$

Presumably, we know how to differentiate $\theta$, so let's write
$(\theta^i)' = \sum_j \psi_{i,j} \theta^j$:

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i \sum_j \psi_{i,j} \theta^j)$$

Writing the $a_i$'s as a column vector $A$, we'll end up with a matrix equation:

$$I = A' + \Psi A$$

This is superficially similar to a Risch equation; it's a matrix Risch
equation.  To proceed in the same manner as before, we'd now have to
construct partial fractions expansions of everything and begin looking
at how cancellation might occur between our various components.  The
variety of possible cancelations between our terms seems daunting.

\vfill\eject

$$\int y\,dx \qquad y^2 = 4-x^2$$

Let's assume that our solution has the form $S = ay+b+\ln (cy+d)$, where $a$
and $b$ are rational functions in $x$ and $c$ is a polynomial in $x$.

$$S' = a' y + a y' + b' + \frac{c'y+cy'+d'}{cy+d}$$
$$2 y y' = -2x \qquad y' = -\frac{x}{y} = - \frac{x}{4-x^2}y$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}\frac{cy-d}{cy-d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})cy^2+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2y^2-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})c(4-x^2)+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c'(4-x^2) - c x)c+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2} + \frac{cd-(c' - c \frac{x}{4-x^2})d}{c^2(4-x^2)-d^2})y + b' + \frac{(c'(4-x^2) - c x)c -d^2}{c^2(4-x^2)-d^2}$$


The correct answer is

$$S = 2i\ln(ix-y)+\frac{x}{2}y$$

Instead, we'll use a different approach...

\vfill\eject

\end{comment}

How might we handle a simple algebraic extension?  A crucial property
of {\it algebraic functions}, as elements of an algebraic extension
are called, is that they admit series expansions everywhere, including
infinity, so long as we allow a finite number of negative exponents.
Such functions are called {\it meromorphic}.  The logarithm function
fails to be meromorphic at the origin, and the exponential function
fails to be meromorphic at infinity, but algebraic functions are
meromorphic everywhere, including infinity.

This means that around any specific point, we can construct a series
expansion of the integrand and integrate termwise to obtain a series
expansion for the integral.  At first this doesn't seem terribly
useful, because series expansions are infinite and we're trying to
construct closed-form solutions, but it turns out that only a finite
number of places will have negative exponents in their series
expansions and that the function is completely specified, up to an
additive constant, by the coefficients of the negative powers.

Thus, the basic strategy is first to identify the function's {\it
poles}, the places where its value becomes infinite, and compute the
{\it principal part} of the series expansions there, which are the
negative exponents and their coefficients.  This is fairly
straightforward, though there are issues of computational complexity
that make it non-trivial.  Then we integrate termwise, which is
trivial, and obtain local series expansions at the poles of the
solution.  Next, we need to reassemble this local information into a
global function (if one exists), a {\it Mittag-Leffler problem}, for
which I will present a basic algorithm in this chapter, although more
efficient techniques have been developed.

What about the logarithmic terms?  This turns out to be the most
difficult part of the problem.  We can begin to analyze them using the
same techniques, by noting that the $t^{-1}$ terms in the principal
parts of the integrand lead directly to logarithms in the integral,
and furthermore that the coefficients of these terms give us the
locations and orders of the poles and zeros in the logarithms.  This
information specifies an algebraic function up to a multiplicative
constant\footnote{Of course.  Due to the presence of a constant of
integration, we expect to specify the main part of the integral up to
an {\it additive} constand, and the logarithmic parts of the integral
up to a {\it multiplicative} constant.}, and our algorithm can be
adapted without too much trouble to handle this case.

The problem is that no algebraic function might exist that match a
given set of zeros and poles, but increasing the order of the zeros
and poles might produce a solution.  This corresponds to raising the
logarithm term to powers, i.e, $\ln f$ is the same as $\frac{1}{2} \ln
f^2$, which is the same as $\frac{1}{3} \ln f^3$, except that in our
case the lower powers might not exist in our function field, even
though higher powers do.  What powers should we use?  We could go on
raising to higher and higher powers, hoping that something will work,
but the only known algorithm to limit this search requires reducing
modulo a prime, and that requires techniques that weren't developed
until the 1960s.  Before heading into {\it modern algebraic geometry},
let's see how far we can get with the classical algebraic geometry of
the nineteenth century.

\section{Basic Algebraic Geometry}

The roots of algebraic geometry lie in studying the zeros of
polynomial equations.  We began with a single polynomial in a single
variable, and have learnt a great deal about it.  We know how to solve
it (at least in terms of radicals) if its degree is less than 5.
Galois proved that no such solution (in radicals) exists (in the
general case) for larger degree, though abstract algebra provides us
with a suitable general theory to handle this case.  Simple long
division tells us that it can have no more roots than its degree, and
Gauss showed that all of the roots exist as complex numbers --- the
Fundamental Theorem of Algebra.

The next logical step is to consider zeros of a single polynomial in
two variables, and this equation has also received a great deal of
attention from mathematicians.  Like the univariate case, we have
theories devoted to low-order special cases --- {\it linear equations}
(all terms first degree or constant), the {\it conic sections} (all
terms second degree or less), and the {\it elliptic curves} (one term
third degree; all others second degree or less).  In the general case,
$\sum a_{ij} x^i y^j = 0$ is called an {\it algebraic curve}, and a
rational function in $x$ and $y$ is called an {\it algebraic
function}.  These will be our main focus of attention in this chapter.

The first problem we face when dealing with algebraic curves is the
multi-valued nature of their solutions.  Consider, once again, the
algebraic function $y$ defined on the algebraic curve $y^2 = x^2 - 1$.
There are, in fact, two seperate algebraic functions that solve this
equation --- both $y$ and $-y$ are solutions.  Conventionally, we
express this by writing something like $y = \pm\sqrt{x-1}$, but for
higher degree curves this kind of notation becomes unsuitable.  How,
for example, do you express the three possible solutions to a cube
root, and how do you deal with the general case where y can appear
multiple times in the curve's defining polynomial?

Our solution to this problem is to regard the entire algebraic curve
as a two-dimensional surface in a four-dimensional space.  Why four
dimensions?  Well, just as in the univariate case, we find it
convenient to work with complex numbers, so as to deal easily with
roots of negative numbers.  Regarding both $x$ and $y$ as complex
numbers (two dimensions each), and plotting them against each other,
we obtain a four dimensional space.  Just as in the real case, where
an equation like $x^2 + y^2 = 1$ defines a circle, an algebraic curve
defines a surface, the loci of $x$ and $y$ that satisfy the defining
polynomial.

The defining polynomial can be regarded as a polynomial in $y$, whose
coefficients are polynomials in $x$, simply by collecting terms with
like powers of $y$.  For any given value of $x$, we have a polynomial
in $y$ with complex coefficients that yields at most $n$ solutions.
We can be more specific.  For any given value of $x$, we have {\it
exactly} $n$ solutions for $y$ {\it unless} one of two things happen.
Either the leading ($y^n$) coefficient is zero, in which case we have
less than $n$ solutions due to having a polynomial of degree less than
$n$, or the polynomial has multiple identical roots, a {\it multiple
point} of the algebraic curve.

There are only a finite number of multiple points, as can be seen by
considering the {\it discriminant} of the defining polynomial, which
is the resultant of the polynomial with its partial derivative.  The
discriminant will be zero at multiple points, so these points can be
located by computing the zeros of a univariate polynomial.

\begin{maximablock}
discriminant(f,y) :=
  resultant(diff(f,y), f, y)$
\end{maximablock}

Multiple points are further classified according to whether or not the
curve is locally Euclidean in their neighborhood.  Geometrically, this
corresponds to looping around the point until you return to your
starting point.  If a single such {\it cycle} covers all the sheets of
the curve, the curve is locally Euclidean, and we have an {\it
ordinary point} of the curve, albeit one with {\it ramification}, the
{\it ramification index} being how many times we had to circle the
point.  Otherwise, multiple cycles are required to cover all of the
sheets, and we have a {\it singular point}.  Analytically, both
partial derivatives of the curve's polynomial are zero at a singular
point, while at least one is non-zero at ordinary points.  This
analysis is facilitated by Newton polygons.

Now, the coefficient of $y^n$ in the defining polynomial will be a
polynomial in $x$, which has a finite number of roots at which it is
zero, so there are only a finite number of points where the defining
polynomial is of degree less than $n$ in $y$.  As $x$ approaches one
of these points, the value of the $y^n$ coefficient approaches zero,
which causes at least one of the roots to approach infinity.  We'll
deal with these points by introducing a line at infinity, forming
{\it projective space} and creating a {\it compact} surface.

\begin{maximablock}
recenter_curve(f,x,y,x0,y0) := block([xcoeff, newf:0],
   if x0 = inf then (f:subst(x=1/x, f), x0:0),
   if y0 = inf then (f:subst(y=1/y, f), y0:0),
   f: num(ratsimp(f)),
/*
   if expand(subst([y=y0, x=x0], f)) # 0 then
      error("attempt to recenter to point not on curve"),
*/
   f : expand(subst([y=y+y0, x=x+x0], f)),
   for xpow: 0 thru hipow(f, x) do (
      xcoeff : coeff(f, x, xpow),
      for ypow: 0 thru hipow(xcoeff, y) do
         newf : newf + radcan(ratsimp(coeff(xcoeff, y, ypow)))*x^xpow*y^ypow
   ),
   newf : subst([y=y-y0, x=x-x0], newf),
   [newf, x0, y0]
)$
\end{maximablock}

\begin{maximablock}
/* recenter_curve(y^2=x^2-1, x, y, 1, 0); */
recenter_curve(y^2-(x^2-1), x, y, 1, 0);
recenter_curve(y^2-(x^2-1), x, y, 0, %i);

recenter_curve(y^2-(x^2-1), x, y, inf, inf);

/* errcatch(recenter_curve(y^2-(x^2-1), x, y, 0, 0)); */

/* why doesn't this produce cdots? */
taylor(sin(x),x,0,3);
\end{maximablock}

\begin{maximablock}
monomial_powers(f,x,y,x0,y0) := block(
  [result: [], xcoeff],
  [f,x0,y0] : recenter_curve(f,x,y,x0,y0),
  for xpow: 0 thru hipow(f, x-x0) do (
    xcoeff : coeff(f, x-x0, xpow),
    if xcoeff # 0 then
       for ypow: 0 thru hipow(xcoeff, y-y0) do (
          if coeff(xcoeff, y-y0, ypow) # 0 then
             result: endcons([ypow,xpow], result)
       )
  ),
  result
)$
\end{maximablock}

\begin{maximablock}
monomial_powers(y^2-(x^2-1), x, y, 1, 0);
monomial_powers(y^2-(x^2-1), x, y, 0, %i);
monomial_powers(y^2-(x^2-1), x, y, inf, inf);
\end{maximablock}

\begin{maximablock}
/* list is a list of pairs, like [[0,1], [0,2], [2,0]] */
/* this function returns the subset of points on the lower convex hull */

lower_convex_points(list) := block([result, nxt, slopes, lowest_slope],
   /* step 1 - sort the list first by increasing x values, then by increasing y values */
   list: sort(list, lambda([u,v], u[1] < v[1] or (u[1] = v[1] and u[2] < v[2]))),
   /* step 2 - discard all points directly above other points */
   list: lreduce(lambda([U,v], if last(U)[1] = v[1] then U else endcons(v,U)), list, [list[1]]),
   /* step 3 - start with the point on the y-axis */
   result: [pop(list)],
   /* step 4 - rotate around the lower convex hull, adding points as we go */
   while length(list) > 0 do (
      nxt: last(result),
      slopes: map(lambda([u], ((nxt - u)[2] / (nxt - u)[1])), list),
      lowest_slope: sort(slopes)[1],
      while (length(slopes) > 0 and slopes[1] > lowest_slope) do (pop(slopes), pop(list)),
      while (length(slopes) > 0 and slopes[1] = lowest_slope) do (pop(slopes), result:endcons(pop(list), result))
   ),
   result
)$

/* this function graphs a line around the lower convex hull */

lower_convex_hull(inlist, x) := block([list],
  list: sort(inlist, lambda([u,v], u[1] < v[1])),
  while length(list) >= 2 and list[2][1] < x do pop(list),
  if length(list) >= 2 then list[1][2] + (x - list[1][1]) * (list[2][2] - list[1][2])/(list[2][1] - list[1][1]) else %nan
)$

newton_polygon(f, x, y, x0, y0) := block([newton_points, maxx, maxy, maxxy, filename],
   newton_points : monomial_powers(f,x,y,x0,y0),
   convex_hull_points : lower_convex_points(newton_points),
   maxx: lreduce(max, map(first, convex_hull_points)),
   maxy: lreduce(max, map(second, convex_hull_points)),
   maxxy: max(maxx, maxy),
   filename: next_pdf_filename(),
   plot2d([[discrete, newton_points], 'lower_convex_hull(convex_hull_points, l)], [l,0,maxxy+1], [y,0,maxxy+1],
       [style, points, [lines, 5]], [color, red, blue],
       [point_type, bullet],
       [legend, false],
       [xlabel, false],
       [xtics, 1], [ytics, 1],
       [ylabel, false],
       [pdf_file, filename]),
   embed_latex_graphic(filename)
)$

recenter_curve(y^2-x^2-1, x, y, 0, 1);
monomial_powers(y^2-x^2-1, x, y, 0, 1);
\end{maximablock}

\example Locate and characterize the multiple points of $y^2 = x^2 - 1$

We normalize the defining polynomial by writing it as $y^2 - x^2 + 1 =
0$, and begin by noting that the coefficient of $y^2$ is 1, so the
defining polynomial is second degree for all finite values of $x$.
Where does it have multiple roots?  We compute the discriminant:

\begin{maximablock}
discriminant(y^2-x^2+1, y);
\end{maximablock}

The multiple points of $y^2 = x^2 - 1$ lie at the roots of the
discriminant, which are $x = \pm 1$.  The partial derivative of the
polynomial with respect to $x$ is $-2x$, which is non-zero, so neither
of these multiple points are singular.

\begin{figure}[H]
\begin{center}
\maximac{newton_polygon(y^2-x^2-1, x, y, %i, 0)$}
\end{center}
\end{figure}

What about the points at infinity?  Introducing the subsitutions
$u=x^{-1}$ and $v=y^{-1}$, our curve becomes $u^2 - v^2 + u^2 v^2 =
v^2 (u^2 - 1) + u^2 = 0$, which has a multiple point at $(0,0)$,
since when $u=0$, both of the roots of $-v^2 = 0$ are identical.
Also, both partial derivatives are zero, so this is a singular
point of the curve.

\endexample

Any ordinary point can be expanded using a power series in
$(x-\alpha)$, which for non-ramified points is a straightforward
application of the Implicit Function Theorem.

IFT: [Baby Rudin 9.28; 2-dim complex version] Let f be an analytic
mapping of an open set $E \in {\mathbb C}^2$ into ${\mathbb C}$, such
that $f(x,y)=0$ and $\frac{df}{dy} \ne 0$, then an analytic
function $g(x)$ exists such that $f(x,g(x))=0$.

For infinity and/or poles, substitute z=1/x or v=1/y.

For ramification points, we use substitution of the form
$x=t^r+\alpha$, where $r$ is the {\it ramification index}, then use
composition of analytic functions (x is analytic everywhere; y is
analytic as a function of x everywhere except at t=0, so y is analytic
as a function of t everywhere except at t=0) to establish that y is
analytic everywhere on the t-plane except possibly at the origin.
Then use existence of the Laurent series (Silverman 11.2) and
continuity of the roots (HOW?) to establish analyticity at the
multiple point, and consequently existance of a power series, but in
$t$, not $(x-\alpha)$, a {\it Puiseux series}.

Singular points will admit multiple Puiseux series, each one
corresponding to a single cycle.  The simplest way to compute Puiseux
series is to use Newton polygons to determine ramification,
then setup a trial series with the correct ramification and
substitute it into the curve's defining equation.

{\tt puiseux}, adopted from {\tt implicit_taylor} in the Maxima
distribution, computes the Puiseux series of a function {\tt f} in
variables $x$ and $y$, centered around place {\tt (x0, y0)},
to degree {\tt deg}.

{\tt puiseux2} is an auxilary function that computes a Puiseux series
corresponding to a single line segment of the Newton polygon.  Letting
$\alpha$ be the $x$ exponent and $\beta$ be the $y$ exponent, so the
monomials in $f$ have the form $x^\alpha y^\beta$, then the equation
of the line segment is $r\alpha + s\beta = p$, where $r$, $s$, and $p$
are integers and $r$ and $s$ are relatively prime.  Making the 
substitution $x=t^r$ and $y=t^s u(t)$, we obtain:

$$f(x,y) = \sum A_{\alpha\beta} x^\alpha y^\beta$$
$$ = \sum A_{\alpha\beta} t^{r \alpha} t^{s \beta} u(t)^\beta$$
$$ = t^p \underbrace{\sum A_{\alpha\beta} t^{r \alpha + s \beta - p} u(t)^\beta}_{g(t,u)}$$

At least two of the $(r \alpha + s \beta - p)$ exponents will be zero
(those monomials corresponding to the endpoints of the line segment on
the Newton polygon); all of the remaining exponents will be positive.
This means that if we expand $u(t)$ in a power series in $t$:

$$u(t) = u_0 + u_1 t + u_2 t^2 + u_3 t^3 + \cdots$$

then any power of $u(t)$ will have the form:

$$u(t)^\beta = U_0(u_0) + U_1(u_0, u_1) t + U_2(u_0, u_1, u_2) t^2 + U_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

then $g(t,u)$ will also have the form:

$$g(t,u) = G_0(u_0) + G_1(u_0, u_1) t + G_2(u_0, u_1, u_2) t^2 + G_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

In order for $g(t,u)=0$ at $t=0$, $G_0(u_0)$ must be zero, and since $G_0(u_0)$ is a polynomial
in $u_0$, this gives us a finite number of values for $u_0$ that can solve our equation.

Now, by setting $g(t,u)=0$, can we obtain $u(t)$ as a function of $t$?

The Implicit Function Theorem states that we can, if $\frac{\delta g}{\delta u}$ is not zero
at the point we wish to expand around.

$$\frac{\delta g}{\delta u}(0,u_0) = \frac{\delta}{\delta u} G_0(u_0)$$

In short, the roots of $G_0(u_0)$ give us the starting values for our series expansion,
and if the roots are simple, then the Implicit Function Theorem guarantees that we'll
have a unique series expansion for $u(t)$ as a function of $t$.  If any of the roots
are not simple, then we can repeat this procedure for $g(t,u)$.  It can be shown
(\cite{bliss} \S 15) that this procedure always terminates.

\begin{maximablocksmall}
puiseux2(f,x,y,x0,y0,deg,r,s,p) :=
 block([n:max(deg,s), xexpansion, yexpansion, res,
        allans, ans, eqns, deqn],

  if x0 # inf then
    xexpansion: x0 + t^r
  else
    xexpansion: (1/t)^r,

  yexpansion: y0 + sum(t^i*a[i],i,s,n),

  res: subst([x=xexpansion, y=yexpansion], f/t^p),

  res: expand(num(radcan(ratsimp(res)))),
  eqns: create_list(coeff(res,t,i),i,0,(n-s)),

  /* eqns[1] should be a polynomial in a[s] */
  /* We can only handle its simple roots, so compute */
  /* its derivative to check that */

  deqn: diff(eqns[1], a[s]),
  allans: sort(radcan(solve(eqns[1],a[s]))),

  /* Maxima's solve() can't handle arbitrary high powered */
  /* polynomials.  Check to see if it worked. */

  if apply("+", multiplicities) # hipow(eqns[1], a[s]) then
     error("NYI: solve didn't work in puiseux2"),

  /* If r > 1, then we expect each solution to be duplicated */
  /* r times, differing only by r'th roots of unity.  These */
  /* different solutions all correspond to the same branch, */
  /* so we'll remove all but one of each. */

  /* We exclude zero as a solution, since zero corresponds */
  /* to a higher degree solution than the ones we're seeking. */

  ans: [],
  for a in allans do
     if rhs(a) # 0
        and not member(ratsimp(a^r), ratsimp(ans^r)) then
           if ratsimp(subst(a[s]=a, deqn)) = 0 then
              error("NYI: multiple root in puiseux2")
           else
              ans: endcons(a, ans),

  /* For each solution to eqns[1], solve the remaining eqns. */
  /* IFT guarantees unique solutions to all eqns. */

  ans: map(makelist, ans),
  map(lambda([oneans], block([eqns2:subst(oneans, eqns)],
    for i:1 thru n-s do block([s:radcan(solve(eqns2[i+1],a[s+i]))],
      if not listp(s) or length(s) # 1 then
         error("puiseux2: internal error"),
      oneans: endcons(s[1], oneans),
      eqns2: subst(oneans, eqns2)),
    subst(oneans, yexpansion))), ans)
  )$
\end{maximablocksmall}

\begin{maximablocksmall}
/* puiseux_fracexp controls whether series are returned with
 * fractional exponents, or using an auxilary variable (t).
 */

puiseux_fracexp : false $

\end{maximablocksmall}

\begin{maximablocktiny}
puiseux3(f,x,y,x0,y0,deg,r,s,p,b) :=
 block([xsubst, tsubst, differential: 1, adjustment: 1, maxpow, maxypow, rf:0, yserieslist, result],

   if x0 # inf then (
      xsubst: x = x0 + t^r,
      tsubst: t^r = x - x0
   ) else (
      xsubst: x = 1 / t^r,
      tsubst: t^r = 1/x
   ),

   /* deg is the highest t-degree we want in our final result */

   maxpow: deg,

   if not freeof(del(x), b) then (
      b: ratsimp(b/del(x)),
      if not freeof(del(x), b) then
         error("puiseux: requested differential can't be normalized"),
      differential: del(t),
      if x0 # inf then (
         /* x= t^r  ->  dx = r t^(r-1) dt */
         /* so we don't need as many terms to get to maxpow */
         maxpow : maxpow - (r-1),
         adjustment : r * t^(r-1)
      ) else (
         /* x= t^-r  ->  dx = -r t^(-r-1) dt */
         /* so we need more terms to get to maxpow */
         maxpow : maxpow + (r+1),
         adjustment : -r * t^(-r-1)
      )
   ),

   /* Expand each of the coefficients of the requested */
   /* function as a Taylor or Laurent series in x, */
   /* leaving y alone.  We'll later substitute a Puiseux */
   /* series for y. */

   maxypow : 0,
   for i: 0 thru hipow(b, y) do block([icomponent],
      /* s is the lowest t-degree expected in the expansion of y */
      /* therefore, maxpow-s*i is the highest t-degree required in rf's expansion */
      /* ceiling((maxpow-s*i)/r) is the highest x-degree required */
      icomponent: ratsimp(subst(xsubst, taylor(ratsimp(coeff(b, y, i)),
                                            x, x0, ceiling((maxpow-s*i)/r)))
                          * adjustment),
      /* maxpow(rf) = minpow(icomponent) + maxpow(y) * i */
      if i # 0 then
         maxypow: ceiling((deg - lopow(icomponent,t))/i),
      rf: rf + icomponent * y^i
   ),

   /* expand y to the requested degree */
   yserieslist: puiseux2(f,x,y,x0,y0,maxypow,r,s,p),

   map(lambda([yseries],
      result: ratsimp(subst(y=yseries, rf)),

      if not atom(num(result)) and part(num(result), 0) = "+" then
         /* this map distributes over the common denominator */
         result: map(lambda([u], if hipow(u,t) - hipow(denom(result), t) <= deg then u/denom(result) else 0), num(result))
      else
         result: if hipow(num(result), t) - hipow(denom(result), t) <= deg then result else 0,

      if puiseux_fracexp then
         subst(t=(x-x0)^(1/r), result)
      else
         [result * differential, tsubst]
   ), yserieslist)
 )$
\end{maximablocktiny}

\begin{maximablocksmall}
puiseux4(f,x,y,x0,y0,deg,rf) :=
 block([result: [], mp, pairs, sign, delta, g, r, s, p],

   /* XXX eliminate segments with the same slope */

   if y0 # inf then (
      sign: -1
   ) else (
      /* If we're expanding y at infinity, don't recenter y, */
      /* but use upward sloping segments on the Newton polygon */

      y0 : 0,
      sign: 1
   ),

   /* Make a list of all line segments on the Newton polygon */

   mp: lower_convex_points(monomial_powers(f,x,y,x0,y0)),
   pairs: makelist([mp[i], mp[i+1]], i, length(mp)-1),

   for pair in pairs do (
     delta: pair[2]-pair[1],
     if delta[2] * sign > 0 then (
       g: gcd(delta[1],delta[2]),
       r: delta[1]/g,
       s: -delta[2]/g,
       p: pair[1][2]*r - pair[1][1]*s,

       result: append(result, puiseux3(f,x,y,x0,y0,deg,r,s,p,rf))
     )
   ),
   result
 )$

\end{maximablocksmall}

\begin{maximablocksmall}
puiseux(f,x,y,x0,y0,deg,[ratfunc]) :=
 block([result: [], rf],

   /* If the caller requested a specific rational function to */
   /* be expaned, use modulo to normalize it.  Default is y. */

   if length(ratfunc) > 0 then
      rf: modulo(ratfunc[1], f, y)
   else
      rf: y,

   /* If y0 is false, compute the possible solutions for y0. */

   if y0 = false then block([ys, f0],
      if x0 # inf then
         f0 : subst(x=x0, f)
      else
         f0 : subst(x=0, num(ratsimp(subst(x=1/x, f)))),
      ys : unique(radcan(solve(f0))),
      if apply("+", multiplicities) # hipow(f0, y) then
         error("NYI: solve didn't work in puiseux"),
      for yi in ys do
         result: append(result, puiseux4(f,x,y,x0,rhs(yi),deg,rf)),

      /* If substituting x0 into f reduced hipow(y), then */
      /* some of our solutions are at infinity. */

      if hipow(f,y) # hipow(f0,y) then
         result: append(result, puiseux4(f,x,y,x0,inf,deg,rf))
   ) else
      result: puiseux4(f,x,y,x0,y0,deg,rf),

   result
 )$
\end{maximablocksmall}

\begin{maximacode}

/* This is a small test suite to verify puiseux's correct operation. */

load("taylor1.mac")$

ev((
  test1: puiseux(y^2 - x^2 - 1, x, y, 0, 1, 5),
  test2: implicit_taylor(y^2 - x^2 - 1, 0, 5, 1),
  if is(test1[1] # test2) then error("puiseux: test failed"),

  test3: puiseux(b^2-a^2-1, a, b, 0, 1, 5),
  if is(test1 # subst(a=x, test3)) then error("puiseux: test failed"),

  test4: puiseux(y^2 + y - x^3, x, y, 0, 0, 10),
  test5: implicit_taylor(y^2 + y - x^3, 0, 10, 0),
  if test4[1] # test5 then error("puiseux: test failed"),

  test6: puiseux(y^2 - x, x, y, 0, 0, 10),
  if test6[1] # -sqrt(x) then error("puiseux: test failed")),
puiseux_fracexp: true)$

ev((
  test7: puiseux(y^2 + x^2 - 1, x, y, inf, inf, 12, x/y),
  test8: puiseux(y^2 + x^2 - 1, x, y, inf, inf, 10, x/y * del(x)),
  if not(test8[1][1] === - test7[1][1] * t^-2 * del(t)) then error("puiseux: test failed")),
puiseux_fracexp: false)$
\end{maximacode}

\begin{maximablock}
puiseux(y^2-x^2-1, x, y, %i, 0, 5);
\end{maximablock}

\example Find the Puiseux expansions of y at the multiple points of the
curve $y^2 = 1 - x^2$

We'll start with the finite zeros of $y$ at $(x,y)=(\pm 1, 0)$.  The
analysis is almost the same in both cases, so I'll just do (1,0).
First, construction of the Newton polygon requires recasting the
curve's polynomial into a form centered about the point being
analyzed, i.e, $y^2 + (x-1)^2 + 2(x-1) = 0$.  Next, we
plot the Newton polygon by putting the $y$ exponents on the horizontal
axis and the $x$ exponents on the vertical:

\begin{figure}[H]
\begin{center}
\maximac{newton_polygon(y^2+x^2-1, x, y, 1, 0)$}
\end{center}
\end{figure}

The only segment on the polygon lower convex hull has slope $-1/2$ and
width 2, telling us that two of our roots (the width of the segment)
will require a single Puiseux series with ramification index 2 (the
denominator of the slope):

$$x=t^2+1$$

We know that y can be expressed as a power series in $t$ with
initial exponent 1 (the numerator of the slope):

$$y= a_1 t + a_2 t^2 + a_3 t^3 + \cdots$$

Now, substituting these expressions for $x^2$ and $y^2$ into the
curve's defining equation $y^2 + x^2 - 1 = 0$ and setting all
coefficients of $t$ to zero, we find:

\begin{maximablock}
ratsimp(subst(
   [x=t^2+1, y=a[1]*t+a[2]*t^2+a[3]*t^3],
   y^2+x^2-1));
\end{maximablock}

$$2 + a_1^2 = 0 \qquad 2 a_1 a_2 = 0 \qquad 1 + 2 a_1 a_3 + a_2^2 = 0$$

The first equation tells us that $a_1 = \pm\sqrt{2}i$,
the second equation tells us that $a_2=0$ and the
third equation tells us that $a_3 = \pm \frac{\sqrt{2}}{4} i$, so

% $$a_0=0; \qquad a_1 = \pm\sqrt{2}i; \qquad a_2 = 0; \qquad a_3 = \pm \frac{\sqrt{2}}{4} i$$

$$x = t^2 +1; \qquad y = \pm\left[ \sqrt{2}it + \frac{\sqrt{2}}{4} it^3 + \cdots \right]$$

It would seem that we have two different series to chose from.  This
is not really the case, as they differ by only a $180^\circ$ rotation
in the t-plane, as can been seen by substituting $t=-t$, which
transforms one of the y-series into the other, while leaving the
x-series unchanged.  Let's confirm this result with Maxima:

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, 1, 0, 3);
\end{maximablock}

Now, let's analyze the singular point at infinity.  Again, we move
infinity to a finite point (0) with the substitutions $x=u^{-1}$ and
$y=v^{-1}$.  Our curve becomes:

$$(u^2 - 1) v^2 - u^2 = 0$$

\begin{figure}[H]
\begin{center}
\maximac{newton_polygon(y^2+x^2-1, x, y, inf, inf)$}
\end{center}
\end{figure}

The Newton polygon's lower convex hull has a single line segment,
slope $-1$, length $2$, telling us that we'll have two separate
cycles, each with ramification index 1.  Thus, $u$ can be used
directly as a uniformizing variable, and we postulate an expansion for
$v$ in the form:

$$v = a_1 u + a_2 u^2 + a_3 u^3 + \cdots$$

% $$v^2 = a_0^2 + 2 a_0 a_1 u + (2 a_0 a_2 + a_1^2) u^2 + (2 a_0 a_3 + 2 a_1 a_2) u^3 + (2 a_0 a_4 + 2 a_1 a_3 + a_2^2) u^4 + \cdots$$

Plugging this into $(u^2 - 1) v^2 - u^2$ and setting all the resulting
coefficients to zero, we conclude:

$$a_1 = \pm i; \qquad a_2 = 0; \qquad a_3 = \mp \frac{1}{2}i$$

$$v = \pm i u \mp \frac{1}{2} i u^3 + \cdots$$

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, inf, inf, 3, 1/y);
\end{maximablock}

This time, without ramification, we actually have two distinct series
that will yield two different values of $v$ for each value of $u$.
Inverting back to our original coordinates, we obtain:

$$y^{-1} = \pm \left[ i x^{-1} + \frac{1}{2} i x^{-3} + \cdots \right]$$

Yet this is not an expansion for $y$, nor is it a Puiseux series,
since it has an infinite number of negative exponents.  We can invert
the series (HOW?) to obtain our final result:

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, inf, inf, 3, y);
\end{maximablock}

\endexample

\example \cite{bliss} \S 68
Compute expansions at all multiple points of

$$y^3+x^3y+x=0$$

We begin by computing the discriminant of the
equation, which gives us the locations of the multiple points.

\begin{maximablock}
discriminant(y^3 + x^3*y + x, y);
\end{maximablock}

The multiple points lie over the roots of this equation: $x=0$ and
the seven roots of $4x^7+27=0$.  Infinity also needs to be
examined.  We begin with $x=0$:

\begin{maximablock}
puiseux(y^3 + x^3*y +x, x, y, 0, false, 10);
\end{maximablock}

This result shows that we have a single cycle at $(x,y)=(0,0)$ with
three sheets.  Now, let's look at a specimen root
of $4x^7+27=0$:

\begin{comment}
puiseux(y^3 + x^3*y +x, x, y, g, -3/(2*g^2), 1);
puiseux(y^3 + x^3*y +x, x, y, g, 3/g^2, 1);
puiseux(y^3 + x^3*y +x, x, y, g, -(3/8)^(1/7), 1);
\end{comment}

\begin{maximablock}
g: (-27/4)^(1/7);
puiseux(y^3 + x^3*y +x, x, y, g, false, 1);
\end{maximablock}

We have one sheet of two cycles at $(g,-3/(2g^2))$
and an ordinary point at $(g,3/g^2)$.

Finally, let's look at what happens when $x$ goes to infinity:

\begin{maximablock}
puiseux(y^3 + x^3*y +x, x, y, inf, false, 10);
\end{maximablock}

Here we have an ordinary point at $(\infty,0)$ and
a single cycle of two sheets at $(\infty,\infty)$.

\endexample

\example \cite{bliss} \S 68
Compute expansions at all multiple points of

$$y^3-3y+2x=0$$

\begin{maximablock}
discriminant(y^3 - 3*y + 2*x, y);
puiseux(y^3 - 3*y + 2*x, x, y, 1, false, 2);
puiseux(y^3 - 3*y + 2*x, x, y, inf, false, 2);
\end{maximablock}

$$y^3+3y-x$$

\begin{maximablock}
discriminant(y^3 + 3*y - x, y);
puiseux(y^3 + 3*y - x, x, y, 2*%i, false, 10);
\end{maximablock}

\endexample

%\mysection{meromorphic functions are analytic}
%
%first, trace of a meromorphic function is meromorphic on C(x), and is
%thus a rational function
%
%Liouville's theorem: a bounded entire function is constant
%
%Proof A: (Silverman) use a Taylor series expansion around z=0, which
%is valid in the entire plane (since the function is entire).  Cauchy's
%inequality $|f| \le M ==> |c_n| \le M/{R^n}$ (eq. 10.8') as R->infty
%implies that the function is constant.
%
%Lemma: A entire function with no singularities, even at infinity, is
%constant.
%
%Proof: We can do a Taylor series expansion at the origin, whose
%non-zero terms will correspond to the principle part of the expansion
%at infinity, which must therefore be zero.
%
%Next: A entire function with only a pole at infinity is a polynomial.
%The principle part at infinity will be a polynomial.  Subtract it out
%to get a function with no singularities, which must be constant.
%
%Next: Given a function with only a finite number of finite poles,
%multiply it by a polynomial (the denominator) matching the poles with
%zeros.  Now we've got a function with only a pole at infinity, which
%must be a polynomial (the numerator).

\example Find the principal parts of $\frac{1}{y}$ on the curve
$y^2 = 1 - x^2$

The {\it principal part} of an algebraic function is the part
of its series expansion with negative exponents.  Theorem ?
states that an algebraic function is completely determined,
up to adding a constant, by its principal parts.

The first step is to locate the function's poles, which in this case is
simply the places where the denominator is zero, and that's just
$x=\pm 1$.  Now, if we use {\tt puiseux}, we can just request a series
truncated at the $-1$ term:

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, 1, 0, -1, 1/y);
\end{maximablock}

\endexample

%\example Find the principal parts of $\frac{1}{y} dx$ on the curve
%$y^2 = 1 - x^2$
%
%Differential forms are different from functions, and have different
%series expansions.
%
%$\frac{1}{y} dx = \left[ -\sqrt{2}i + \cdots \right] $
%
%\endexample

\example Find the principal parts of $\frac{x}{y} \ud x$ on the curve
$y^2 = 1 - x^2$

Differential forms are not functions, and have different series
expansions, due to the presence of the
differential, which must be adjusted at ramification points.

Let's expand $\frac{x}{y}$ at $x=1$:

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, 1, 0, 5, x/y);
\end{maximablock}

Now $x=t^2+1$, so $\ud x=2t\ud t$.  Thus, multiplying $\frac{x}{y}$
by $\ud x$ and changing our variable to $t$ will multiply
all of the terms in our expansion by $2t$:

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, 1, 0, 6, x/y*del(x));
\end{maximablock}

Even though both $\frac{1}{y}$ and $\frac{x}{y}$ have poles
at $x=1$, $\frac{x}{y} \ud x$ does not!

Its behavior at infinity also requires analysis.

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, inf, inf, -1, x/y * del(x));
\end{maximablock}

In short, while $\frac{x}{y}$ has poles at $(\pm 1,0)$,
$\frac{x}{y} \ud x$ has poles only at infinity.

\endexample

\example Evaluate $\int \frac{x}{\sqrt{x^2+1}}\,dx$

This is a simple integral that can be easily solved using first year
calculus techniques, but let's see how to attack it using the more
sophisticated techniques of this chapter.

First, we normalize the integrand (CLARIFY) and convert it into
a rational function on an algebraic curve:

$$\int \frac{x}{x^2+1}y\,dx\qquad y^2=x^2+1$$

Next, we identify the poles of the integrand.  They are located
where $x^2+1=0$, over the points $x=\pm i$.  These are also
ramification points of the algebraic curve.  Its Riemann surface
looks something like this:

\begin{figure}[H]
\begin{center}
\begin{maximacode}
pdfplot3d ([sqrt((x**2-y**2+1)**2+(2*x*y)**2),
   -sqrt((x**2-y**2+1)**2+(2*x*y)**2), [x, -2, 2], [y, -2, 2]],
   [legend, false], [xlabel, "Re"], [ylabel, "Im"], [zlabel, false],
   [xtics, -1,1,1], [ytics, -1,1,1], [ztics, false],
   [azimuth, 70], [elevation, 80], [grid, 100, 100], [mesh_lines_color, false])$
\end{maximacode}
\end{center}
\end{figure}

Let's compute a Puiseux series expansion of $\frac{x}{x^2+1}y\,dx$
at $(x,y)=(i,0)$.

% We'll use the uniforming variable $t$, with
% $x=t^2+i$.  $y^2 = x^2+1 = (t^2+i)^2+1 = t^4+2it^2$

\begin{maximablock}
puiseux(y^2 - x^2 - 1, x, y, %i, 0, 3,
        y*x/(x^2+1));
ev(puiseux(y^2 - x^2 - 1, x, y, %i, 0, 3,
           y*x/(x^2+1)),
   puiseux_fracexp=true);
\end{maximablock}

There appears to be a pole here, but appearances are deceptive.
We can't integrate this series termwise with respect to $x$;
we have to change variables in the differential:

$$d \sqrt{x-i} = \frac{1}{2} \frac{1}{\sqrt{x-i}} dx$$
$$dx = 2 \sqrt{x-i} d \sqrt{x-i}$$

\begin{maximablock}
distrib(2 * %[1] * sqrt(x-%i));
\end{maximablock}

So, even though the {\it integrand} has a pole at $(i,0)$, the {\it
differential} does not... and the differential is what matters!

\begin{maximablock}
puiseux(y^2 - x^2 - 1, x, y, %i, 0, 4,
        y*x/(x^2+1) * del(x));
\end{maximablock}

The only other place we might have a pole is infinity.

\begin{maximablock}
puiseux(y^2 - x^2 - 1, x, y, inf, inf, 3, 1/y);
puiseux(y^2 - x^2 - 1, x, y, inf, inf, 3, x/y);
\end{maximablock}

Now it appears that we have two sheets with no poles, the expansions indicating simply
that $\lim_{x\to\infty}\frac{x}{\sqrt{x^2+1}} = \pm 1$, depending
on whether we use the positive or negative square root,
but again
we have to take the differential into account.  Since $x=\frac{1}{t}$,
$dx=-\frac{1}{t^2} dt$, and we actually have second order poles
at infinity.

\begin{maximablock}
puiseux(y^2 - x^2 - 1, x, y, inf, inf, 0,
        x/y * del(x));
\end{maximablock}

Integrating termwise, we see that since our differential has second
order poles at infinity, our integral must have first order poles
at infinity, and theorem ? states that this completely
characterizes the integral.

We already know of a rational function with first order poles
at infinity -- $y$ itself!

\endexample

% A {\it Riemann surface}, more precisely, is a two-dimensional manifold
% with a {\it complex analytic structure}.

\vfill\eject
\mysection{Examples}

\example Compute $\int \sqrt{4-x^2} \,dx$

A solution method from first year calculus might be to note that
this integrand forms one leg of a right triangle:

\begin{center}
\setlength{\unitlength}{1cm}
\begin{picture}(6,5)
\put(5,1){\line(0,1){3}}
\put(5,1){\line(-1,0){4}}
\put(1,1){\line(4,3){4}}
\put(2.5,0.5){$\sqrt{4-x^2}$}
\put(3,2.8){2}
\put(5.2,2.5){$x$}
\put(1.7,1.15){$\theta$}
\end{picture}
\end{center}

$$x=2\sin\theta \qquad \sqrt{4-x^2}=2\cos\theta \qquad dx=2\cos\theta\,d\theta$$


\begin{eqnarray*}
\int \sqrt{4-x^2} \, dx & = & \int 4 \cos^2\theta \, d\theta \\
& = & \int \left( 2 + 2\cos 2\theta \right) \, d\theta \\
& = & 2\theta + \sin 2\theta \\
& = & 2\theta + 2\sin\theta\cos\theta \\
& = & 2\arcsin\frac{x}{2} + \frac{x \sqrt{4-x^2}}{2} \\
\end{eqnarray*}

Now let's attack this integral using the methods of this chapter.
First, transform the problem into an algebraic curve:

$$\int y\,dx \qquad y^2 = 4-x^2$$

Since $\lim_{x\to\infty} y = \infty$, the integrand has poles at
infinity.  We want infinity to be an ordinary point of the curve (no
ramification; no singularities) with no poles in the integrand.  The
simplest transformation is to exchange zero with infinity, and in this
case zero is an ordinary point with places $(0,2)$ and $(0,-2)$,
neither of which is a pole of the integrand.  So we'll invert
$x$ and $y$ into $u$ and $v$:

$$x=\frac{1}{u} \qquad y=\frac{1}{v}$$
$$\left(\frac{1}{v}\right)^2 = 4 - \left(\frac{1}{u}\right)^2 \Longrightarrow 4u^2v^2 - v^2 - u^2=0$$
$$\int\frac{1}{v} \, d\left(\frac{1}{u}\right) \Longrightarrow -\int\frac{1}{vu^2}\,du$$

The only poles in this integrand occur when either $u=0$ or $v=0$.
Substituting these values into $4u^2v^2 - v^2 -u^2=0$, we see that
these condiutions only occur at $(u,v)=(0,0)$, so let's analyze our
curve at that point, starting with the Newton polygon:

\begin{center}
$4 u^2 v^2 - v^2 - u^2 = 0$ \\
\setlength{\unitlength}{1cm}
\begin{picture}(3,3)
\put(0,0){\line(0,1){2.5}}
\put(0,0){\line(1,0){3}}
\put(1.9,-0.1){x}
\put(1.9,1.9){x}
\put(-0.1,1.9){x}
\thicklines
\put(0,2){\line(1,-1){2}}
\end{picture}
\end{center}

The Newton polygon has a single line segment of span 2 and slope -1, so
we have two cycles, each with ramification index one: a singularity.
Since there is no ramification, $u$ is a uniformizing parameter
and we expect to expand $v$ as follows:

$$v = c_1 u + c_2 u^2 + c_3 u^3 + \cdots$$
$$v^2 = c_1^2 u^2 + 2 c_1 c_2 u^3 + (2 c_1 c_3 + c_2^2) u^4 + \cdots$$

Substituting these expansions into $4u^2v^2 - v^2 - u^2 = 0$, we obtain:

$$ 4 c_1^2 u^4 + 8 c_1 c_2 u^5 + (8 c_1 c_3 + 4 c_2^2) u^6 + \cdots $$
$$ - c_1^2 u^2 - 2 c_1 c_2 u^3 - (2 c_1 c_3 + c_2^2) u^4 + \cdots - u^2 = 0$$

Equating terms in $u^2$, we see that $c_1 = \pm i$.  Each of these
two values corresponds to one branch of the singularity.  There
is only a single term in $u^3$, which forces $c_2$ to be zero,
and equating terms in $u^4$ produces $c_3 = 2 c_1$, so

$$v = \pm (iu + 2iu^3 + \cdots) \qquad @(0,0)$$

Inverting $v$ and substituting into our 1-form, we obtain

$$\frac{1}{v} = \pm (-i \frac{1}{u} + 2i u + \cdots) \qquad @(0,0)$$

$$\frac{1}{vu^2}\, du = \pm \left[ -i \frac{1}{u^3} + 2i \frac{1}{u} + \cdots \right] \, du \qquad @(0,0)$$

The $u^{-1}$ terms will integrate into logarithms, so let's ignore
them for the moment and concentrate on the $u^{-3}$ terms, which will
integrate into $u^{-2}$ terms, so we're looking for a function with
second order poles at both places at the $(0,0)$ singularity.

Starting with our standard basis for all rational functions,
$\{1,\,v\}$, we seek to modify it into a basis for
${\rm P}^2(0,0)_a{\rm P}^2(0,0)_b$.  Note first that $v$ has
poles at $u=\pm\frac{1}{2}$.  Using $y=1/u$, we analyze
at $(\pm\frac{1}{2}, \infty)$ as follows:

\begin{center}
$y^2\left[(u-\frac12)^2+(u-\frac12)+\frac14\right]-4(u-\frac12)^2-4(u-\frac12)$
\\
\setlength{\unitlength}{1cm}
\begin{picture}(3,3)
\put(0,0){\line(0,1){2.5}}
\put(0,0){\line(1,0){3}}
\put(0.9,-0.1){x}
\put(1.9,-0.1){x}
\put(-0.1,1.9){x}
\put(0.9,1.9){x}
\put(1.9,1.9){x}
\thicklines
\put(0,2){\line(1,-2){1}}
\end{picture}
\end{center}

Our line segment has span 1 and slope -2, indicating a single place
with ramification 2, and $y$ as a uniformizing parameter.  Setting

$$(u-\frac12) = c_1 y + c_2 y^2 + \cdots$$
$$(u-\frac12)^2 = c_1^2 y^2 + \cdots$$

Substituting, we find that $c_1 = 0$ and $c_2 = \frac{1}{16}$, so

$$(u-\frac12) = \frac{1}{16} y^2 + \cdots \qquad v=y^{-1} \qquad @(\frac12, \infty)$$

$$(u+\frac12) = \frac{1}{16} y^2 + \cdots \qquad v=y^{-1} \qquad @(-\frac12, \infty)$$

In short, $v$ has first order poles at $(\pm\frac12,\infty)$ and
$(u\pm\frac12)$ has second order zeros, so we can adjust our basis
accordingly and obtain $\{1,\,(4u^2-1)v\}$ for a basis with no finite
poles.  We can also use a theorem of Trager to shortcut this calculation.

Returning to our analysis at $(0,0)$, we see that 1 has zero order
(obviously) and $(4u^2-1)v$ has a first order zero at both sheets
there, since $4u^2-1=-1$ is finite and $v$ has first order zeros.
We also know that $u$ is a uniformizing parameter, so it's easy
to modify our basis and obtain

$$\left\{\frac{1}{u^2},\,\frac{4u^2-1}{u^3}v\right\} {\rm is\, a\,} {\bf C}[x]{\rm -basis\, for\, P^2(0,0)_aP^2(0,0)_b}$$

Is this basis normal at infinity?  Well, the representation order of
$\frac{1}{u^2}$ is 2 and its $u^-2$ coefficients at $(\infty, \pm
\frac12)$ are both 1, while the representation order of $\frac{4u^2-1}{u^3}v$
is 1, and its $u^-1$ coefficients are 2 and -2.  Since

$$\det C = \begin{array}{|cc|} 1 & 2 \\ 1 & -2 \end{array} = -4$$

is non-zero, the basis is normal at infinity.

The Riemann-Roch theorem says that the dimension of ${\mathfrak l}(D)$ is 5,
$\frac{1}{u^2}$ can be multiplied by any polynomial up to second
degree without introducing poles at infinity, and $\frac{4u^2-1}{u^3}v$
can be multiplied by any polynomial up to first degree, so

$$\left\{\frac{1}{u^2},\, \frac{1}{u},\, 1,\, \frac{4u^2-1}{u^3}v,\, \frac{4u^2-1}{u^2}v\right\}$$

is a ${\cal C}$-module basis for ${\mathfrak l}(D)$.

Any linear combination of these functions is a multiple of the
divisor, but not all of them produce the correct residues.  Looking at
the residues, we see that only $\frac{4u^2-1}{u^3}v = \frac{1}{uv}$
has residues of $\pm i$ on the two sheets at the $(0,0)$ singularity.
Dividing by 2 to correct for the 2 that will be introduced by the
integration, we conclude that $\frac{1}{2uv} = \frac{xy}{2} =
\frac{x\sqrt{4-x^2}}{2}$ is the desired function.

Next, we have to deal with the logarithms.  Going back to the
series expansions of our 1-form, we see that we have residues
of $\pm 2i$ on our two sheets at $(0,0)$.  The objective
now is a bit different; we want a function with exactly
the divisor $Z(0,0)_a P(0,0)_b$.  Starting with an integral basis:

$$\{1, (4u^2-1)v\}$$

we want to modify these functions to make them multiples
of $Z(0,0)_a P(0,0)_b$.  The pole isn't a problem for
an integral basis, and looking at the series expansion
for $v$ at $(0,0)$ we see that it (and therefore $(4u^2-1)v$)
has a simple zero there, but $1$ needs to be replaced with $u$:

$$\{u, (4u^2-1)v\}$$

Now we construct a matrix with the coefficients in the series expansions:

$$\left[ \begin{array}{cc} 1 & -i \\ 0 & 0 \end{array} \right] \begin{array}{ll} \leftarrow (0,0)_a \\ \leftarrow (0,0)_b \end{array} $$

$$\left[ \begin{array}{cc} 1 & -i \\ 0 & 0 \end{array} \right] \left[ \begin{array}{c} i \\ 1 \end{array} \right] = 0$$

The solution shows us how to modify the basis:

$$\{u, \frac{iu + (4u^2-1)v}{u}\} = \{u, i + \frac{(4u^2-1)v}{u}\}$$

$$\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right] \begin{array}{ll} \leftarrow (0,0)_a \\ \leftarrow (0,0)_b \end{array} $$

$$\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right] \left[ \begin{array}{c} 0 \\ 1 \end{array} \right] = 0$$

$$\{u, i\frac{1}{u} + \frac{(4u^2-1)v}{u^2}\}$$

$$\left| \begin{array}{cc} 1 & -2i \\ 0 & 2i \end{array} \right| = 2i$$

At the last step, the determinant is non-zero, which shows that we
now have a basis for multiples of the divisor except at infinity.
Is it normal at infinity?  $u$'s expansion at both places at infinity
is $\left(\frac{1}{u}\right)^{-1}$, so its representation order is -1,
and the second element's expansion at infinity starts $\pm 2 + \cdots$,
so its representation order is 0 and:

$$\det C = \begin{array}{|cc|} 1 & 2 \\ 1 & -2 \end{array} = -4$$

So the basis is normal at infinity.  If an exact multiple of
the divisor exists, it is one of the basis elements.  It's not $u$,
since $u$ has a pole at infinity, but the second element is exact:

$$i\frac{1}{u} + \frac{(4u^2-1)v}{u^2} = i\frac{1}{u} - \frac{1}{v} = ix-y$$

The desired residues are $\pm 2i$, so the function we want is

$$2i \ln(ix-y) = 2i \ln(\frac{y}{2}-i\frac{x}{2}) + 2i \ln(-2) $$
$$= 2i \ln\left(\sqrt{1-\left(\frac{x}{2}\right)^2} - i\frac{x}{2}\right) = 2i (-i \arcsin \frac{x}{2}) = 2 \arcsin \frac{x}{2}$$

(the constant disappears into the constant of integration) and the final answer is:

$$ \int \sqrt{4-x^2} \, dx  = 2\arcsin\frac{x}{2} + \frac{x \sqrt{4-x^2}}{2}$$

\endexample


\vfill\eject
\mysection{arcsin}

\example Compute $\int {1\over{\sqrt{1-x^2}}} \,dx$

The obvious attempt is to use the algebraic extension $y^2=1-x^2$ and
integrate ${1\over y}\,dx$.

But we first need to determine if this differential has any poles at
infinity, by inverting the field and looking for poles at zero.
Setting $u={1\over x}$, we convert our minimal polynomial into
$u^2y^2=u^2-1$ (after multiplying through by $u^2$), and using
$v=uy$ we obtain our inverse field ${\bf C}(u,v); v^2=u^2-1$.

Since $x={1\over u}$ and $y={v\over u}$, we convert our differential as follows:

 $${1\over y}\,dx ={u\over v} (-{1\over{u^2}} \, du) = -{1\over{uv}} \, du$$

Now, $\{1, v\}$ is an integral basis for the inverse field, so we
multiply through by $v\over v$ to obtain:

 $$= -{v\over{uv^2}} du = -{1\over{u(u^2-1)}}v \, du $$

which is now in normal form and clearly has a pole at $u=0$, or $x=\infty$.  Note that

 $${1\over y} = {u\over v} = {{uv}\over{v^2}}
 = {u\over{u^2-1}} v$$

has no pole at $u=0$, a clear example of a differential having a pole
at a place where its constituent function has none.

In any event, we clearly can not use the original field to conduct the
integration, since it would require constructing a function with a
pole at infinity, and our algorithm can't handle this.  So we need to
transform into a field where the differential has no pole at infinity.

Actually, we've already done this!  Note that the integrand had no pole
at zero in the original field:

 $${1\over y}\,dx = {y\over y^2}\,dx = {1\over{1-x^2}}y \,dx $$

Since the inverse field swapped zero with infinity, it follows that
there is no pole at infinity in the inverse field, so we can proceed
to integrate $-{1\over{u(u^2-1)}}v \,du$ in ${\bf C}(u,v)$;
$v^2=u^2-1$.

Simple inspection of the integrand (already in normal form) shows that
its poles are at $(0, i)$, $(0, -i)$, $(1, 0)$, and $(-1, 0)$.
Remember that we're now working on the Riemann surface of an algebraic
extension, so we need to specify $\it both$ $u$ and $v$ to
specify a place.

The next step is to compute the residues at each of these places,
using Theorem \ref{Trager's residue theorem}:

\begin{center}
\begin{supertabular}{l l l}
  $(0, i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, i)$     & = $i$    \cr
  $(0, -i)$  &  $\displaystyle -{1\over{(u^2-1)}}v$ @ $(0, -i)$   & = $-i$    \cr
  $(1, 0)$  &  $\displaystyle -2{1\over{u(u+1)}}v$ @ $(1, 0)$      & = $0$    \cr
  $(-1, 0)$  &  $\displaystyle -2{1\over{u(u-1)}}v$ @ $(-1, 0)$    & = $0$    \cr
\end{supertabular}
\end{center}

The poles with zero residues can be ignored.  We're interested in the
other two, which exist in ${\bf Q}[i]$, which can be regarded as a
vector field over ${\bf Q}$ with basis $\{1, i\}$, and we want to
construct a function whose poles and zeros match the $i$-component of
the residues (the 1-component is uniformly zero).

We start by constructing an ${\cal I}$-module generator set for the divisor
with a simple zero at $(0,i)$ and a simple pole at $(0,-i)$.  Theorem
\ref{simple pole construction} shows that:

$$f = {{v^2+1}\over{u(v+i)}} = {{v-i}\over{u}} $$

has a simple pole at $(0,-i)$.  At $(0,i)$, L'H\^opital's rule gives:

$$ \lim_{(u,v)\to (0,i)} {{v-i}\over{u}}
   = {{(v-i)'}\over{u'}} {{dv}\over{du}} = {{dv}\over{du}} = {u\over v} = 0 $$

where the last transformation was accomplished by differentiating the
mimimal polynomial.  So $f$ has a zero at $(0,i)$, and I'll note that
we've just stumbled into the solution.  Theorem \ref{simple pole
construction} already assures us that $f$ has only a single finite
simple pole, and we can see that its only zeros occur when
$v-i=0$, which, according to the minimum polynomial, can only
occur at $u=0$, thus $(0,i)$ is its only finite zero, and it is
simple, as we can verify by showing that the corresponding pole in its
inverse is simple:

$$ {1\over f} = {u\over{v-i}} = {{u(v+i)}\over{v^2+1}}
  = {{u(v+i)}\over{u^2}} = {1\over u}v + {i\over u} $$


So we've found the function we're looking for by accident.  Let's save the
general case for the next example, and convert back to
our original field:

$${{v-i}\over{u}} = x({y\over x}-i) = y - ix $$

Remembering that our residues came multiplied by a factor of $i$, we
conclude that our solution is $i\,\ln(y-ix)$, or:

\begin{eqnarray*}
\int {1\over{\sqrt{1-x^2}}} \,dx &=& i\,\ln(\sqrt{1-x^2}-ix) \\
                                 &=& -i\,\ln({1\over{\sqrt{1-x^2}-ix}}) \\
                                 &=& -i\,\ln({{\sqrt{1-x^2}+ix}\over{1-x^2+x^2}}) \\
                                 &=& -i\,\ln({\sqrt{1-x^2}+ix}) \\
                                 &=& \arcsin x \\
\end{eqnarray*}

where I used the negative of a logarithm being the logarithm of the
inverse, and the last transformation came from section
\ref{sec:Root/Logarithm/Exponential Form}.


\endexample

\vfill\eject
\mysection{Geddes's example}

\example Compute $\int {1\over{x\sqrt{x^4+1}}} \, dx$

We'll use ${\bf C}(x,y); y^2=x^4+1$ and integrate ${1\over{xy}} =
{y\over{x^5+x}}$.  Inverting this field ($z={1\over x}$) shows that
this integrand has no poles at infinity, so we can proceed directly:

$$ {y\over{x^5+x}} = {y\over{x(x+\omega)(x-\omega)(x+i\omega)(x-i\omega)}} \qquad \omega = \sqrt{i} = {\sqrt{2}\over2} + {\sqrt{2}\over2} i $$

\bigskip
\begin{center}
\begin{supertabular}{l l l}
  $(0, 1)$  &  $\displaystyle {y\over{x^4+1}}$ @ $(0, 1)$     & = $1$    \cr
  $(0, -1)$  &  $\displaystyle {y\over{x^4+1}}$ @ $(0, -1)$     & = $-1$    \cr
  $(\omega, 0)$  &  $\displaystyle 2{y\over{x(x^2+i)(x+\omega)}}$ @ $(\omega, 1)$     & = $0$    \cr
  \multicolumn{2}{l}{$(-\omega, 0), (i\omega, 0), (-i\omega, 0)\qquad\cdots$}    & = $0$    \cr
\end{supertabular}
\end{center}

We now use theorem \ref{simple pole construction} to construct a
function with a simple pole at $(0,-1)$:

$${{f(0,y)}\over{x(y+1)}} = {{y^2-1}\over{x(y+1)}} = {{y-1}\over{x}} $$

This function has a zero at $(0,1)$, but, unfortunately, it is third order,
as can be seen from either L'H\^opital's rule:

$$y^2=x^4+1$$
$$2y\,dy=4x^3\,dx$$
$${{dy}\over{dx}} = 2{x^3\over y}$$

$${{y-1}\over{x}} @ (0,1) = \lim {{dy}\over{dx}} = 2 {{x^3}\over y} = 0$$
$${{y-1}\over{x^2}} @ (0,1) = \lim {1\over{2x}}{{dy}\over{dx}} = {{x^2}\over y} = 0$$
$${{y-1}\over{x^3}} @ (0,1) = \lim {1\over{3x^2}}{{dy}\over{dx}} = {2\over3}{{x}\over y} = 0$$
$${{y-1}\over{x^4}} @ (0,1) = \lim {1\over{4x^3}}{{dy}\over{dx}} = {1\over2}{1\over y} = {1\over2}$$

\vfil\eject

\ldots or from a series expansion of $y$ at (0,1):

$$y^2 = x^4 + 1 $$
$$(y-1)^2 = x^4 - 2(y-1)$$
$$(y-1) = {1\over2}x^4 - {1\over2}(y-1)^2$$
$$(y-1) = c_0 + c_1 x + c_2 x^2 + c_3 x^3 + \cdots$$
$$(y-1)^2 = c_0^2 + (2 c_0 c_1) x + (2 c_0 c_2 + c_1^2) x^2 + (2 c_0 c_3 + 2 c_1 c_2) x^3 + \cdots$$

$$ c_0, c_1, c_2, c_3 = 0 $$
$$ c_4 = {1\over2}$$
$$ c_5, c_6, c_7 = 0 $$
$$ c_8 = -{1\over8} $$

$$ (y-1) = {1\over2} x^4 - {1\over8} x^8 + \cdots$$

$$ {(y-1)\over x} = {1\over2} x^3 - {1\over8} x^7 + \cdots$$

\ldots or from the norm:

$$N({{y-1}\over{x}}) = {{y-1}\over{x}} \cdot {{-y-1}\over{x}} = - {{y^2-1}\over{x^2}} = - {{x^4}\over{x^2}} = - x^2$$

Since we know that the function has a simple pole at $(0,-1)$, so it
must have a third order zero at $(0,1)$ to form a norm with a second
order zero.

We can eliminate the inconvenient zero by adding a constant to the
function, say 1: ${{x+y-1}\over x}$.  We can now use theorem
\ref{simple zero construction} to create a simple zero at
(0,1) by multiplying by $x+y-1$:

$${{x+y-1}\over x} (x+y-1) = {{2x-2}\over x} y + {{x^4+x^2-2x+2}\over x} $$

This function has a simple pole at (0,-1) and a simple zero at (0,1),
but does it have other poles and zeros?  If so, can it be modified to
eliminate them?  To find out, we form the generators of an ${\cal I}$-module:

$$\{ {{2x-2}\over x} y + {{x^4+x^2-2x+2}\over x}, x \}$$

Noting that ${{x^4+x^2}\over{x}} = x(x^2+1)$ and $x^2+1 \in {\cal I}$,
we can simplify this:

$$\{ {{2x-2}\over x} y - {{2x-2}\over x}, x \}$$

\vfill\eject

Using the integral basis $\{1, y \}$, we convert this to a
${\bf C}[x]$-module:

$$\{ {{2x-2}\over x} y - {{2x-2}\over x}, {{2x-2}\over x} (x^4+1) - {{2x-2}\over x} y, x, xy \}$$

and since $(2x-2){{x^4}\over{x}} = x(2x^3-2x^2)$ and
$2x^3-2x^2 \in {\bf C}[x]$, we simplify:

$$\{ {{2x-2}\over x} y - {{2x-2}\over x}, {{2x-2}\over x} - {{2x-2}\over x} y, x, xy \}$$

and write it in matrix form:

% $$\pmatrix{-{{2x-2}\over{x}} & {{2x-2}\over{x}} \cr {{2x-2}\over{x}} & -{{2x-2}\over{x}} \cr x & 0 \cr 0 & x} \pmatrix{1 \cr y}$$

$${1\over x}\begin{pmatrix}-(2x-2) & 2x-2 \cr 2x-2 & -(2x-2) \cr x^2 & 0 \cr 0 & x^2\end{pmatrix} \begin{pmatrix}1 \cr y\end{pmatrix}$$

Elementary row operations\footnote{Read right to left; $R_{i,j,\lambda}$ adds $\lambda$ times row $j$ to row $i$; $R_{i,\alpha}$ multiplies row $i$ by $\alpha$ (a unit)} $R_{4,3,x^2} R_{1,3,(2x-2)} R_{3,4,-1} R_{3,1,{1\over2}(x+1)} R_{2,1,1} $ yield:

$${1\over x}\begin{pmatrix}1 & -1 \cr x^2 & 0\end{pmatrix} \begin{pmatrix}1 \cr y\end{pmatrix} = \begin{pmatrix}{{1-y}\over{x}} \cr x\end{pmatrix} $$

so $\{ {{1-y}\over{x}}, x \} $ forms a generator set for
the ${\bf C}[x]$-module of the finite multiples of $Z(0,1)P(0,-1)$.
We convert to a basis normal at infinity: $\{1, v\} = \{1, {y\over x^2}\}$:

$$\begin{pmatrix}{{1-y}\over{x}} \cr x\end{pmatrix} = \begin{pmatrix}{1\over x} & -x \cr x & 0\end{pmatrix} \begin{pmatrix}1 \cr {y\over x^2}\end{pmatrix}
= \begin{pmatrix}x & \cr & x\end{pmatrix} \begin{pmatrix}{1\over x^2} & -1 \cr 1 & 0\end{pmatrix} \begin{pmatrix}1 \cr {y\over x^2}\end{pmatrix}$$

$$\det_{@ \infty} \begin{pmatrix}{1\over x^2} & -1 \cr 1 & 0\end{pmatrix} = 1$$

so $\{ {{1-y}\over{x}}, x \} $ is normal at infinity.  $x$ clearly has
a pole at infinity, so it can't be the function we're looking for, but
what about ${{1-y}\over{x}}$?  Switching back to $\{u,v\}$
coordinates, we obtain ${{1-y}\over{x}} = {{u^2-v}\over u}$, which has
$\{u,v\}$ poles at both $\{0,1\}$ and $\{0,-1\}$, which translate into
poles at $x=\infty$.  Therefore, no rational function on this
algebraic curve has a simple pole at (0, -1), a simple zero at (0,1),
and no other poles or zeros.

\vfill\eject

So, let's try a double pole at (0,-1) and a double zero at (0,1).  We
can just square our previous generators:
$\{ {{1-y}\over{x}}, x \} $
to obtain: 
$\{ {(1-y)^2\over{x^2}}, x^2 \} = \{ {{1-2y+x^4+1}\over{x^2}}, x^2 \}$
which simplifies to
$\{ {{1-y}\over{x^2}}, x^2 \}$.  We again check for normalcy
at infinity:

$$\begin{pmatrix}
{1\over{x^2}} & -1 \cr
x^2 & 0\end{pmatrix} \begin{pmatrix}1 \cr {y\over{x^2}}\end{pmatrix} = \begin{pmatrix}1 & \cr & x^2\end{pmatrix}\begin{pmatrix}
{1\over{x^2}} & -1 \cr
1 & 0\end{pmatrix}
\begin{pmatrix}1 \cr {y\over{x^2}}\end{pmatrix} $$

$$\det_{@\infty}\begin{pmatrix}
{1\over{x^2}} & -1 \cr
1 & 0\end{pmatrix} = 1$$

So, $\{{{1-y}\over{x^2}}, x^2\}$ is a ${\bf C}[x]$-module, normal at
infinity, containing the finite multiples of $Z^2(0,1)P^2(0,-1)$.
$x^2$ has a pole at infinity, but does ${1-y}\over{x^2}$?
Switching to $x={1\over z}; y={u\over{z^2}}; u^2 = z^4 + 1$ and
Writing it as $z^2(1-{u\over{z^2}}) = z^2 - u$ shows that it has no
zero at $(z,u) = (0, \pm 1)$, and thus no pole at $x = \infty$.  It
is, therefore, the function we are looking for:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx
   = {1\over2} \ln{{1-\sqrt{x^4+1}}\over{x^2}}$$

I'll now point out to you what's been pointed out to me, and that is a
traditional solution technique for this integral:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx$$
$$x^2 = u \qquad 2 x\, dx = du$$
$$\int {1\over{2u\sqrt{u^2+1}}} \, du$$
$$u = \tan z \qquad du = \sec^2 z dz$$
$${1 \over 2} \int \csc z \, dz = {1\over 2} \ln \tan {z\over2}$$
$$={1\over2} \ln {{\sec z - 1}\over{\tan z}}$$
$$={1\over2} \ln {{\sqrt{u^2+1}-1}\over{u}}$$
$$={1\over2} \ln {{\sqrt{x^4+1}-1}\over{x^2}}$$

\vskip 0.5in

$${{1 - \cos z}\over{\sin z}} = {{1 - \cos^2 {z\over2} + \sin^2 {z\over2}}\over{2\sin{z\over2}\cos{z\over2}}} = {{2\sin^2 {z\over2}}\over{2\sin{z\over2}\cos{z\over2}}} = {{\sin {z\over2}}\over{\cos{z\over2}}} $$

\vfill\eject

We can also proceed like this:

$$\int {1\over{x\sqrt{x^4+1}}} \, dx$$
$$x^4 = u \qquad 4 x^3\, dx = du$$
$$\int {1\over{4u\sqrt{u+1}}} \, du$$
$$v = u + 1 \qquad dv = du$$
$$\int {1\over{4(v-1)\sqrt{v}}} \, dv$$
$$z^2 = v \qquad 2 z dz = dv$$
$$\int {1\over{4(z^2-1)z}} \, 2z\, dz$$
$$\int {1\over{2(z^2-1)}} \, dz$$
$${1\over{z^2-1}} = {1\over2}{1\over{z-1}} - {1\over2}{1\over{z+1}}$$
$${1\over4} \int {1\over{z-1}} - {1\over{z+1}} \, dz$$
$${1\over4} \ln (z-1) - \ln (z+1)$$
$${1\over4} \ln {{z-1}\over{z+1}}$$
$${1\over4} \ln {{\sqrt{v}-1}\over{\sqrt{v}+1}}$$
$${1\over4} \ln {{\sqrt{u+1}-1}\over{\sqrt{u+1}+1}}$$
$${1\over4} \ln {{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}}$$

so from the previous page and this one, we conclude

$$\int {1\over{x\sqrt{x^4+1}}} \, dx
={1\over2} \ln {{\sqrt{x^4+1}-1}\over{x^2}}
={1\over4} \ln {{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}}$$

Is this last equality true?  Well, $\ln f^2 = 2\ln f$, so
${1\over4}\ln f^2 = {1\over2}\ln f$, and\ldots

$$\Big({{\sqrt{x^4+1}-1}\over{x^2}}\Big)^2
= {{(\sqrt{x^4+1}-1)^2}\over{x^4}}$$

$${{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}+1}} \cdot
{{\sqrt{x^4+1}-1}\over{\sqrt{x^4+1}-1}}
= {{(\sqrt{x^4+1}-1)^2}\over{x^4+1-1}}$$




\endexample

\vfill\eject
\mysection{Chebyshev's Integral}

\example Compute:
\label{Chebyshev's Integral}
$$\int {{2x^6+4x^5+7x^4-3x^3-x^2-8x-8}\over{(2x^2-1)^2\sqrt{x^4+4 x^3+2 x^2+1}}} \,{\rm d}x$$

The polynomial under the square root is square-free:

\begin{maximablock}
num : 2*x^6 + 4*x^5 + 7*x^4-3*x^3-x^2-8*x-8;
den: 2*x^2-1;
root : x^4+4*x^3+2*x^2+1;
factor(root);
\end{maximablock}

\ldots so $y^2 = x^4+4 x^3+2 x^2+1$; $\{1, y\}$ is an integral basis;
and our normal form for this integral is:

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)y}\over{(2x^2-1)^2(x^4+4 x^3+2 x^2+1)}} \,{\rm d}x$$

Applying now Bronstein's Hermite reduction from
section 2.1 of his ``Symbolic Integration Tutorial'' with $v=2x^2-1$
to eliminate this square in the denominator:

\begin{maximablock}
gradef(y,x,ratsimp(D(root)/2*y/root));
D(y);
U: root;
V: den;
S2: ratsimp(U*V^2*D(y/V));
\end{maximablock}

Now we want to solve $f_2 S_2 = A_2 y$ where $A_2 y$ is our numerator.

\begin{maximablock}
f[2] ::: num*y/S2;
T[2] ::: num(f[2]);
/* Q::: S2/y; */
Q ::: denom(f[2]);

[A,R,g] : gcdex(V,Q,x);
/* T2: num; */
[Q2, B2]: divide(T[2]*R, V, x);
h ::: A*num*y/(V*U) - (D(V)*Q2+D(B2))*y/V + Q2*D(y);

simp: false$
'integrate(num*y/den,x) = ratsimp(B2*y/V) + 'integrate(h,x);
simp: true$
\end{maximablock}

\vfill\eject
\bigskip
\begin{center}
Non-zero residues

\begin{supertabular}{r @{} l | r @{} l | r @{} l}
\multicolumn{2}{c|}{x} & \multicolumn{2}{c|}{y} & \multicolumn{2}{c}{residue} \cr
\hline
&$\sqrt{2}\over 2$ & &${1\over 2} + \sqrt{2}$ & &${5\over2}$ \cr
&$\sqrt{2}\over 2$ & $-$&${1\over 2} - \sqrt{2}$ & $-$&${5\over2}$ \cr
$-$&${\sqrt{2}\over 2}$ & &${1\over 2} - \sqrt{2}$ & &${5\over2}$ \cr
$-$&${\sqrt{2}\over 2}$ & $-$&${1\over 2} + \sqrt{2}$ & $-$&${5\over2}$ \cr
\end{supertabular}
\end{center}


$$A(x) = 1023x^8+4104x^7+5048x^6+2182x^5+805x^4+624x^3+10x^2+28x$$
$$B(x) = 1025x^{10} + 6138x^9 + 12307x^8 + 10188x^7 + 4503x^6 + 3134x^5 + 1598x^4 + 140x^3 + 176x^2 +2$$
$$C(x) = 32x^{10}-80x^8+80x^6-40x^4+10x^2-1$$

$$\int {{(2x^6+4x^5+7x^4-3x^3-x^2-8x-8)y}\over{(2x^2-1)^2(x^4+4 x^3+2 x^2+1)}} \,{\rm d}x
= {{(x+{1\over2})y}\over{2x^2-1}} + {1\over2}\ln{{A(x)y - B(x)}\over{C(x)}}
$$


\endexample

\vfill\eject
\mysection{An integral Maxima can't solve}

\example
\label{an integral Maxima can't solve}
Integrate $\int \frac{x^9+2x^7-x}{(x^4+2x^2+1)\sqrt{x^8+1}}\, dx$

When I say that Maxima can't solve this integral, I mean that its
built-in integration routine can't solve the integral:

\begin{maximablock}
f: y^2 - x^8 - 1$

integrand: (x^9+2*x^7-x)/((x^4+2*x^2+1)*y);

integrate(subst([y=sqrt(x^8+1)], integrand), x);
\end{maximablock}

Now let's attack the problem using the techniques of this book.

Since $x^4+2x^2+1=(x^2+1)^2$, our finite poles are at $x=\pm i$,
as well as the eight roots of $x^8+1=0$.

\begin{maximablock}
puiseux(f, x, y, %i, sqrt(2), -1,
        integrand * del(x));
puiseux(f, x, y, -%i, sqrt(2), -1,
        integrand * del(x));
\end{maximablock}
\begin{maximablocksmall}
for i:1 thru 15 step 2 do
   disp(puiseux(f, x, y, cis(i*%pi/8), 0, -1,
                integrand * del(x)));
\end{maximablocksmall}

We also need to check for poles at infinity.

\begin{maximablock}
puiseux(f, x, y, inf, inf, -1,
        integrand * del(x));
\end{maximablock}

We've found two second order poles at the ordinary points $(\pm i, \sqrt{2})$,
as well as two third order poles at a singular point with two sheets at infinity.

Our next goal is to construct a basis for a suitable Riemann-Roch space.
We ignore the poles at infinity and invert the signs of the finite poles,
since the convention for Riemann Roch spaces is their functions must
have order greater than the {\it negative} of a divisor, and conclude
that the Riemann-Roch space that we're interested in is:

$${\cal L}(Z^2(i, \sqrt{2}) Z^2(-i, \sqrt{2}))$$

i.e, all the functions on our algebraic curve with at most second order
poles at $(\pm i,\sqrt{2})$ and no other finite poles.

% https://ocw.mit.edu/courses/mathematics/18-782-introduction-to-arithmetic-geometry-fall-2013/lecture-notes/MIT18_782F13_lec21.pdf
% Theorem 21.9. For any divisor D we have dim L(D)  deg D0 + 1.

% or use Riemann-Roch theorem directly

\endexample
