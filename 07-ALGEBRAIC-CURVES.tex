
\mychapter{Algebraic Curves}

Having addressed logarithmic and exponential extensions, we now turn
to the algebraic extension, which turns out to be completely different
in character from the two transcendental cases.

\begin{comment}

To justify that statement, let's begin by trying to attack algebraic
extensions in the same manner as the two transcendental cases.

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i (\theta^{i})')$$

Presumably, we know how to differentiate $\theta$, so let's write
$(\theta^i)' = \sum_j \psi_{i,j} \theta^j$:

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i \sum_j \psi_{i,j} \theta^j)$$

Writing the $a_i$'s as a column vector $A$, we'll end up with a matrix equation:

$$I = A' + \Psi A$$

This is superficially similar to a Risch equation; it's a matrix Risch
equation.  To proceed in the same manner as before, we'd now have to
construct partial fractions expansions of everything and begin looking
at how cancellation might occur between our various components.  The
variety of possible cancelations between our terms seems daunting.

\vfill\eject

$$\int y\,dx \qquad y^2 = 4-x^2$$

Let's assume that our solution has the form $S = ay+b+\ln (cy+d)$, where $a$
and $b$ are rational functions in $x$ and $c$ is a polynomial in $x$.

$$S' = a' y + a y' + b' + \frac{c'y+cy'+d'}{cy+d}$$
$$2 y y' = -2x \qquad y' = -\frac{x}{y} = - \frac{x}{4-x^2}y$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}\frac{cy-d}{cy-d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})cy^2+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2y^2-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})c(4-x^2)+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c'(4-x^2) - c x)c+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2} + \frac{cd-(c' - c \frac{x}{4-x^2})d}{c^2(4-x^2)-d^2})y + b' + \frac{(c'(4-x^2) - c x)c -d^2}{c^2(4-x^2)-d^2}$$


The correct answer is

$$S = 2i\ln(ix-y)+\frac{x}{2}y$$

Instead, we'll use a different approach...

\vfill\eject

\end{comment}

How might we handle a simple algebraic extension?  A crucial property
of {\it algebraic functions}, as elements of an algebraic extension
are called, is that they admit series expansions everywhere, including
infinity, so long as we allow a finite number of negative exponents.
Such functions are called {\it meromorphic}.  The logarithm function
fails to be meromorphic at the origin, and the exponential function
fails to be meromorphic at infinity, but algebraic functions are
meromorphic everywhere, including infinity.

This means that around any specific point, we can construct a series
expansion of the integrand and integrate termwise to obtain a series
expansion for the integral.  At first this doesn't seem terribly
useful, because series expansions are infinite and we're trying to
construct closed-form solutions, but it turns out that only a finite
number of places will have negative exponents in their series
expansions and that the function is completely specified, up to an
additive constant, by the coefficients of the negative powers.

Thus, the basic strategy is first to identify the function's {\it
poles}, the places where its value becomes infinite, and compute the
{\it principal part} of the series expansions there, which are the
negative exponents and their coefficients.  This is fairly
straightforward, though there are issues of computational complexity
that make it non-trivial.  Then we integrate termwise, which is
trivial, and obtain local series expansions at the poles of the
solution.  Next, we need to reassemble this local information into a
global function (if one exists), a {\it Mittag-Leffler problem}, for
which I will present a basic algorithm in this chapter, although more
efficient techniques have been developed.

What about the logarithmic terms?  This turns out to be the most
difficult part of the problem.  We can begin to analyze them using the
same techniques, by noting that the $t^{-1}$ terms in the principal
parts of the integrand lead directly to logarithms in the integral,
and furthermore that the coefficients of these terms give us the
locations and orders of the poles and zeros in the logarithms.  This
information specifies an algebraic function up to a multiplicative
constant\footnote{Of course.  Due to the presence of a constant of
integration, we expect to specify the main part of the integral up to
an {\it additive} constand, and the logarithmic parts of the integral
up to a {\it multiplicative} constant.}, and our algorithm can be
adapted without too much trouble to handle this case.

The problem is that no algebraic function might exist that match a
given set of zeros and poles, but increasing the order of the zeros
and poles might produce a solution.  This corresponds to raising the
logarithm term to powers, i.e, $\ln f$ is the same as $\frac{1}{2} \ln
f^2$, which is the same as $\frac{1}{3} \ln f^3$, except that in our
case the lower powers might not exist in our function field, even
though higher powers do.  What powers should we use?  We could go on
raising to higher and higher powers, hoping that something will work,
but the only known algorithm to limit this search requires reducing
modulo a prime, and that requires techniques that weren't developed
until the 1960s.  Before heading into {\it modern algebraic geometry},
let's see how far we can get with the classical algebraic geometry of
the nineteenth century.

\section{Basic Algebraic Geometry}

The roots of algebraic geometry lie in studying the zeros of
polynomial equations.  We began with a single polynomial in a single
variable, and have learnt a great deal about it.  We know how to solve
it (at least in terms of radicals) if its degree is less than 5.
Galois proved that no such solution (in radicals) exists (in the
general case) for larger degree, though abstract algebra provides us
with a suitable general theory to handle this case.  Simple long
division tells us that it can have no more roots than its degree, and
Gauss showed that all of the roots exist as complex numbers --- the
Fundamental Theorem of Algebra.

The next logical step is to consider zeros of a single polynomial in
two variables, and this equation has also received a great deal of
attention from mathematicians.  Like the univariate case, we have
theories devoted to low-order special cases --- {\it linear equations}
(all terms first degree or constant), the {\it conic sections} (all
terms second degree or less), and the {\it elliptic curves} (one term
third degree; all others second degree or less).  In the general case,
$\sum a_{ij} x^i y^j = 0$ is called an {\it algebraic curve}, and a
rational function in $x$ and $y$ is called an {\it algebraic
function}.  These will be our main focus of attention in this chapter.

The first problem we face when dealing with algebraic curves is the
multi-valued nature of their solutions.  Consider, once again, the
algebraic function $y$ defined on the algebraic curve $y^2 = x^2 - 1$.
There are, in fact, two seperate algebraic functions that solve this
equation --- both $y$ and $-y$ are solutions.  Conventionally, we
express this by writing something like $y = \pm\sqrt{x-1}$, but for
higher degree curves this kind of notation becomes unsuitable.  How,
for example, do you express the three possible solutions to a cube
root, and how do you deal with the general case where y can appear
multiple times in the curve's defining polynomial?

Our solution to this problem is to regard the entire algebraic curve
as a two-dimensional surface in a four-dimensional space.  Why four
dimensions?  Well, just as in the univariate case, we find it
convenient to work with complex numbers, so as to deal easily with
roots of negative numbers.  Regarding both $x$ and $y$ as complex
numbers (two dimensions each), and plotting them against each other,
we obtain a four dimensional space.  Just as in the real case, where
an equation like $x^2 + y^2 = 1$ defines a circle, an algebraic curve
defines a surface, the loci of $x$ and $y$ that satisfy the defining
polynomial.

The defining polynomial can be regarded as a polynomial in $y$, whose
coefficients are polynomials in $x$, simply by collecting terms with
like powers of $y$.  For any given value of $x$, we have a polynomial
in $y$ with complex coefficients that yields at most $n$ solutions.
We can be more specific.  For any given value of $x$, we have {\it
exactly} $n$ solutions for $y$ {\it unless} one of two things happen.
Either the leading ($y^n$) coefficient is zero, in which case we have
less than $n$ solutions due to having a polynomial of degree less than
$n$, or the polynomial has multiple identical roots, a {\it multiple
point} of the algebraic curve.

Now, the coefficient of $y^n$ in the defining polynomial will be a
polynomial in $x$, which has a finite number of roots at which it is
zero, so there are only a finite number of points where the defining
polynomial is of degree less than $n$ in $y$.  As $x$ approaches one
of these points, the value of the $y^n$ coefficient approaches zero,
which causes at least one of the roots to approach infinity.  We'll
deal with these points by introducing a line at infinity, forming
{\it projective space} and creating a {\it compact} surface.

Likewise, there are only a finite number of multiple points with
multiple identical roots, as can be seen by considering the
discriminant of defining polynomial (Theorem ?), regarded as a
polynomial in $y$, with coefficients in ${\bf C}[x]$.  The
discriminant, as the determinant of a matrix with coefficients in
${\bf C}[x]$, exists itself in ${\bf C}[x]$, and therefore will have
only a finite number of points where it is zero.  Thus, an algebraic
curve has only a finite number of multiple points, which can be found
by computing the zeros of simple, univariate polynomials.

Multiple points are further classified according to whether or not the
curve is locally Euclidean in their neighborhood.  Geometrically, this
corresponds to looping around the point until you return to your
starting point.  If a single such {\it cycle} covers all the sheets of
the curve, the curve is locally Euclidean, and we have an {\it
ordinary point} of the curve, albeit one with {\it ramification}, the
{\it ramification index} being how many times we had to circle the
point.  Otherwise, multiple cycles are required to cover all of the
sheets, and we have a {\it singular point}.  Analytically, both
partial derivatives of the curve's polynomial are zero at a singular
point, while at least one is non-zero at ordinary points.  This
analysis is facilitated by Newton polygons.

\example Find the singular points of $y^2 = x^2 - 1$

We normalize the defining polynomial by writing it as $y^2 - x^2 + 1 =
0$, and begin by noting that the coefficient of $y^2$ is 1, so the
defining polynomial is second degree for all finite values of $x$.
Where does it have multiple roots?  We compute the discriminant:

$${\rm disc}_y(y^2 - x^2 +1) = {\rm res}_y(y^2-x^2+1, 2y) =
\det\left\vert \begin{matrix}
1 & 0 & -x^2+1 \cr
2 & 0 & 0 \cr
0 & 2 & 0\end{matrix} \right\vert = 4(x^2-1)$$

Thus, we conclude that the multiple points of $y^2 = x^2 - 1$ lie at
the roots of the discriminant, which are $x = \pm 1$.  The partial
derivative of the polynomial with respect to $x$ is $-2x$, which is
non-zero, so neither of these multiple points are singular.

What about the points at infinity?  Introducing the subsitutions
$u=x^{-1}$ and $v=y^{-1}$, our curve becomes $u^2 - v^2 + u^2 v^2 =
v^2 (u^2 - 1) + u^2 = 0$, which has a multiple point at $(0,0)$,
since when $u=0$, both of the roots of $-v^2 = 0$ are identical.
Also, both partial derivatives are zero, so this is a singular
point of the curve.

\endexample

Any ordinary point can be expanded using a power series in
$(x-\alpha)$, which for non-ramified points is a straightforward
application of the Implicit Function Theorem.

IFT: [Baby Rudin 9.28; 2-dim complex version] Let f be an analytic
mapping of an open set $E \in {\mathbb C}^2$ into ${\mathbb C}$, such
that $f(x,y)=0$ and $\frac{df}{dx} \ne 0$, then an analytic
function $g(y)$ exists such that $f(x,g(y))=0$.

For infinity and/or poles, substitute z=1/x or v=1/y.

For ramification points, we use substitution of the form
$x=t^r+\alpha$, where $r$ is the {\it ramification index}, then use
composition of analytic functions (x is analytic everywhere; y is
analytic as a function of x everywhere except at t=0, so y is analytic
as a function of t everywhere except at t=0) to establish that y is
analytic everywhere on the t-plane except possibly at the origin.
Then use existence of the Laurent series (Silverman 11.2) and
continuity of the roots (HOW?) to establish analyticity at the
multiple point, and consequently existance of a power series, but in
$t$, not $(x-\alpha)$, a {\it Puiseux series}.

Singular points will admit multiple Puiseux series, each one
corresponding to a single cycle.  The simplest way to compute Puiseux
series is to use Newton polygons to determine ramification,
then setup a trial series with the correct ramification and
substitute it into the curve's defining equation.

%\mysection{meromorphic functions are analytic}
%
%first, trace of a meromorphic function is meromorphic on C(x), and is
%thus a rational function
%
%Liouville's theorem: a bounded entire function is constant
%
%Proof A: (Silverman) use a Taylor series expansion around z=0, which
%is valid in the entire plane (since the function is entire).  Cauchy's
%inequality $|f| \le M ==> |c_n| \le M/{R^n}$ (eq. 10.8') as R->infty
%implies that the function is constant.
%
%Lemma: A entire function with no singularities, even at infinity, is
%constant.
%
%Proof: We can do a Taylor series expansion at the origin, whose
%non-zero terms will correspond to the principle part of the expansion
%at infinity, which must therefore be zero.
%
%Next: A entire function with only a pole at infinity is a polynomial.
%The principle part at infinity will be a polynomial.  Subtract it out
%to get a function with no singularities, which must be constant.
%
%Next: Given a function with only a finite number of finite poles,
%multiply it by a polynomial (the denominator) matching the poles with
%zeros.  Now we've got a function with only a pole at infinity, which
%must be a polynomial (the numerator).

\example Find the Puiseux expansions of y at the multiple points of the
curve $y^2 = 1 - x^2$

We'll start with the finite zeros of $y$ at $(x,y)=(\pm 1, 0)$.  The
analysis is almost the same in both cases, so I'll just do (1,0).
First, construction of the Newton polygon requires recasting the
curve's polynomial into a form centered about the point being
analyzed, i.e, $y^2 + (x-1)^2 + 2(x-1) = 0$.  The polygon's only
non-trivial line segment has slope -2 and width 1, telling us that
we'll require a single Puiseux series with ramification index 2:

$$x=t^2+1; \qquad x^2=t^4+2t^2+1$$

We know that y can be expressed as a power series in t, so we'll write
it in that form:

$$y=a_0 + a_1 t + a_2 t^2 + a_3 t^3 + a_4 t^4 + \cdots$$

$$y^2 = a_0^2 + 2 a_0 a_1 t + (2 a_0 a_2 + a_1^2) t^2 + (2 a_0 a_3 + 2 a_1 a_2) t^3 + (2 a_0 a_4 + 2 a_1 a_3 + a_2^2) t^4 + \cdots$$

Now, substituting these expressions for $x^2$ and $y^2$ into the
curve's defining equation $y^2 + x^2 - 1 = 0$ and equating coefficients
of like powers of $t$, we find:

$$a_0^2=0 \qquad 2 a_0 a_1 = 0 \qquad 2 + 2 a_0 a_2 + a_1^2 = 0$$
$$2 a_0 a_3 + 2 a_1 a_2 = 0 \qquad 1 + 2 a_0 a_4 + 2 a_1 a_3 + a_2^2 = 0$$

The first equation tells us that $a_0 = 0$, the second equation tells
us nothing (becasue $a_0=0$), the third equation tells us that $a_1
= \pm\sqrt{2}i$, the fourth equation tells us that $a_2=0$ and the
fifth equation tells us that $a_3 = \pm \frac{\sqrt{2}}{4} i$, so

% $$a_0=0; \qquad a_1 = \pm\sqrt{2}i; \qquad a_2 = 0; \qquad a_3 = \pm \frac{\sqrt{2}}{4} i$$

$$x = t^2 +1; \qquad y = \pm\left[ \sqrt{2}it + \frac{\sqrt{2}}{4} it^3 + \cdots \right]$$

It would seem that we have two different series to chose from.  This
is not really the case, as they differ by only a $180^\circ$ rotation
in the t-plane, as can been seen by substituting $t=-t$, which
transforms one of the y-series into the other, while leaving the
x-series unchanged.

Now, let's analyze the singular point at infinity.  Again, we move
infinity to a finite point (0) with the substitutions $x=u^{-1}$ and
$y=v^{-1}$.  Our curve becomes:

$$(u^2 - 1) v^2 - u^2 = 0$$

The Newton polygon has a single line segment, slope -1, length 2,
telling us that we'll have two separate cycles, each with ramification
index 1.  Thus, $u$ can be used directly as a uniformizing variable,
and we postulate an expansion for $v$ in the form:

$$v = a_0 + a_1 u + a_2 u^2 + a_3 u^3 + \cdots$$

$$v^2 = a_0^2 + 2 a_0 a_1 u + (2 a_0 a_2 + a_1^2) u^2 + (2 a_0 a_3 + 2 a_1 a_2) u^3 + (2 a_0 a_4 + 2 a_1 a_3 + a_2^2) u^4 + \cdots$$

Plugging this into $(u^2 - 1) v^2 - u^2$ and setting all the resulting
coefficients to zero, we conclude:

$$a_0 = 0; \qquad a_1 = \pm i; \qquad a_2 = 0; \qquad a_3 = \pm \frac{1}{2}i$$

$$v = \pm i u \pm \frac{1}{2} i u^3 + \cdots$$

This time, without ramification, we actually have two distinct series
that will yield two different values of $v$ for each value of $u$.
Inverting back to our original coordinates, we obtain:

$$y^{-1} = \pm \left[ i x^{-1} + \frac{1}{2} i x^{-3} + \cdots \right]$$

Yet this is not an expansion for $y$, nor is it a Puiseux series,
since it has an infinite number of negative exponents.  We can invert
the series (HOW?) to obtain our final result:

\endexample

\example Find the principal parts of $\frac{1}{y}$ on the curve
$y^2 = 1 - x^2$

Remember that principal parts of an algebraic function are the parts
of its series expansion with negative powers at its poles.  So, the
first step is to locate the function's poles, which in this case is
simply the places where the denominator is zero, and that's just
$x=\pm 1$.  We're already computed a series expansion for $y$ at these
points, so we can invert that solution to obtain a series for
$\frac{1}{y}$.  We do this by noting that since $y$ has a simple zero
at $(\pm 1, 0)$, $\frac{1}{y}$ must have a simple pole at these places,
postulating a series expansion starting with $t^{-1}$, multiplying
and equating terms:

$$\frac{1}{y} = \pm\left[ -\frac{\sqrt{2}}{2} it^{-1} + \cdots \right]$$

\endexample

%\example Find the principal parts of $\frac{1}{y} dx$ on the curve
%$y^2 = 1 - x^2$
%
%Differential forms are different from functions, and have different
%series expansions.
%
%$\frac{1}{y} dx = \left[ -\sqrt{2}i + \cdots \right] $
%
%\endexample

\example Find the principal parts of $\frac{x}{y} \ud x$ on the curve
$y^2 = 1 - x^2$

Differential forms are not functions, and have different series
expansions.  This is primarily due to the presence of the
differential, which must be adjusted at ramification points.  Thus,
we've already computed a series expansion for $\frac{1}{y}$ at $x=\pm
1$, expressed in terms of $t$.  Now $x=\pm 1 + t^2$, so $\ud x=2t\ud
t$

$$\frac{x}{y} \ud x = \pm \left[\pm 1 + t^2 \right] \left[ -\frac{\sqrt{2}}{2} it^{-1} + \cdots \right] 2t \ud t$$

$$\frac{x}{y} \ud x = \left[ -\sqrt{2} i + \cdots \right] \ud t$$

In short, even though both $\frac{1}{y}$ and $\frac{x}{y}$ have poles
at $x=\pm 1$, $\frac{x}{y} \ud x$ does not!  Its behavior at infinity
also requires analysis, since $x$ and $\ud x$ both have poles at
infinity, but $\frac{1}{y}$ does not (it has a zero at infinity).
As before, we'll use $u=\frac{1}{x}$ for a uniformizing variable,
so $\ud x = -\frac{1}{u^2} \ud u$

$$\frac{x}{y} \ud x = \pm \frac{1}{u} \left[ i u + \frac{1}{2} i u^3 + \cdots \right] \left[-\frac{1}{u^2} \ud u\right]$$
$$ = \mp \left[ i u^{-2} + \frac{1}{2} i + \cdots \right] \ud u$$


\endexample

\example Evaluate $\int \frac{x}{\sqrt{x^2+1}}\,dx$

This is a simple integral that can be easily solved using first year
calculus techniques, but let's see how to attack it using the more
sophisticated techniques of this chapter.

First, we normalize the integrand (CLARIFY) and convert it into
a rational function on an algebraic curve:

$$\int \frac{x}{x^2+1}y\,dx\qquad y^2=x^2+1$$

Next, we identify the poles of the integrand.  They are located
where $x^2+1=0$, over the points $x=\pm i$.  These are also
ramification points of the algebraic curve.  Its Riemann surface
looks something like this:

\begin{maximacode}
plot3d ([[sqrt((x**2-y**2+1)**2+(2*x*y)**2), [x, -2, 2], [y, -2, 2]],
   -sqrt((x**2-y**2+1)**2+(2*x*y)**2)], [x, -2, 2], [y, -2, 2],
   [legend, false], [xlabel, "Re"], [ylabel, "Im"], [zlabel, false],
   [xtics, -1,1,1], [ytics, -1,1,1], [ztics, false],
   [azimuth, 70], [elevation, 80], [grid, 100, 100], [mesh_lines_color, false],
   [pdf_file, "./07-GRAPH1.pdf"])$
\end{maximacode}

\begin{figure}[H]
\begin{center}
\optionalgraphics[width=0.7\textwidth]{07-GRAPH1.pdf}
\end{center}
\end{figure}

Let's compute a Puiseux series expansion of $\frac{x}{x^2+1}y\,dx$
at $(x,y)=(i,0)$.  We'll use the uniforming variable $t$, with
$x=t^2+i$.  $y^2 = x^2+1 = (t^2+i)^2+1 = t^4+2it^2$

{\tt puiseux}, adopted from {\tt implicit_taylor} in the Maxima
distribution, computes the Puiseux series of a function {\tt f} in
variables $x$ and $y$, centered around place {\tt (pt,init_a0)},
to degree {\tt deg}.

\begin{maximablock}
puiseux(f,pt,deg,init_a0):=
 block([n:deg,ratwtlvl:deg+1,ratweights:[],expansion,res,ans:[],eqns],
  ratweight(x,1),
  expansion:sum((x-pt)^i*a[i],i,0,n+1),
  res:subst([y=expansion],f),
  res:ratsimp(res),
  eqns:create_list(coeff(res,x,i),i,1,n+1),
  ans:[a[0]=init_a0],
  eqns:subst(ans,eqns),
  for i:1 thru n+1 do block([s:solve(eqns[i],a[i-1])],
    if length(s) >= 1 then ans:endcons(s[1],ans),
    eqns: subst(ans,eqns)),
  ratwtlvl:deg,
  rat(subst(ans,expansion)))$

puiseux(y^2 - (x^2+%i)^2 - 1, 0, 5, 0);
\end{maximablock}

\begin{comment}
  for i:1 thru n do (ans:append(ans,solve(eqns[i],concat(a,i))),
                   eqns: sublis(ans,eqns));
  for i:1 thru n do (ans:append(ans,solve(eqns[i],a[i])),
                   eqns: sublis(ans,eqns));
  sublis(ans,expansion));
\end{comment}


\endexample

% A {\it Riemann surface}, more precisely, is a two-dimensional manifold
% with a {\it complex analytic structure}.
