
\mychapter{Algebraic Curves}

{\bf THIS CHAPTER IS INCOMPLETE.}

Having addressed logarithmic and exponential extensions, we now turn
to the algebraic extension, which at first appears to be completely
different in character from the two transcendental cases.  The
differences stem largely from the lack of unique factorization in the
algebraic case; algebraic extensions are not, in general, unique
factorization domains.

\begin{comment}

To justify that statement, let's begin by trying to attack algebraic
extensions in the same manner as the two transcendental cases.

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i (\theta^{i})')$$

Presumably, we know how to differentiate $\theta$, so let's write
$(\theta^i)' = \sum_j \psi_{i,j} \theta^j$:

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i \sum_j \psi_{i,j} \theta^j)$$

Writing the $a_i$'s as a column vector $A$, we'll end up with a matrix equation:

$$I = A' + \Psi A$$

This is superficially similar to a Risch equation; it's a matrix Risch
equation.  To proceed in the same manner as before, we'd now have to
construct partial fractions expansions of everything and begin looking
at how cancellation might occur between our various components.  The
variety of possible cancelations between our terms seems daunting.

\vfill\eject

$$\int y\,dx \qquad y^2 = 4-x^2$$

Let's assume that our solution has the form $S = ay+b+\ln (cy+d)$, where $a$
and $b$ are rational functions in $x$ and $c$ is a polynomial in $x$.

$$S' = a' y + a y' + b' + \frac{c'y+cy'+d'}{cy+d}$$
$$2 y y' = -2x \qquad y' = -\frac{x}{y} = - \frac{x}{4-x^2}y$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}\frac{cy-d}{cy-d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})cy^2+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2y^2-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})c(4-x^2)+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c'(4-x^2) - c x)c+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2} + \frac{cd-(c' - c \frac{x}{4-x^2})d}{c^2(4-x^2)-d^2})y + b' + \frac{(c'(4-x^2) - c x)c -d^2}{c^2(4-x^2)-d^2}$$


The correct answer is

$$S = 2i\ln(ix-y)+\frac{x}{2}y$$

Instead, we'll use a different approach...

\vfill\eject

\end{comment}

There are four basic operations we perform on a rational
function in order to integrate it:

\begin{enumerate}
\item We {\bf factor} its numerator and denominator
\begin{sagecode}[ch7-intro]

load("sagecommon.sage");
R.<x> = QQ[];
n = (x-1)^2*(x+3);
d = (x-2)*(x-3);
p = n/d;
def partfrac2(n,d):
    r = partfrac(n%d,d)
    a = latex(n//d)
    a += '&'
    for k,v in r.items():
        if v < 0:
            a += "-\\frac{"+latex(-v)+"}{"+latex(k[0])+"}"
        else:
            a += "+\\frac{"+latex(v)+"}{"+latex(k[0])+"}"
    return a
def partfrac3(n,d):
    r = partfrac(n%d,d)
    a = latex(n//d).replace("x", "\\frac{{1}}{{1/x}}")
    a += '&'
    for k,v in r.items():
        if v < 0:
            a += "-\\frac{"+latex(-v)+"}{"+latex(k[0])+"}"
        else:
            a += "+\\frac{"+latex(v)+"}{"+latex(k[0])+"}"
    return a
\end{sagecode}
$$\sage[ch7-intro]{p} = \frac{\sage[ch7-intro]{n.factor()}}{\sage[ch7-intro]{d.factor()}}$$
From the factorization, we can read off the locations of the function's
zeros and poles.  In this example, our zeros are at -3 and 1 (multiplicity 2),
and our poles are at 2 and 3.
\item We construct a {\bf partial fractions expansion}
\begin{IEEEeqnarray*}{rCCL}
\sage[ch7-intro]{p} & = & \sage[ch7-intro]{partfrac2(n, d)} \\
                    & = & \sage[ch7-intro]{partfrac3(n, d)}
\end{IEEEeqnarray*}
\item We reconstruct a function given a factorization of its numerator and denominator
(or equivalently, a list of poles and zeros along with
their multiplicities)
\item We reconstruct a function given a partial fractions expansion (or
equivalently, a set of principal parts expansions
at the function's poles)
\end{enumerate}

In this chapter, we'll develop a basic set of technical tools
for working in the simplest kind of algebraic extension, an
extension of ${\bf C}(x)$.  This will prepare us for the
next chapter, where we'll study {\it Abelian integrals}, which
are integrals whose integrands are formed from polynomials and roots
of polynomials.  In other words, integrands in an algebraic extension
of ${\bf C}(x)$.

How might we handle an algebraic extension of ${\bf C}(x)$?  A crucial
property of {\it algebraic functions}, as elements of an algebraic
extension are called, is that they admit series expansions everywhere,
including infinity, so long as we allow a finite number of negative
exponents.  Such functions are called {\it meromorphic}.  The
logarithm function fails to be meromorphic at the origin, and the
exponential function fails to be meromorphic at infinity, but
algebraic functions are meromorphic everywhere, including infinity.

This means that around any specific point, we can construct a series
expansion of the integrand and integrate termwise to obtain a series
expansion for the integral.  At first this doesn't seem terribly
useful, because series expansions are infinite and we're trying to
construct closed-form solutions, but it turns out that only a finite
number of places will have negative exponents in their series
expansions and that an algebraic function is completely specified, up to an
additive constant, by the coefficients of the negative powers.

Thus, the basic strategy is first to identify the function's {\it
poles}, the places where its value becomes infinite, and compute the
{\it principal part} of the series expansions there, which are the
negative exponents and their coefficients.  This is fairly
straightforward, though there are issues of computational complexity
that make it non-trivial.  Then we integrate termwise, which is
trivial, and obtain local series expansions at the poles of the
solution.  Next, we need to reassemble this local information into a
global function (if one exists), a {\it Mittag-Leffler problem}, for
which I will present a basic algorithm in this chapter, although more
efficient techniques have been developed.

What about the logarithmic terms?  This turns out to be the most
difficult part of the problem.  We can begin to analyze them using the
same techniques, by noting that the $t^{-1}$ terms in the principal
parts of the integrand lead directly to logarithms in the integral,
and furthermore that the coefficients of these terms give us the
locations and orders of the poles and zeros in the logarithms.  This
information specifies an algebraic function up to a multiplicative
constant\footnote{Of course.  Due to the presence of a constant of
integration, we expect to specify the main part of the integral up to
an {\it additive} constand, and the logarithmic parts of the integral
up to a {\it multiplicative} constant.}, and our algorithm can be
adapted without too much trouble to handle this case.

The problem is that no algebraic function might exist that match a
given set of zeros and poles, but increasing the order of the zeros
and poles might produce a solution.  This corresponds to raising the
logarithm term to powers, i.e, $\ln f$ is the same as $\frac{1}{2} \ln
f^2$, which is the same as $\frac{1}{3} \ln f^3$, except that in our
case the lower powers might not exist in our function field, even
though higher powers do.  What powers should we use?  We could go on
raising to higher and higher powers, hoping that something will work,
but the only known algorithm to limit this search requires reducing
modulo a prime, and that requires techniques that weren't developed
until the 1960s.  Before heading into such {\it modern algebraic
geometry}, however, let's see how far we can get with the classical
algebraic geometry of the nineteenth century.

\mysection{Classical Algebraic Geometry}

The roots of algebraic geometry lie in studying the zeros of
polynomial equations.  We began with a single polynomial in a single
variable, and have learnt a great deal about it.  We know how to solve
it (at least in terms of radicals) if its degree is less than 5.
Galois proved that no such solution (in radicals) exists (in the
general case) for larger degree, though abstract algebra provides us
with a suitable general theory to handle this case.  Simple long
division tells us that it can have no more roots than its degree, and
Gauss showed that all of the roots exist as complex numbers --- the
Fundamental Theorem of Algebra.

The next logical step is to consider zeros of a single polynomial in
two variables, and this equation has also received a great deal of
attention from mathematicians.  Like the univariate case, we have
theories devoted to low-order special cases --- {\it linear equations}
(all terms first degree or constant), the {\it conic sections} (all
terms second degree or less), and the {\it elliptic curves} (one term
third degree; all others second degree or less).  In the general case,
$\sum a_{ij} x^i y^j = 0$ is called an {\it algebraic curve},
and will be our main focus of attention in this chapter.

% , and a
% rational function in $x$ and $y$ is called an {\it algebraic
% function}.  These will
% Note that an algebraic curve's rational functions form a field, the
% {\it function field} of the curve.

One of the first problems we face when dealing with algebraic curves
is the multi-valued nature of their solutions.  Consider the algebraic
algebraic curve $y^2 = x^2 - 1$.  For almost any given $x$,
there are two seperate $y$ values that solve this equation.
Conventionally, we
express this by writing something like $y = \pm\sqrt{x-1}$, but for
higher degree curves this kind of notation becomes unsuitable.  How,
for example, do you express the three possible solutions to a cube
root, and how do you deal with the general case where $y$ appears
multiple times in the curve's defining polynomial, something
like $y^3 + x^2y^2 - x + 4y=0$?

Our solution to this problem is to regard the entire algebraic curve
as a two-dimensional surface in a four-dimensional space.  Why four
dimensions?  Well, just as in the univariate case, we find it
convenient to work with complex numbers, so as to deal easily with
roots of negative numbers.  Regarding both $x$ and $y$ as complex
numbers (two dimensions each), and plotting them against each other,
we obtain a four dimensional space.  Just as in the real case, where
an equation like $x^2 + y^2 = 1$ defines a circle, an algebraic curve
defines a surface, the loci of $x$ and $y$ that satisfy the defining
polynomial.

% TODO: clarify algebraic geometry conventions w.r.t dimension

% We'd like to construct a series expansion at each point.

Our solution to this problem is to regard the entire algebraic curve
as a surface in $x$-$y$ space, instead of regarding it as a function
of $x$.  How then, do we construct series expansions of rational
functions that involve both $x$ and $y$?  At each point of the
surface, we seek to find a single {\it uniformizing variable} that is
suitable for constructing power series expansions that converge in
an open neighborhood of the point.  This is also
called a {\it local uniformizer}, since no one function is a suitable
uniformizing variable at all points of an algebraic curve.

% A {\it Riemann surface} is a one dimensional connected complex manifold.

Is this possible?

The answer is a qualified ``yes''.  At most points on an algebraic
curve, the answer is an unqualified ``yes'' as a result of the
Implicit Function Theorem:

\theorem
{\bf Implicit Function Theorem}
\label{implicit function theorem}

\cite{baby rudin} Theorem 9.28 is a real version of the theorem.

\cite{guillemin} Lecture 7 starts with a complex version of the theorem.

See {\tt https://math.stackexchange.com/questions/489789}

{\tt https://math.stackexchange.com/a/289640/71520} discusses the
difficulty of moving along the transition $R \to R^n \to C \to C^n$.

The two-dimensional complex analytic version:

Let f be an analytic
mapping of an open set $E \in {\mathbb C}^2$ into ${\mathbb C}$, such
that $f(x,y)=0$ and $\frac{df}{dy} \ne 0$, then an analytic
function $g(x)$ exists such that $f(x,g(x))=0$.
\endtheorem

This theorem applies everywhere on the curve that $\frac{df}{dy} \ne 0$,
which is everywhere except a finite number of points.  Where does it
fail?  Those points at which both $f(x,y)=0$ and $\frac{df}{dy}$
equals zero.  In other words, points at which the defining function
and its derivative with respect to one of its variables share
a zero.  At these points, the curve exhibits a behavior
called {\it ramification}.

If the derivative with respect to one variable is zero, could we try
to apply the theorem using the derivative with respect to the other
variable?  The answer is often yes, but not always.  Ramification thus
occurs with respect to a specific variable, more generally with
respect to a specific mapping.

% The defining polynomial can be regarded as a polynomial in $y$, whose
% coefficients are polynomials in $x$, simply by collecting terms with
% like powers of $y$.  If we fix a given complex value of $x$, we have a
% polynomial in $y$ with complex coefficients that can be solved as a
% univariate polynomial and yields at most $n$ solutions for $y$.  We
% can be more specific.  For any given value of $x$, we have {\it
% exactly} $n$ solutions for $y$ {\it unless} one of two things happen.
% Either the leading ($y^n$) coefficient is zero, in which case we have
% less than $n$ solutions due to having a polynomial of degree less than
% $n$ and the curve has a point at infinity, or the polynomial has
% multiple identical roots at a {\it multiple point} of the curve.

% add a box of examples

% A curve has only a finite number of multiple points, as can be seen
% by considering the {\it discriminant} of the defining polynomial,
% which is the resultant of the polynomial with its partial derivative
% with respect to one of its variables.  The discriminant will be zero
% at multiple points, so these points can be located by computing the
% zeros of a univariate polynomial.

% Multiple points are further classified according to whether or not the
% curve is locally Euclidean in their neighborhood.  Geometrically, this
% corresponds to looping around the point until you return to your
% starting point.  If a single such {\it cycle} covers all the sheets of
% the curve, the curve is locally Euclidean, and we have an {\it
% ordinary point} of the curve, albeit one with {\it ramification}, the
% {\it ramification index} being how many times we had to circle the
% point.  Otherwise, multiple cycles are required to cover all of the
% sheets, and we have a {\it singular point} or {\it singularity}.

On some curves, however, there are points, called {\it singularities},
at which the derivative with respect to both variables is zero.
One might hope to pick a uniformizing variable different
from either coordinate variable that would allow the IFT
to be applied, but this turns out to be impossible.  The
derivative with respect to any algebraic function could
be expanded using partial derivatives with respect to
the coordinate variables, and the resulting derivative
would necessarily be zero:

$$\frac{df}{dz} = \frac{df}{dx}\frac{dx}{dz} + \frac{df}{dy}\frac{dy}{dz} = 0\frac{dx}{dz} + 0\frac{dy}{dz} = 0$$

Something more is required.

Since both $f$ and $\frac{df}{dy}$ share a zero at
a given value of $x$, $f$'s zero must be at least
second order, so singularities are necessarily
multiple points of the curve.

% For infinity and/or poles, substitute z=1/x or v=1/y.

Consider a specific multiple point where $n$ sheets of the curve meet
at a single point $P$.  A small circle at distance $\epsilon$ from $P$
will map to $n$ values of the curve.  Without loss of generality,
let's assume that $P$ is the origin.

[Silverman] Theorem 10.13: If a function is analytic on every point of
a open disk $K: |z-a| < R$, then it has a power series expansion
centered on $a$ that converges everywhere in $K$.

By this theorem, we see that we can surround $P$ with disks of radius
$\epsilon$; a finite number of them will circle around $P$.  These
disks don't have to cover every point in a neighborhood of $P$; they
need only circle $P$ so that we cleanly identify a permutation
of the sheets.

Label the $r$ values of $y$ at $(x+\epsilon)$
as $y_1(x),\ldots,y_n(x)$.  As we trace along the circle
defined by $x=\epsilon e^{i\theta}$ these values
deform continuously as $\theta$ goes from $0$ to $2\pi$.
Once we reach $2\pi$, we have come full circle
and the $y$ values match up, but permuted.
Call the permutation $\sigma$,
so $y_1(x),\ldots,y_n(x)$ map to $y_{\sigma 1}(x),\ldots,y_{\sigma n}(x)$.
The permutation $\sigma$ of the $n$
sheets can decomposed into $k$ disjoint cycles.  We seek to
show that a Riemann surface can be obtained by replacing the singular
point with $k$ distinct points.

Consider a single cycle of length $r$, the {\it ramification index},
and an open disk in the $t$-plane, $\Delta=\{t:|t|<\epsilon\}$.
We wish to exhibit an bihomomorphism from $\Delta$ to the
cycle.  Set $x(t)=t^r$, which is clearly holomorphic in $\Delta$
(in fact, everywhere on the $t$-plane).  The function

$$y(t) = y_{\sigma^{\lfloor (r \, \arg t)/(2 \pi) \rfloor}}(t^r)$$

where $\arg t$ is the complex argument, with range $[0, 2\pi)$,
$\lfloor \cdot \rfloor$ is the integer floor function, and powering
$\sigma$ by an integer applies the permutation that many times.  The
permutation ensures continuity of the function at each transition
between the various $y_n$ functions.  $y(t)$ is obviously holomorphic
away from the origin, but is it also holomorphic at the origin?  Yes,
according to Riemann's theorem on removable singularities.

% where $r$ is the {\it ramification index}, and
% construct a power series, not in $(x-\alpha)$, but in
% $t=(x-\alpha)^{1/r}$, a {\it Puiseux series}.

% As a function of $t$, both $x$ and $y$ are analytic in an open
% neighborhood of $t=0$.  $x$ is analytic as a function of $t$ because
% of the I.F.T. applied to $x=t^r+\alpha$.  $y$ is analytic as a
% function of $x$ because of the I.F.T. applied to the defining equation
% of the curve.  Composition of analytic functions are analytic,
% showing that $y$ is analytic as function of $t$.

% These arguments hold in an open neighborhood of $t=0$.  Continuity of
% the roots shows that $y$ is continuous at $t=0$.  Then use existence
% of the Laurent series (Silverman 11.2) and continuity of the roots
% (HOW?) to establish analyticity at the ramification point.


\begin{mdframed}[backgroundcolor=yellow!20]
\begin{center}
{\bf Riemann removable singularity theorem}
\end{center}
\theorem
\label{Riemann removable singularity theorem}
Let $D \subset \mathbb{C}$ be an open subset of the complex plane,
$a \in D$ a point of $D$ and $f$ a holomorphic function defined on
the set $D \setminus \{a\}$.  The following are equivalent:

\begin{enumerate}
\item $f$ is holomorphically extendable over $a$.
\item $f$ is continuously extendable over $a$.
\item There exists a neighborhood of $a$ on which $f$ is bounded.
\item $\lim_{z\to a}(z - a) f(z) = 0$.
\end{enumerate}

\proof

The implications $1 \Rightarrow 2 \Rightarrow 3 \Rightarrow 4$ are
trivial. To prove $4 \Rightarrow 1$, we first recall that the
holomorphy of a function at $a$ is equivalent to it being analytic at
$a$, i.e. having a power series representation. Define

$$
h(z) = \left\{
\begin{array}{rl}
(z - a)^2 f(z) &  z \ne a ,\\
0              &  z = a .
\end{array} \right.
$$

Clearly, $h$ is holomorphic on $D\backslash\{a\}$, and there exists
$h'(a)=\lim_{z\to a}\frac{(z - a)^2f(z)-0}{z-a}=\lim_{z\to a}(z - a) f(z)=0$
by 4, hence $h$ is holomorphic on $D$ and has a Taylor series about $a$:

$$h(z) = c_0 + c_1(z-a) + c_2 (z - a)^2 + c_3 (z - a)^3 + \cdots \, .$$

We have $c_0 = h(a) = 0$ and $c_1 = h'(a) = 0$; therefore

$$h(z) = c_2 (z - a)^2 + c_3 (z - a)^3 + \cdots \, .$$

Hence, where $z \ne a$, we have:

$$f(z) = \frac{h(z)}{(z - a)^2} = c_2 + c_3 (z - a) + \cdots \, .$$

However,

$$g(z) = c_2 + c_3 (z - a) + \cdots \, .$$

is holomorphic on $D$, thus an extension of $f$.

\endtheorem
Source: copied verbatim from Wikipedia
\end{mdframed}

To see that function $y(t)$ is bounded on $\Delta$, pick a $\delta$ so
that $f(0,\delta)$ is $G$, then ensure that $\epsilon$ is small enough
to ensure that $f(x,\delta) \ne 0 \forall x \in \Delta_\epsilon$.
Since $y(t)$ is continuous on $\Delta_\epsilon$, if it were not
bounded, then $f(t^r,y(t)) = \delta$ for some value $t$, contridicting
our assumption.  Thus, $y(t)$ is bounded on $\Delta$, so it can be
holomorphically extended to the origin by the previous theorem.

{\bf Lemma.}  Given a polynomial $f(x,y)$,
then at any given point $(x_0,y_0)$ and any given real number $\delta>0$,
there exists a real number $\epsilon$ such that
$$f(x,y)\ne f(x_0,y_0) \quad\forall x,y; |x-x_0| < \epsilon; |y-y_0| = \delta$$

{\bf Proof.}  Consider $g(y) = f(x_0,y) - f(x_0,y_0)$, a polynomial in $y$ with
a zero at $y_0$.  Pick a number $a$ such that no other zero is within $\pm a$
of $y_0$.  Consider the complex circle of radius $a$ centered at $y_0$.
$g(y)$ on this circle must have a minimum value that is not zero;
call this minimum value $m$.  Now, at any point $y_0+a$,
$f(x_0, y_0+a)$ by {\bf complex} continuity will have a value $\epsilon$ such that
$x_0$ can be varied up to $\epsilon$ without changing the function by
more than $m$.  But do all of these values have a minimum
greater than zero?

In short, to obtain a complex manifold,
we need to modify our curve slightly by adding additional
points at singularities.
Theorem \ref{Riemann removable singularity theorem}
tells us that no additional information is need to specify
the behavior of holomorphic functions at those additional
points -- their behavior at an isolated point is completely determined by
their behavior in an open neighborhood surrounding that point.

What about meromorphic functions?  They can just be promoted
to holomorphic functions by multiplying them by a suitable
power of the uniformizing variable ($t$, in the above treatment),
and Theorem \ref{Riemann removable singularity theorem} again
tells us that their behavior is completely specified.

Interestingly enough, a rational function can have different values
at the same points over a singularity.

\example
Example: $y^2 = x^3 + x^2$ has a singularity at the origin,
since $f(x,y) = y^2 - x^3 - x^2$, $\frac{df}{dx} = -3x^2-2x$,
$\frac{df}{dy} = 2y$, and $\frac{df}{dx}(0,0) = 0$
and $\frac{df}{dy}(0,0) = 0$.

% INSERT GRAPHIC

The function $y/x$ has the value $1$ on one branch and $-1$
on the other.  It is also possible, straightforward even, to construct functions
with a zero on only one branch ($y/x - 1$) or the other
($y/x + 1$), or a pole on only one branch ($x/(y-x)$),
or a pole on one branch and zero on the other ($x/(y-x) + 1/2$).
\endexample


Analytically, both partial derivatives of the curve's polynomial are
zero at a singular point, while at least one is non-zero at ordinary
points. (PROOF)



Resolving our singularities in this manner creates a complex
manifold, but it lacks a crucial property: topological compactness.
In order to apply Theorem \ref{holomorphic functions on compact
manifolds are constant}, we need a {\it compact} manifold.
We fix this problem by embedding our algebraic curve
in projective space, using a standard construction.


Now, the coefficient of $y^n$ in the defining polynomial will be a
polynomial in $x$, which has a finite number of roots at which it is
zero, so there are only a finite number of points where the defining
polynomial is of degree less than $n$ in $y$.  As $x$ approaches one
of these points, the value of the $y^n$ coefficient approaches zero,
which causes at least one of the roots to approach infinity.  We'll
deal with these points by introducing a line at infinity, forming
{\it projective space} and creating a {\it compact} surface.

\definition
A topological space $X$ is {\it compact} if any open covering
of $X$ admits a finite subset that covers $X$.
\enddefinition

There are several crucial theorems that depend on the topological
property of {\it compactness}.  The complex plane is not compact; we
remedy this by adding a point at infinity to obtain the {\it Riemann
sphere}.  Likewise, two-dimensional complex space is not compact,
either; we remedy this by adding a line at infinity and obtaining {\it
projective space}.

Theorem: Projective space is compact

Munkres Theorem 26.2: Every closed subspace of a compact space is compact.

Theorem: The Zariski topology is the coursest topology in which
singletons are closed.

Varieties are closed in the Zariski topology.

Standard topology is finer than Zariksi, so all open sets in Zariski
are open in standard, and all Zariki-closed sets are closed in standard.

So, varieties are closed in the standard topology, and are therefore
compact in projective space.



Projective space.  Compactness.

Another highly desirable property is to be locally isomorphic to
Euclidean space.  A differentiable surface that is everywhere locally
Euclidean is called a {\it manifold}.

By adding a line at infinity and resolving our singularities, we can
coax our algebraic curve into a compact, connected, complex manifold.
The primary utility of this construction is embodied in the following
theorems.

Regular functions.  Holomorphic functions.  Identical on complex
projective varieties.

\theorem
\label{holomorphic functions on compact manifolds are constant}

Every holomorphic function $M \to C$ on a compact, connected, complex manifold $M$ is constant.
\footnote{
In fact, we don't need the complex structure and can make the stronger
statement that the only regular functions on any projective variety are the constants.
See Proposition 4.2 in
{\tt https://www.math.utah.edu/~bertram/6030/Projective.pdf},
or Hartshorne Theorem (I, 3.4a),
or {\tt https://math.stackexchange.com/questions/56236}.
If the base field is not algebraically closed, however,
new constants may appear as algebraic functions.
For example, the polynomial $(x+y)^2+1$ is
irreducible in the ring $\QQ[x,y]$, so we can use it to construct
an algebraic curve, but $x+y$ is a constant on this
curve, a square root of $-1$, in fact.
}

\proof

{\tt https://math.stackexchange.com/questions/881742}

\cite{guillemin} Lecture 2 contains a proof of the Maximum Modulus Priciple.

\endtheorem

\definition
The {\it principal part} of an algebraic function at a pole is the
portion of its corresponding Laurent series with negative exponents.
\enddefinition

\theorem
\label{algebraic functions are characterized by their principal parts}

An algebraic function on an algebraic curve is completely characterized, up to an additive
constant, by its principal parts.

\proof

Consider two algebraic functions $f$ and $g$ with identical principal
parts.  Taking the difference between them, we obtain a function $f-g$
with no principal parts, i.e, a holomorphic function.  By
Theorem \ref{holomorphic functions on compact manifolds are constant},
$f-g$ must be constant.

\endtheorem

\theorem
\label{algebraic functions are characterized by their divisors}

An algebraic function on an algebraic curve is completely characterized, up to a multiplicative
constant, by its divisor.

\proof

Consider two algebraic functions $f$ and $g$ with identical divisors.
Dividing $f/g$ we obtain a function with no poles (or zeros),
a holomorphic function.  By
Theorem \ref{holomorphic functions on compact manifolds are constant},
$f/g$ must be constant.

\endtheorem

Related: Hartshorne Corollary 6.10. A principle divisor on a complete
nonsingular curve has degree zero.

Given the importance of an algebraic function's principal parts, we will
now develop tools to calculate them.

\vfill\eject

\mysection{Puiseux Expansions}

The previous section showed that, in complex projective space,
a covering surface can be constructed for an algebraic curve,
which is isomorphic to the curve except at singularities
(where additional points must be introduced), such that:
\begin{itemize}
\item the covering surface is a complex manifold (a {\it Riemann surface}),
\item the curve's rational functions are meromorphic functions
on that manifold, and
\item the rational functions admit series expansions at
every point of the manifold.
\end{itemize}

Our next task is to compute those series expansions.

Since the rational functions in the curve's function field are
formed as rational functions in $x$ and $y$ (or whatever our
variables are named), our primary goal is to compute
series expansions for $x$ and $y$ at arbitrary points on the curve.  With such
expansions in hand, it is straightforward to construct expansions for
any algebraic function, simply by substituting in the $x$ and $y$ expansions.

% Given an algebraic function on an algebraic curve, we wish to compute
% its principal parts by locating its poles and computing series
% expansions there.

At any point where the discriminant is non-zero and $y$ obtains a
finite value, a series expansion for $y$ exists as a power series in
$(x-\alpha)$, which, as we have seen, is a straightforward application
of the Implicit Function Theorem.  At these {\it ordinary} points, we
need only postulate a Taylor series for $y$ in powers of $(x-\alpha)$,
substitute this into the curve's defining polynomial, and equate like
powers to obtain a set of equations to be solved simultaneously.
Multiple solutions will typically be found, corresponding to multiple
branches of the curve.

% EXAMPLE

At ramification points, the series expansion exists in terms
of fractional powers of $(x-\alpha)$, where the denominator
of the fractions is the ramification index.  Issac Newton,
in 1676, first proposed a method of computing the ramification index
using what are now called {\it Newton polygons}.

% Uniformizing elements.  Show that they exist, and uniquely
% define the order of a function, and the residue of a differential.

% Singular points will admit multiple Puiseux series, each one
% corresponding to a single cycle.

Let's assume that we're expanding around the point $(0,0)$, as this
simplifies the analysis with no loss of generality.  Consider
factoring the defining polynomial of the algebraic curve:

$$p_n y^n + p_{n-1}y^{n-1} + \cdots + p_0 = (y-r_1)(y-r_2)\cdots(y-r_n)$$

How might we do this, if the polynomial is irreducible?  We need to
extend to a larger field where the polynomial's roots exist.  The
analysis above shows that Puiseux series form a suitable extension.

For each root $r_i$, define its {\it order} as the lowest power of $t$
that appears in its Puiseux expansion, divided by its ramification
index.  Multiplying factors together adds their orders, so
$p_0$'s order will be the sum of all of the $r_i$'s orders.

% $p_i$ is a sum of terms, each term with $y^i$ and $n-i$ of the $r$'s
% multiplied together.  The term with the lowest order will be formed by
% multiplying the $n-i$ lowest order roots, but there may be
% cancellation between multiple sets of $n-i$ such roots.


Now let's consider increasing $i$ by one.  How does $p_0$'s order
change?  $p_1$ is formed by adding together all products of $n-1$
roots, so $p_1$'s order will be lower than $p_0$'s order by the
largest of $r_i$'s orders, unless there are multiple $r_i$'s with the
same order.  In this case, cancellation between these multiple terms
could result in $p_1$ having a larger order than otherwise expected.

If there are $j$ $r_i$'s with the same largest order, increasing $i$
by $j$ will lower $p_i$'s order by $j$ times that largest order.

The Newton polygon is formed by plotting the orders of the $p_i$
coefficients, with $i$ varying along the horizonal axis and the order
plotted vertically.  The easiest way to do this is to plot the powers
of the monomials that appear in the equation, and construct the
polygon's lower convex hull.

Thus, a segment on the lower convex hull of the Newton polygon will
correspond to as many solutions as the width of the line segment, each
with order equal to the change in height divided by the width, i.e,
the negative slope of the line segment.  The denominator of the slope
will be the ramification index, and the numerator of the slope will be
the lowest exponent expected in the expansion of $y$.

Consider a Puiseux series
corresponding to a single line segment of the Newton polygon.
Letting
$\alpha$ be the $x$ exponent and $\beta$ be the $y$ exponent, so the
monomials in $f$ have the form $x^\alpha y^\beta$, then the equation
of the line segment is $r\alpha + s\beta = p$, where $r$, $s$, and $p$
are integers and $r$ and $s$ are relatively prime.  Making the
substitution $x=t^r$ and $y=t^s u(t)$, we obtain:

$$f(x,y) = \sum A_{\alpha\beta} x^\alpha y^\beta$$
$$ = \sum A_{\alpha\beta} t^{r \alpha} t^{s \beta} u(t)^\beta$$
$$ = t^p \underbrace{\sum A_{\alpha\beta} t^{r \alpha + s \beta - p} u(t)^\beta}_{g(t,u)}$$

At least two of the $(r \alpha + s \beta - p)$ exponents will be zero
(those monomials corresponding to the endpoints of the line segment on
the Newton polygon); all of the remaining exponents will be positive.
This means that if we expand $u(t)$ in a power series in $t$:

$$u(t) = u_0 + u_1 t + u_2 t^2 + u_3 t^3 + \cdots$$

then any power of $u(t)$ will have the form:

$$u(t)^\beta = U_0(u_0) + U_1(u_0, u_1) t + U_2(u_0, u_1, u_2) t^2 + U_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

and $g(t,u)$ will also have the form:

$$g(t,u) = G_0(u_0) + G_1(u_0, u_1) t + G_2(u_0, u_1, u_2) t^2 + G_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

In order for $g(t,u)=0$ at $t=0$, $G_0(u_0)$ must be zero, and since $G_0(u_0)$ is a polynomial
in $u_0$, this gives us a finite number of values for $u_0$ that can solve our equation.

Now, by setting $g(t,u)=0$, can we obtain $u(t)$ as a function of $t$?

The Implicit Function Theorem states that we can, if $\frac{\delta g}{\delta u}$ is not zero
at the point we wish to expand around.

$$\frac{\delta g}{\delta u}(0,u_0) = \frac{\delta}{\delta u} G_0(u_0)$$

In short, the roots of $G_0(u_0)$ give us the starting values for our series expansion,
and if the roots are simple, then the Implicit Function Theorem guarantees that we'll
have a unique series expansion for $u(t)$ as a function of $t$.  If any of the roots
are not simple, then we can repeat this procedure for $g(t,u)$.  It can be shown
(\cite{bliss} \S 15) that this procedure always terminates.

\vfill\eject

\begin{sagecode}
from sage.geometry.newton_polygon import NewtonPolygon
from sage.geometry.polyhedron.constructor import Polyhedron

def newton_polygon(f, x, y, x0, y0):
    vertices = [(b,a) for a,b in f.subs({x:x-x0, y:y-y0}).exponents()]
    vertices.sort(key=lambda a: (a[0], -a[1]))
    polyhedron = Polyhedron(base_ring=QQ, vertices=vertices)
    return NewtonPolygon(polyhedron)

R.<x,y> = QQ[];
\end{sagecode}

\example\label{y^2 = 1 - x^2}
Construct Puiseux expansions of $y$ at the multiple points of the
curve $y^2 = 1 - x^2$

We normalize the defining polynomial by writing it as
$y^2 + x^2 - 1 = 0$.  Where does it have multiple points?
We compute the discriminant:

\begin{sageblock}
R.<x,y> = QQbar[];
(y^2 + x^2 - 1).discriminant(y).factor()
\end{sageblock}

The multiple points of $y^2 = 1 - x^2$ lie at the roots of the
discriminant, which are $x = \pm 1$.  In both cases, $y=0$ is the only
solution, so the curve has multiple points at $(x,y)=(\pm 1, 0)$.  The
partial derivative of the defining polynomial with respect to $x$
is $2x$, which is non-zero, so neither of these multiple
points are singular; we'll get ramification instead.
The analysis is almost the same in both cases, so I'll just do $(1,0)$.

First, construction of the Newton polygon requires recasting the
curve's polynomial into a form centered about the point being
analyzed, i.e, $y^2 + (x-1)^2 + 2(x-1) = 0$.  Next, we construct the Newton
polygon by plotting the monomial powers, putting the $y$ exponents on the horizontal axis and the
$(x-1)$ exponents on the vertical:

\begin{figure}[H]
\begin{center}
\begin{sagecode}
f = y^2+x^2-1;
x0 = 1;
y0 = 0;
np_vertices = [(b,a) for a,b in f.subs({x:x-x0, y:y-y0}).exponents()];
np_vertices.sort(key=lambda a: (a[0], -a[1]));
np = NewtonPolygon(np_vertices);
#np = newton_polygon(f, x, y, x0, y0)
p1 = plot(np, thickness=2, aspect_ratio=1, frame=True);
p2 = points(np_vertices, size=50);
pdf(p1+p2, axes=False, gridlines=[[], []], xmax=3, ymax=3, ticks=[[0,1,2,3],[0,1,2,3]])
\end{sagecode}
\end{center}
\end{figure}

The only segment on the Newton polygon's lower convex hull has slope
$-1/2$ and width 2, telling us that two of our roots (the width of the
segment) will require a single Puiseux series with ramification index
2 (the denominator of the slope):

$$x=t^2+1$$

We know that y can be expressed as a power series in $t$ with
initial exponent 1 (the numerator of the slope):

$$y= a_1 t + a_2 t^2 + a_3 t^3 + \cdots$$

Now, substituting these expressions for $x^2$ and $y^2$ into the
curve's defining equation $y^2 + x^2 - 1 = 0$ and setting all
coefficients of $t$ to zero, we find:

\begin{sageblock}
var('x, y, t, a1, a2, a3');
f = y^2 + x^2 - 1;
exp = f.subs({x: t^2+1,
              y: a1*t + a2*t^2 + a3*t^3});
exp.collect(t)
\end{sageblock}

$$2 + a_1^2 = 0 \qquad 2 a_1 a_2 = 0 \qquad 1 + 2 a_1 a_3 + a_2^2 = 0$$

The first equation tells us that $a_1 = \pm\sqrt{2}i$,
the second equation tells us that $a_2=0$ and the
third equation tells us that $a_3 = \pm \frac{\sqrt{2}}{4} i$, so

% $$a_0=0; \qquad a_1 = \pm\sqrt{2}i; \qquad a_2 = 0; \qquad a_3 = \pm \frac{\sqrt{2}}{4} i$$

\begin{equation}
\label{(1,0) expansion}
x = t^2 +1; \qquad y = \pm\left[ \sqrt{2}it + \frac{\sqrt{2}}{4} it^3 + \cdots \right]
\end{equation}

It would seem that we have two different series to chose from.  This
is not really the case, as they differ by only a $180^\circ$ rotation
in the t-plane, as can been seen by substituting $t=-t$, which
transforms one of the y-series into the other, while leaving the
x-series unchanged.


Now, let's analyze the point at infinity.  We move infinity to a
finite point (0) with the substitution $x=u^{-1}$, then combine all of
our terms over a common denominator and discard the denominator.  Our
curve becomes:

$$y^2 u^2 + 1 - u^2 = 0$$

\begin{figure}[H]
\begin{center}
\begin{sagecode}
R.<u,v> = QQ[];
f = v^2 * u^2 + 1 - u^2;
u0 = 0;
v0 = 0;
np_vertices = [(b,a) for a,b in f.subs({u:u-u0, v:v-v0}).exponents()];
np = NewtonPolygon(np_vertices);
p1 = plot(np, thickness=2, aspect_ratio=1, frame=True);
p2 = points(np_vertices, size=50);
pdf(p1+p2, axes=False, gridlines=[[], []], xmax=3, ymax=3, ticks=[[0,1,2,3],[0,1,2,3]])
\end{sagecode}
\end{center}
\end{figure}

The Newton polygon's lower convex hull has a single line segment,
slope $1$, length $2$, telling us that we'll have two separate
poles, each with ramification index 1.  Thus, $u$ can be used
directly as a uniformizing variable, and we postulate an expansion for
$y$ in the form:

$$y = a_{-1} \frac{1}{u} + a_0 + a_1 u + a_2 u^2 + a_3 u^3 + \cdots$$

Plugging this into $y^2 u^2 + 1 - u^2$ and setting all the resulting
coefficients to zero, we conclude:

\begin{sageblock}
var('u, y, t, a0, a1, a2, a3');
var('an1', latex_name='a_{-1}');
f = y^2*u^2 + 1 - u^2
exp = f.subs({y: an1*(1/u) + a0 + a1*u + a2*u^2 + a3*u^3});
exp.collect(u)
\end{sageblock}

%% $$a_{-1}^2 + 1 = 0; \qquad 2 a_{-1} a_0 = 0 \qquad (2a_{-1}a_1+a_0^2-1)=0$$

$$a_{-1} = \pm i; \qquad a_0 = 0; \qquad a_1 = \mp \frac{1}{2}i; \qquad a_2 = 0; \qquad a_3 = \mp \frac{1}{8}i$$

$$y = \pm i \frac{1}{u} \mp \frac{1}{2} i u \mp \frac{1}{8} i u^3 + \cdots$$

This time, there is no ramification, since $u$, and not a power of
$u$, is $\frac{1}{x}$.  We actually have two distinct series that will
yield two different values of $y$ for each value of $u$.
Geometrically, we have two sheets that approach each other and touch
at a singular point where the curve is not locally Euclidean.

\endexample

\vfill\eject
\mysection{Orders and Places}

Driven in no small part by the difficulty in visualizing higher dimensional geometric shapes,
mathematicians have developed increasingly algebraic techniques to manipulate geometric objects.
In particular, the techniques of the previous section were presented largely for educational purposes,
as they are now considered obsolete.
The current state of the art is to use the tools of abstract algebra developed in the early
twentieth century, such as rings, ideals, and fields.

We can't use ideals directly in a function field, or any field for that matter, because
there are only two ideals in any field -- the zero ideal and the entire field.  This
follows directly from the invertability of field elements.  As all field element possess
inverses, any non-zero ideal generator can be multiplied by its multiplicative inverse
to generate 1, which then generates the entire field.

To solve this problem, we decompose the function field into maximal orders, of which
there are two of primary interest: the maximal order and the infinite order.
The ideals of the maximal order correspond to
finite places, and the ideals of the infinite order correspond to infinite places.
Using one or the other, we obtain ideals that represent all places in the function field.

%\renewcommand{\theexample}{\ref{y^2 = 1 - x^2} cont}
\newtheorem*{examplecont}{Example \ref{y^2 = 1 - x^2} cont}
\begin{examplecont}
\begin{quote}\rm
%\hfil\break
Construct Puiseux expansions of $y$ at the multiple points of the
curve $y^2 = 1 - x^2$

Sage has built-in routines to compute Puiseux expansions without having to
construct Newton polygons and substitute trial expansions, although a
different syntax is required.  First we create a function field
in one variable with coefficients in $\QQbar$:

\begin{sageblock}[ch7]
R.<x> = FunctionField(QQbar)
\end{sageblock}

Next we create a ring of polynomials in $y$ with coefficients in the function field,
which allows us to write the minimal polynomial of the algebraic curve.  We then
create a new function field that is an extension of the rational function field:

\begin{sageblock}[ch7]
L.<y> = R[]
F.<y> = R.extension(y^2 + x^2 - 1)
\end{sageblock}

The function fields have two maximal orders (DEFINE), one finite and the other infinite.
Since we wish to construct our series expansion at a point with finite coordinates
(remember that this is projective space, so we also have points at infinity),
we use the finite maximal order, construct the ideal corresponding the desired
point, then construct the unique place (DEFINE) at that point:

\begin{sageblock}[ch7]
O = F.maximal_order()
pl = O.ideal(x-1, y).place()
\end{sageblock}

Finally, we construct the completion map at the place, which maps from the function
field to a Laurent series, specified the desired precision of the expansion,
and apply the map to the function field element ($y$) that we wish to expand:

\begin{sageblock}[ch7]
F.completion(pl, prec=4)(y)
\end{sageblock}

This answer differs from the one we computed by hand because the
choice of uniformizing variable is not unique, and because the
computer made a different choice than we did.  Our ``$t$'' variable
was roughly $\sqrt{x-1}$.  Even though square roots don't exist {\it
per se} in this field, we can construct a series expansion of $x-1$,
construct the square root of the series expansion as another series expansion.
This only works if the series for $x-1$ has valuation 2, which it does:

\begin{sageblock}[ch7]
xminus1 = F.completion(pl, prec=4)(x-1)
t = xminus1^(1/2)
\end{sageblock}

This series gives us $\sqrt{x-1}$ in powers of $s$.
We reverse the series, which only works if the series has valuation 1,
giving us a new series that expresses $s$ in powers of $\sqrt{x-1}$.
We substitute the reversed series into the original $s$-expansion of $y$,
which gives us an expansion of $y$ in powers of $\sqrt{x-1}$:

\begin{sageblock}[ch7]
F.completion(pl, prec=4)(y)(t.reverse())
\end{sageblock}

Comparing this to equation \ref{(1,0) expansion}, we see that they're the same.

Now let us turn to the point at infinity.  We begin by constructing the
maximal infinite order of the underlying rational function field $C(x)$,
because its structure is quite simple and we know that it will only have a single point,
and a single place, at infinity.

\begin{sageblock}[ch7]
Rinf = R.maximal_order_infinite().ideal(1/x).place()
\end{sageblock}

Sage function fields come equipped with a {\tt places\_above} method that
allow us to obtain a list of all places in an extension field that lie
over a given place in the underlying function field.  Having obtained
these places, represented as ideals in the maximal infinite order,
it is straightforward to use the {\tt completion} method, as before,
to construct Puiseux series.

\begin{sageblock}[ch7]
Pinf = F.places_above(Rinf)
[F.completion(pl, prec=6)(y) for pl in Pinf]
\end{sageblock}

\begin{sageblock}[ch7]
[F.completion(pl, prec=6)(1/x) for pl in Pinf]
\end{sageblock}

\hfill$\Box$\end{quote}
\end{examplecont}

%\renewcommand{\theexample}{\thesection.\arabic{example}}

\example \cite{bliss} \S 68
Compute expansions at all multiple points of

$$y^3+x^3y+x=0$$

We begin by computing the discriminant of the
equation, which gives us the locations of the multiple points.

\begin{sageblock}
R.<x,y> = QQbar[];
f = y^3 + x^3*y + x
f.discriminant(y).factor()
\end{sageblock}

That result is rather confusing.  Let's try factoring over $\QQ$
instead of $\QQbar$:

\begin{sageblock}
f.discriminant(y).change_ring(QQ).factor()
\end{sageblock}

The multiple points lie over the roots of this equation: $x=0$ and
the seven roots of $4x^7+27=0$.  Infinity also needs to be
examined.  We begin with $x=0$:

\begin{sageblock}[ch7-2]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^3 + x^3*y + x)
O = F.maximal_order()
O.ideal(x).factor()
pl = O.ideal(x, y).place()
xseries = F.completion(pl, prec=8)(x)
yseries = F.completion(pl, prec=10)(y)
yseries((xseries^(1/3)).reverse())
\end{sageblock}

This result shows that we have a single cycle at $(x,y)=(0,0)$ with
three sheets.  Now, let's look at a specimen root
of $4x^7+27=0$:

%% This was from my old Maxima puiseux routine
\begin{comment}
puiseux(y^3 + x^3*y +x, x, y, g, -3/(2*g^2), 1);
puiseux(y^3 + x^3*y +x, x, y, g, 3/g^2, 1);
puiseux(y^3 + x^3*y +x, x, y, g, -(3/8)^(1/7), 1);
\end{comment}

\begin{sageblock}[ch7-2]
g = QQbar(-27/4)^(1/7)
pl = O.ideal(x-g, y+3/(2*g^2)).place()
xseries = F.completion(pl, prec=3)(x)
yseries = F.completion(pl, prec=2)(y)
# This is the algebraic number that takes forever to print
# yseries(((xseries-g)^(1/2)).reverse())
pl = O.ideal(x-g, y-3/g^2).place()
xseries = F.completion(pl, prec=3)(x)
yseries = F.completion(pl, prec=2)(y)
yseries((xseries-g).reverse())
\end{sageblock}

\begin{sageblock}[ch7-3]
R1.<g> = QQ[]
S.<g> = NumberField(4*g^7+27)

R.<x> = FunctionField(S)
L.<y> = R[]
F.<y> = R.extension(y^3 + x^3*y + x)

O = F.maximal_order()

O.ideal(x-g).factor()

pl = O.ideal(x-g, y+3/(2*g^2)).place()
xseries = F.completion(pl, prec=3)(x)
yseries = F.completion(pl, prec=2)(y)
xseries((yseries+3/(2*g^2)).reverse())

pl = O.ideal(x-g, y-3/g^2).place()
xseries = F.completion(pl, prec=2)(x)
yseries = F.completion(pl, prec=2)(y)
yseries((xseries-g).reverse())
\end{sageblock}

We have one sheet of two cycles at $(g,-3/(2g^2))$
and an ordinary point at $(g,3/g^2)$.

Finally, let's look at what happens when $x$ goes to infinity:

\begin{sageblock}[ch7-2]
Rinf = R.maximal_order_infinite().ideal(1/x).place()
Pinf = F.places_above(Rinf)
xseries = F.completion(Pinf[0], prec=3)(x)
yseries = F.completion(Pinf[0], prec=10)(y)

xseries = F.completion(Pinf[1], prec=3)(x)
yseries = F.completion(Pinf[1], prec=10)(y)
\end{sageblock}

Here we have an ordinary point at $(\infty,0)$ and
a single cycle of two sheets at $(\infty,\infty)$.

We have examined all of this curve's ramification points,
including those at infinity (since we analyzed all of its
points at infinity), and found that all of them admitted
a single Puiseux expansion.

Therefore, this curve is {\it non-singular}, and according to the
genus-degree formula (MORE INFO), its geometric and arithmetic genus
are the same.  Its arithmetic genus is $\frac{1}{2}(d-1)(d-2) = 3$,
where $d=4$ is the degree of the defining polynomial.  Computing
the geometric genus is more difficult\footnote{
{\tt https://www.singular.uni-kl.de/Overview/Examples/Genus/genus1.html}

{\tt https://en.wikipedia.org/wiki/Algebraic_curve\#Classification_of_singularities}

{\tt https://math.stackexchange.com/questions/150840}

{\tt http://mathforum.org/library/drmath/view/71229.html}
}, but we can verify our
information with Sage, being careful to work in {\it projective} space:

\begin{sageblock}
PP.<x,y,z> = ProjectiveSpace(QQ, 2)
C = Curve(y^3*z + x^3*y + x*z^3)
C.is_singular()
C.arithmetic_genus()
C.geometric_genus()
\end{sageblock}


\endexample

%\mysection{meromorphic functions are analytic}
%
%first, trace of a meromorphic function is meromorphic on C(x), and is
%thus a rational function
%
%Liouville's theorem: a bounded entire function is constant
%
%Proof A: (Silverman) use a Taylor series expansion around z=0, which
%is valid in the entire plane (since the function is entire).  Cauchy's
%inequality $|f| \le M ==> |c_n| \le M/{R^n}$ (eq. 10.8') as R->infty
%implies that the function is constant.
%
%Lemma: A entire function with no singularities, even at infinity, is
%constant.
%
%Proof: We can do a Taylor series expansion at the origin, whose
%non-zero terms will correspond to the principal part of the expansion
%at infinity, which must therefore be zero.
%
%Next: A entire function with only a pole at infinity is a polynomial.
%The principal part at infinity will be a polynomial.  Subtract it out
%to get a function with no singularities, which must be constant.
%
%Next: Given a function with only a finite number of finite poles,
%multiply it by a polynomial (the denominator) matching the poles with
%zeros.  Now we've got a function with only a pole at infinity, which
%must be a polynomial (the numerator).

\example Find the principal parts of $\frac{1}{y}$ on the curve
$y^2 = 1 - x^2$

The {\it principal part} of an algebraic function is the part
of its series expansion with negative exponents.  Theorem
\ref{algebraic functions are characterized by their principal parts}
states that an algebraic function is completely determined,
up to adding a constant, by its principal parts.

The first step is to locate the function's poles, which in this case is
simply the places where the denominator is zero, and that's just
$x=\pm 1$.  Now, if we use {\tt puiseux}, we can just request a series
truncated at the $-1$ term:

\begin{sageblock}[ch7-4]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 + x^2 - 1)
D = (1/y).divisor()
# there's nothing in Sage function field code to set absolute precision, unfortunately
table([[p, F.completion(p, prec=1)(1/y)] \
    for p,m in D.list() if m < 0])
\end{sageblock}

\endexample

\example Find the principal parts of $\frac{x}{y} \ud x$ on the curve
$y^2 = 1 - x^2$

Differential forms are not functions, and have different series
expansions, due to the presence of the
differential, which must be adjusted at ramification points.

Let's expand $\frac{x}{y}$ at $x=1$:

\begin{sageblock}[ch7-5]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 + x^2 - 1)
O = F.maximal_order()
pl = O.ideal(x-1, y).place()
F.completion(pl, prec=6)(x)
F.completion(pl, prec=6)(x/y)
\end{sageblock}

Now $x=t^2+1$, so $\ud x=2t\ud t$.  Thus, multiplying $\frac{x}{y}$
by $\ud x$ and changing our variable to $t$ will multiply
all of the terms in our expansion by $2t$:

\begin{sageblock}[ch7-5]
# Completion of a differential is not implemented in production Sage code
# F.completion(pl, prec=6)(x/y*x.differential())
dx = F.completion(pl, prec=6)(x).derivative()
F.completion(pl, prec=6)(x/y) * dx
\end{sageblock}

Even though $\frac{x}{y}$ has a pole
at $x=1$, $\frac{x}{y} \ud x$ does not!

Its behavior at infinity also requires analysis.

\begin{sageblock}[ch7-5]
# this is here to manually introduce i into our number field
F.maximal_order().ideal(x - sqrt(QQbar(-1)));

Rinf = R.maximal_order_infinite().ideal(1/x).place()
Pinf = F.places_above(Rinf)

F.completion(Pinf[0], prec=2)(x)
F.completion(Pinf[0], prec=2)(y)
F.completion(Pinf[0], prec=2)(x/y)
dxinf = F.completion(Pinf[0], prec=2)(x).derivative()
F.completion(Pinf[0], prec=2)(x/y) * dxinf
\end{sageblock}

$\frac{x}{y}$ has no poles at infinity, and approaches
the limiting values $\pm i$ as $x$ and $y$ approach
infinity.  The differential $\frac{x}{y} \ud x$,
on the other hand, requires us to multiply by $\ud x$,
and since $x=\frac{1}{t}$, $\ud x = - \frac{1}{t^2} \ud t$.

In short, while $\frac{x}{y}$ has poles only at $(\pm 1,0)$,
$\frac{x}{y} \ud x$ has poles only at infinity.

\begin{sageblock}[ch7-5]
# F(x) and not x because otherwise we get a divisor in the underlying rational function field
D = (x/y).divisor() + F(x).differential().divisor()
table([[p, F.completion(p, prec=2)(x/y) * F.completion(p)(x).derivative()] for p,m in D.list() if m < 0])
\end{sageblock}

\endexample

\begin{comment}

\example
Compute expansions at all multiple points of
the exercises in \cite{bliss} \S 68.

To facilitate this example, let's define an
auxiliary function to perform the analysis:

\begin{sageblocksmall}
def analyze_multiple_points(f, pr=False):
   P = f.parent()
   Base = P.base_ring()
   x,y = P.gens()
   # This next code is here to avoid Trac #25271, though it assumes
   # that the curve's polynomial has only rational coefficients.
   # (which is true for all of our test cases)
   if False:
      disc = f.discriminant(y)
   else:
      QQR = QQ[x,y]
      disc = P(QQR(f).discriminant(QQR(y)))
   if pr: print '$$', latex(disc.factor()), '$$'

   for x0 in Base[x](disc).roots(multiplicities=False):
      sheets = 0
      for y0 in Base[y](f.subs({x: x0})).roots(multiplicities=False):
          p = puiseux(f, x0, y0, 3)
          for cycle in p:
             assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
             if cycle[0] == x0:
                sheets = sheets + 1
             else:
                sheets = sheets + (cycle[0] - x0).valuation()
          if pr: print '$$', latex((x0,y0)), '$$'
          if pr: print '$$', latex(p), '$$'
      if f.subs({x: x0}).degree(y) != f.degree(y):
          p = puiseux(f, x0, oo, 3)
          for cycle in p:
             assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
             if cycle[0] == x0:
                sheets = sheets + 1
             else:
                sheets = sheets + (cycle[0] - x0).valuation()
          if pr: print '$$', latex((x0,oo)), '$$'
          if pr: print '$$', latex(p), '$$'
      assert(sheets == f.degree(y))

   finf = Base[y](f.subs({x: 1/x}).numerator().subs({x:0}))
   for y0 in finf.roots(multiplicities=False):
      p = puiseux(f, oo, y0, 3)
      for cycle in p:
         assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
      if pr: print '$$', latex((oo,y0)), '$$'
      if pr: print '$$', latex(p), '$$'
   if finf.degree() < f.degree(y):
      p = puiseux(f, oo, oo, 3)
      for cycle in p:
         assert(f.subs({x : cycle[0], y : cycle[1]}) == 0)
      if pr: print '$$', latex((oo,oo)), '$$'
      if pr: print '$$', latex(p), '$$'
\end{sageblocksmall}

\begin{sageblock}
test_curves = [
    y^3 - 3*y + 2*x,
    y^3 + 3*y - x,
    y^3 - 3*y^2 - x,
    y^4 - 4*y - x,
    y^4+2*(1-2*x)*y^2+1,
    y^3-3*y^2+x^6,
    y^3-3*y+2*x^2*(2-x^2),
    y^3-3*y+2*x^3*(2-x^3),
    3*x*(x-1)*y^4 -4*(x-1)*(x-2)*y^3 + (4/27)*(x-2)^4,
    y^5 + (x^2-1)*y^4 - (4^4)/(5^5)*x^2*(x^2-1),
    y^3 - x*y - x^2,
    y^3 - 3*x^2*y + 2*x,
    y^3 - 3*x*y + 2*x^2,
    y^3 - 3*y + x^6];

# for f in test_curves:
#     analyze_multiple_points(f)
\end{sageblock}

%\begin{sageblocksmall}
%analyze_multiple_points(y^3-3*y+2*x^3*(2-x^3), True);
%\end{sageblocksmall}

$$y^3 - 3axy + x^3$$

This function contains an extra variable and is, in fact, a family of
algebraic curves.

\endexample

\end{comment}

% A {\it Riemann surface}, more precisely, is a two-dimensional manifold
% with a {\it complex analytic structure}.

\vfill\eject
\mysection{Places and Valuations}

We've seen how to construct Puiseux expansions at arbitrary points of
an algebraic curve, but some points have multiple expansions,
corresponding to multiple cycles on their corresponding surfaces.

We now seek some mechanism for distinguishing between multiple cycles
at a single point.  To this end, we introduce the concept of a {\it
place}.  Intuitively speaking, a place is a cycle, and places are in
one-to-one correspondence with Puiseux expansions.  Therefore, we
can handle singularities by thinking in terms of places.
Non-singular points have a unique place associated
with them, while there are multiple places (and multiple cycles)
associated with a singular point.

We can characterize places algebraically, both to formalize this
concept and also to analyze them in a manner that will generalize
in Chapter 10 to arbitrary algebraic extensions.

While points correspond to ideals of the curve's coordinate ring,
places correspond to ideals of maximal orders of the curve's
function field.  These maximal orders can be constructed
using the algebraic process of {\it normalization}.

To establish this connection, we introduce the concept of a
{\it valuation ring}.

Consider, for example the lemniscate of Bernoulli, defined by the equation

$$ (x^2+y^2)^2 - (x^2-y^2) = 0$$

\begin{sageblock}[lemniscate]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension((x^2+y^2)^2 - (x^2-y^2))

O = F.maximal_order()
I = O.ideal(x,y)
I.factor()
\end{sageblock}

\begin{sageblock}[lemniscate]
O.basis()
\end{sageblock}

One characterization of the maximal finite order $O$ is that it contains
all functions with no poles at finite places.  The first three elements
in the basis are obvious, but why is the fourth so complicated?
Isn't $\frac{y}{x}$ in $O$?

\begin{sageblock}[lemniscate]
y/x in O
\end{sageblock}

Doesn't $\frac{y}{x}$ approach either $1$ or $-1$ as it approaches the
origin?  Remember that we're working in complex space.  We've got
four roots, not two.  Let's look at some examples of limiting values
using some numerical examples:

\begin{sageblock}[lemniscate]
set_verbose(-1)
R.<u,v> = CC[]
ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.0001))
ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.0001)).variety()
[d[v] for d in ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.0001)).variety()]
[d[v]/.0001 for d in ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.0001)).variety()]
[d[v]/.00001 for d in ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.00001)).variety()]
[(d[v]^3+d[v])/.00001 for d in ideal(((u^2+v^2)^2 - (u^2-v^2)), (u-.00001)).variety()]
\end{sageblock}

\begin{sageblock}[lemniscate]
Fs = O.ideal(x).factor()
F.completion(Fs[0][0].place(), prec=7)(y)
F.completion(Fs[1][0].place(), prec=7)(y)
F.completion(Fs[2][0].place(), prec=7)(y)
F.completion(Fs[3][0].place(), prec=7)(y)
\end{sageblock}

How does one of these ideal generators factor?

\begin{sageblock}[lemniscate]
A.<a> = QQbar[]
(1/2*a^2+1/2*sqrt(QQbar(-1))*a+1).factor()
\end{sageblock}


[SwansonHuneke] Proposition 6.8.14 Let $R$ be an integral domain. Then
the integral closure of the ring $R$ equals $\cap_V V$ , where $V$
varies over all the valuation domains between $R$ and its field of
fractions. If $R$ is Noetherian, all the $V$ may be taken to be
Noetherian.

[Kollar] Definition 1.23
Let $S$ be an integral domain with quotient field $Q(S)$.
The {\it normalization} of $S$ in $Q(S)$, denoted by $\overline{S}$,
is the unique largest subring $\overline{S} \subset Q(S)$
such that every homomorphism $\phi: S \to R$ to a DVR
extends to a homomorphism $\overline{\phi}: \overline{S} \to R$.

[Kollar] Lemma 1.24
A unique factorization domain is normal.  In particular,
any polynomial ring over a field is normal.

[Kollar] Lemma 1.25.
Assume that $t \in Q(S)$ satisfies a monic equation
with coefficients in $S$ (i.e, $t$ is integral).
Then $t \in \overline{S}$.

[Kollar] Defintion 1.27.
Let $S$ be an integral domain.  The {\it normalization} of $S$
is its integral closure in its quotient field

Kollar: ``The easy argument that every normal integral domain $S$
is the intersection of all the valuation rings sitting between
it and its quotient field is given in [AM69, 5.22].  Working
only with discrete valuation rings is a bit harder.  The strongest
theorem in this direction is Serre's condition for normality;
see [Mat70, 17.1] or [Mat89, 23.8].''

[Mat89]:

$R_i$ condition: $A_p$ is regular for all $P \in {\rm Spec } A$ with ${\rm ht } P \le i$

$S_i$ condition: depth $A_p \ge {\rm min}({\rm ht } P, i)$ for all $P \in {\rm Spec } A$

$(S_0)$ always holds. $(S_1)$ says that all the associated primes of $A$ are minimal.
$(R_0)+(S_1)$ is n.a.s.c. for $A$ to be reduced.

Theorem 23.8: $(R_1)+(S_2)$ are n.a.s.c. for a Notherian ring $A$ to be normal.

A {\bf Dedekind domain} is a integral domain in which all ideals factor into a product of prime ideals.

Theorem: such a factorization is necessarily unique.

[Wiki Dedekind domain] Theorem: Let R be a Dedekind domain with
fraction field K. Let L be a finite degree field extension of K and
denote by S the integral closure of R in L. Then S is itself a
Dedekind domain.

Theorem (Kummer): If $\{1,y,...,y^{n-1}\}$ is a local integral basis
at some prime polynomial $\mathfrak{p}$ in $x$, then we can factor
the ideal $\mathfrak{p}^e$ by factoring the field's defining
polynomial mod $\mathfrak{p}$. [Stichnoch Theorem 3.3.7]

An {\bf Artinian ring} satisfied the descending chain condition on ideals.

Theorem: the modulo ring constructed from a Dedekind domain and a proper ideal is an Artinian ring.

[Wiki Ideal] Theorem: if $f: A \to B$ is surjective and $ \mathfrak {a}\supseteq \ker f$ then:

\begin{itemize}
\item $\mathfrak {a}^{ec}={\mathfrak {a}}$ and ${\mathfrak {b}}^{ce}={\mathfrak {b}}$
\item ${\mathfrak {a}}$ is a prime ideal in $A \Leftrightarrow {\mathfrak  {a}}^{e}$ is a prime ideal in $B$.
\item ${\mathfrak {a}}$ is a maximal ideal in $A \Leftrightarrow  {\mathfrak  {a}}^{e}$ is a maximal ideal in $B$.
\end{itemize}

So, to factor $\mathfrak{p}$, we need to find all of the prime/maximal ideals
in $R \mod \mathfrak{p}$.

Constructing $R \mod \mathfrak{p}$ as a finite dimensional algebra,
we have an algorithm to enumerate all of the algebra's maximal ideals.

We also want to find each ideal's ramification index and relative degree.

The ramification indices are the powers of the prime ideals.  Cohen Theorem 4.8.3:

$$p {\mathbb Z}_K = \prod_{i=1}^{g} {\mathfrak p}_i^{e_i}$$

Stichnoch defines the ramification index as the integer such
that $\nu_{P'}(x) = e \nu_P(x)$ for all $x$ in the base field.
($P'$ lies over $P$)

We can find the ramification index in the modulo ring by raising each maximal
ideal to successive powers and determining when it stabilizes.  The Artinian
condition guarantees that it will eventually stabalize.

The relative (or residual) degree of ${\mathfrak p}$ is defined (Cohen Definition 4.8.4):

$$f_i = [{\mathbb Z}_K/{\mathfrak p}_i : {\mathbb Z}/p{\mathbb Z}]$$

This seems to be the nullity of the matrix that defines the ideal in the algebra.

Why?  The dimension of the algebra (over ${\mathbb Z}/p{\mathbb Z}$)
is the dimension of ${\mathbb Z}_K$.  The number of basis elements in
the ideal is the dimension of ${\mathfrak p}_i$.  The dimension of
${\mathbb Z}_K/{\mathfrak p}_i$ is the difference of these two numbers
(the dimension of the algebra minus the dimension of the ideal), which
is the dimension of the vector space minus the dimension of the
ideal's basis matrix's kernel, or the nullity of this matrix.

\vfill\eject
\mysection{Normalization}

Given: a rational function $f$ and a prime ideal $I$ in a maximal order $O$.

Goal: compute the value of $f \bmod I$.

Step 0: Precompute a $k[x]$-module basis for $I$ in Hermite normal form (doable since $I$ is finitely generated over $O$ and $O$ is finitely generated over $k[x]$) and $\alpha$, a rational function with a simple pole at the place corresponding to $I$ and no other finite poles.

Step 1: Compute the valuation of $f$'s denominator; call it $\nu$.  Multiply both $f$'s numerator and denominator by $\alpha^\nu$.  Now both the numerator and denominator are in $O$, and the denominator is not in $I$ (i.e, it has a finite value).  So we've reduced to the case of computing the residue of an element of $O$, as we now can divide by the denominator's residue, since we know it's not zero.

Step 2: Find a primitive element of the residue field; call it $g$.

Step 3: Compute $g$'s minimal polynomial and use it to construct the residue field as an algebraic extension of the constant base field.

Reducing an element $f \in O$ modulo the HNF basis of $I$ gives a $k[x]$-vector that represents $f \bmod I$, i.e, when this $k[x]$-vector is multiplied by the HNF basis vector, we obtain an element of $O$ equivalent (mod $I$) to $f$.

We can represent $g$ and its powers in this form, and construct a matrix that converts from $g$-basis to HNF basis, then invert it to obtain a matrix that converts from HNF basis to $g$-basis.

Use reduction mod $I$'s HNF basis, then multiply by this inverse matrix to obtain an element in the residue field.



\vfill\eject
\mysection{Divisors}

Given a function on an algebraic curve, we can ask at which places it
has poles and zeros.  The location and strengths of a function's
poles and zeros are called its {\it divisor}.

For non-singular curves, the points and places are in one-to-one
correspondence, and a function's divisor can be described in terms
of the points where its poles and zeros lie.

Thus, one way of defining a divisor is to associate integers (positive
for zeros, negative for poles) with each point of the curve, subject
to the stipulation that all but a finite number of those integers is
zero.  Such a description is called a {\it Weil divisor}, and is most
suitable for working in {\it intersection theory}.

For singular curves, the situation is more complicated.  A divisor
needs to be associated with places, not points.  Such a divisor is
called a {\it Cartier divisor}, and is more suitable for our purposes.

\vfill\eject
\mysection{Riemann-Roch spaces}

A {\it Riemann-Roch space} is a subspace of an algebraic curve's
function field characterized by specifying a minimum order that the
function must obtain at all of the curve's points.  Aside from having
great theoretical significance, Riemann-Roch spaces are practically
useful because they are finite dimensional, and algorithms exist for
constructing Riemann-Roch bases.  Finding a basis for a Riemman-Roch
space in a crucial first step in solving a Mittag-Leffler problem.

Numerous algorithms have been developed for computing bases of
Riemann-Roch spaces.  Sage uses an implementation of Hess's algorithm
from \cite{hess}.

\begin{comment}
I've implemented in Maxima one of the oldest,
from \cite{bliss}, though it probably dates back
to \cite{dedekind-weber}.

We begin the process with a ${\mathrm C}(x)$-basis for the entire
function field, namely $\{1, y, \ldots, y^{n-1}\}$.

Next, we want to convert this into a ${\mathrm C}[x]$-basis for the
finite portion of the divisor.  First, we multiple the basis by
whatever polynomials in $x$ are required to place the basis elements
into the divisor's function space, then for each value of $x$ form
a matrix of coefficients, and keep reducing until its determinant is zero.

Finally, we need to adjust this basis to match the divisor's requirements at infinity.

A divisor's basis can be transformed to another basis for the same
divisor by multiplying by a matrix in ${\mathrm C}[x]$ with
determinant a constant not equal to zero. (Bliss Th. 21.1)

If we have a cycle at infinity, multiplying by x will multiply
the expansions by (1/t^r).

{\tt riemannroch(f,x,y,divisor)} computes a basis for the Riemann-Roch
space $L(D)$.  {\tt divisor} is a list of elements, each in the form
{\tt [[$x_i$, $y_i$], $\nu_i$]}, where $(x_i, y_i)$ is a point on the
curve, and $\nu_i$ is the order of the divisor at that point.  For
singular points, either the standard syntax can be used, indicating
that the order of the divisor is the same at all points of the
singularity, or $\nu_i$ can be replaced with a list of values, one for
each sheet at the singularity.  The order of sheets is the same
returned by {\tt puiseux}.  Specifying multiple orders at
singularities with cycles is currently not supported.

\end{comment}

Here's a simple example\footnote{From
{\tt https://math.stackexchange.com/questions/294644}}
of a Riemann-Roch space calculation:

\begin{sageblock}[riemannroch]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 - x^3 + x)


O = F.maximal_order()
P = O.ideal(x,y)
D = P.divisor()

D.basis_function_space()
(2*D).basis_function_space()
(3*D).basis_function_space()
(4*D).basis_function_space()
\end{sageblock}

Here are the examples from \cite{alvanos} \S6.3:

\begin{sageblock}[riemannroch]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 - x^3 - 1)

O = F.maximal_order()
P1 = O.ideal(x-2,y-3)
P2 = O.ideal(x-2,y+3)

Rinf = R.maximal_order_infinite().ideal(1/x).place()
Pinf = F.places_above(Rinf)[0]

D1 = P1.divisor()
D2 = P2.divisor()
Dinf = Pinf.divisor()

(Dinf-D1).basis_function_space()
(2*Dinf-D1).basis_function_space()
(3*Dinf-D1).basis_function_space()
(4*Dinf-D1).basis_function_space()
(Dinf).basis_function_space()
(2*Dinf).basis_function_space()
(3*Dinf).basis_function_space()
\end{sageblock}

\vfill\eject
\mysection{Mittag-Leffler Problems}

Theorem \ref{algebraic functions are characterized by their principal parts}
tells us that a rational function on an algebraic
curve is completely characterized, up to an additive constant,
by the principal parts of the Puiseux expansions at its poles.
Note that Theorem \ref{algebraic functions are characterized by their principal parts}
does not guarantee the existence of a function with
specified principal parts.  It only shows that any
two such functions, {\it if they exist}, differ
by at most a constant.

A {\it Mittag-Leffler problem} is the practical application of this
theorem -- given a set of principal parts, find a function that
matches them all, or prove that no such function exists.

The first step in solving a Mittag-Leffler problem is to identify the
maximum strengths of the poles, and construct a basis for a
Riemann-Roch space that includes all functions with poles of such
strength.  We now have a finite basis for a vector space that must
include the function we are looking for.  We construct Puiseux
expansions for the basis functions, and use them to construct
a matrix equation that, when solved, gives the coefficients
needed to form the function we seek from the basis functions.

The input data is a set of principal parts or, alternately, a divisor
combined with a vector of coefficients.

Let's assume that we've got our data in the latter form, so we can run
{\tt riemannroch} on the divisor and obtain a set of basis functions.
Now let's construct a Sage function to extract the principal parts
of the basis functions and form them into a matrix:

\begin{sagecommon}
# the prec=3 is a problem, because we want absolute precision but can't specify it,
# so I just use a high enough precision to work for the problems in the book
def principal_parts_matrix(div, basis):
    F = div.parent().function_field()
    coeffs = [(F.completion(p, prec=3), i) for p,m in div.list() for i in range(-m,0)]
    return matrix([[c[0](b)[c[1]] for c in coeffs] for b in basis]).transpose()
\end{sagecommon}

Given a vector {\tt b} of coefficients, we now want to
solve a matrix equation:

$$m \cdot v = b$$

This will typically be an overspecified system -- a non-square matrix
that may or may not have a solution.  That's fine; since some
integrals have no elementary form, this doesn't represent a limitation
in our theory.  Failure to solve this matrix equation would only show
that no function exists on this curve with the coefficients {\tt b}.

To proceed, we'll use the
{\it Moore-Penrose pseudoinverse}\footnote{\tt https://en.wikipedia.org/wiki/System_of_linear_equations\#Matrix_solution},
which has the property that if a solution exists, it can
be found by multiplying the $b$ vector by the pseudoinverse.

To find out if there actually is a solution, we first compute a trial
solution by multiplying the pseudoinverse by {\tt b}, then checking
to see if the trial solution actual solves the original equation.

\example
Let's say that we've identified a divisor on an algebraic
curve (example \ref{an integral Maxima can't solve}):

We now compute its principal parts matrix:

\begin{sageblock}[riemannroch]
R.<x> = FunctionField(QQbar)
L.<y> = R[]
F.<y> = R.extension(y^2 - x^8 - 1)

# do this to force the extension code to run
y.divisor();

O = F.maximal_order()
Oinf = F.maximal_order_infinite()

Dfinite = add([O.ideal(x-a*QQbar(-1).sqrt(), y-b*QQbar(2).sqrt()).place().divisor() for a in [-1, 1] for b in [-1, 1]])

Rinf = R.maximal_order_infinite().ideal(1/x).place()
Dinf = add([pl.divisor() for pl in F.places_above(Rinf)])

D1 = Dfinite + 2*Dinf

basis = Dfinite.basis_function_space()

D1.basis_function_space()

principal_parts_matrix(D1, basis)
\end{sageblock}

{\bf TODO}

Introduce a sample vector {\tt b} and show how to proceed.

\endexample

\mysection{Parallels with the Transcendental Cases}

At this point, it may seem that we've spent this entire chapter
developing a suite of technical tools that appear completely different
from everything that came before them.  Why should the algebraic case
be so much different from the transcendental cases?  What would happen
if we used here the same kind of techniques from earlier in the book?

First, the key difference in the algebraic case is the lack of unique
factorization.  Algebraic extensions are not, in general, unique
factorization domains, a classic example being the factorization of
$6$ into either $3\cdot 2$ or $(1+\sqrt{-5})\cdot(1-\sqrt{-5})$ in the
ring $\ZZ[\sqrt{-5}]$.  You can show that all four numbers $3$, $2$,
$(1+\sqrt{-5})$ and $(1-\sqrt{-5})$ are all prime in $\ZZ[\sqrt{-5}]$,
so we have two distinct factorizations in this ring.

{\bf Show an example in a function field.}

The main problem with our earlier tools is the difficulty in defining
factorization.  How, for example, do you construct a partial fractions
expansion?  A review of Theorems \ref{logarithmic integration theorem}
and \ref{exponential integration theorem} reveals that both depend
not merely on the construction of a partial fractions expansions, but
also on its {\it uniqueness}.  Without unique factorization, how can
you possibly have a unique partial fractions expansion?

The primary goal of this chapter is to develop techniques to carry
out the same kinds of operations we did earlier, but without relying
on unique factorization.

For example, a principle parts expansion of a function on an algebraic
curve is exactly analogous to a partial fractions expansion of a
rational function.

{\bf Demonstrate}

Although we began our development using infinite series expansions, we
ultimately concluded that we can completely specify a function (up to
an additive constant), using only a finite number of constants -- the
principle parts coefficients, which turn out to align precisely with
the coefficients in a partial fractions expansion.

Reassembling a partial fractions expansion into a rational function is
easy -- you just promote all the fractions to a common denominator,
add up the terms, and cancel any common factors that remain between
the numerator and the denominator.  Solving a Mittag-Lefler problem is
considerably more difficult, but is in principle the same operation --
given the principle parts coefficients (resp. the partial fractions
expansion), construct a single rational function that matches.  The
major caveat here is that, unlike reassembling a partial fractions
expansion, there might be not solution.  Not every principle parts
expansion has a matching algebraic function.

Likewise, finding an algebraic function's divisor is exactly analogous
to factoring the numerator and denominator of a rational function.
You get a finite set of poles and zeros with their locations and
multiplicities.  Again, in the algebraic case, it's more complicated --
you might have singularities with multiple places lying over a single
point; the ``coordinate'' is more complicated that a simple $(x,y)$
coordinate pair, but the principle is the same.

And finding a function with a specified set of poles and zeros is the
same as taking a rational function in factored form and multiplying
the factors together again.  Again, there's a caveat -- in the
algebraic curve case there might be no solution.

So, if the tools we've developed in this chapter parallel neatly with
the tools we used in Chapter 4 to solve integrals of rational
functions, can we generalize these tools to handle more complicated
transcendental fields, like we did in Chapters 5 and 6?  And do we
have anything like the Hermite reduction procedure we developed at the
end of Chapter 4?

The answers to both of these questions is 'yes'.  For the purpose of a
clear exposition, I've developed this theory so far in its simplest
form, and if you're seeing it for the first time, I suspect that you
already appreciate not having met it in its full generality!  We can
drop the assumption of an algebraically closed coefficient field and
lose very little except simplicity; this will be the subject of
Chapter 10.  Barry Trager showed in \cite{trager} how the Hermite
reduction can be performed in an algebraic extension; it's now called
{\it Hermite-Trager reduction} and I'll present it at the end of
Chapter 8.

However, continuing with the intent of presenting the theory in its
simplest form first, we'll begin the next chapter by looking at how to
use these tools to integrate Abelian integrals, much like we first met
partial fractions expansion when learning to integrate in first year
calculus, and only later generalized it into a form suitable for
integrating in arbitrary transcendental extensions.  We'll find that
completely solving integrals in even this simplest of algebraic
extensions will require a significant excursion into modern algebraic
geometry, so much so that the entirety of Chapter 9 will be devoted to
proving the book's most exotic theorem.

If there's a lesson to be learned from Chapter 7, though, it's this:

\begin{key point}
Divisors, principle parts expansions, Riemann-Roch spaces, and Mittag
Leffler problems are how we do factorization, partial fractions
expansions, and their inverse operations in algebraic extensions where
we've lost unique factorization.
\end{key point}
