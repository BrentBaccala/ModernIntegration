
\mychapter{Algebraic Curves}

{\bf THIS CHAPTER IS INCOMPLETE.}

Having addressed logarithmic and exponential extensions, we now turn
to the algebraic extension, which turns out to be completely different
in character from the two transcendental cases.

\begin{comment}

To justify that statement, let's begin by trying to attack algebraic
extensions in the same manner as the two transcendental cases.

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i (\theta^{i})')$$

Presumably, we know how to differentiate $\theta$, so let's write
$(\theta^i)' = \sum_j \psi_{i,j} \theta^j$:

$$(\sum_i a_i \theta^i)' = \sum_i ( a_i' \theta^i + a_i \sum_j \psi_{i,j} \theta^j)$$

Writing the $a_i$'s as a column vector $A$, we'll end up with a matrix equation:

$$I = A' + \Psi A$$

This is superficially similar to a Risch equation; it's a matrix Risch
equation.  To proceed in the same manner as before, we'd now have to
construct partial fractions expansions of everything and begin looking
at how cancellation might occur between our various components.  The
variety of possible cancelations between our terms seems daunting.

\vfill\eject

$$\int y\,dx \qquad y^2 = 4-x^2$$

Let's assume that our solution has the form $S = ay+b+\ln (cy+d)$, where $a$
and $b$ are rational functions in $x$ and $c$ is a polynomial in $x$.

$$S' = a' y + a y' + b' + \frac{c'y+cy'+d'}{cy+d}$$
$$2 y y' = -2x \qquad y' = -\frac{x}{y} = - \frac{x}{4-x^2}y$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})y+d}{cy+d}\frac{cy-d}{cy-d}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})cy^2+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2y^2-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c' - c \frac{x}{4-x^2})c(4-x^2)+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2})y + b' + \frac{(c'(4-x^2) - c x)c+cdy -(c' - c \frac{x}{4-x^2})dy-d^2}{c^2(4-x^2)-d^2}$$
$$S' = y = (a' - a \frac{x}{4-x^2} + \frac{cd-(c' - c \frac{x}{4-x^2})d}{c^2(4-x^2)-d^2})y + b' + \frac{(c'(4-x^2) - c x)c -d^2}{c^2(4-x^2)-d^2}$$


The correct answer is

$$S = 2i\ln(ix-y)+\frac{x}{2}y$$

Instead, we'll use a different approach...

\vfill\eject

\end{comment}

In this chapter, we'll begin studying {\it Abelian integrals}, which
are integrals whose integrands are formed from polynomials and roots
of polynomials.  In other words, integrands in an algebraic extension
of ${\bf C}(x)$.

How might we handle an algebraic extension of ${\bf C}(x)$?  A crucial
property of {\it algebraic functions}, as elements of an algebraic
extension are called, is that they admit series expansions everywhere,
including infinity, so long as we allow a finite number of negative
exponents.  Such functions are called {\it meromorphic}.  The
logarithm function fails to be meromorphic at the origin, and the
exponential function fails to be meromorphic at infinity, but
algebraic functions are meromorphic everywhere, including infinity.

This means that around any specific point, we can construct a series
expansion of the integrand and integrate termwise to obtain a series
expansion for the integral.  At first this doesn't seem terribly
useful, because series expansions are infinite and we're trying to
construct closed-form solutions, but it turns out that only a finite
number of places will have negative exponents in their series
expansions and that the function is completely specified, up to an
additive constant, by the coefficients of the negative powers.

Thus, the basic strategy is first to identify the function's {\it
poles}, the places where its value becomes infinite, and compute the
{\it principal part} of the series expansions there, which are the
negative exponents and their coefficients.  This is fairly
straightforward, though there are issues of computational complexity
that make it non-trivial.  Then we integrate termwise, which is
trivial, and obtain local series expansions at the poles of the
solution.  Next, we need to reassemble this local information into a
global function (if one exists), a {\it Mittag-Leffler problem}, for
which I will present a basic algorithm in this chapter, although more
efficient techniques have been developed.

What about the logarithmic terms?  This turns out to be the most
difficult part of the problem.  We can begin to analyze them using the
same techniques, by noting that the $t^{-1}$ terms in the principal
parts of the integrand lead directly to logarithms in the integral,
and furthermore that the coefficients of these terms give us the
locations and orders of the poles and zeros in the logarithms.  This
information specifies an algebraic function up to a multiplicative
constant\footnote{Of course.  Due to the presence of a constant of
integration, we expect to specify the main part of the integral up to
an {\it additive} constand, and the logarithmic parts of the integral
up to a {\it multiplicative} constant.}, and our algorithm can be
adapted without too much trouble to handle this case.

The problem is that no algebraic function might exist that match a
given set of zeros and poles, but increasing the order of the zeros
and poles might produce a solution.  This corresponds to raising the
logarithm term to powers, i.e, $\ln f$ is the same as $\frac{1}{2} \ln
f^2$, which is the same as $\frac{1}{3} \ln f^3$, except that in our
case the lower powers might not exist in our function field, even
though higher powers do.  What powers should we use?  We could go on
raising to higher and higher powers, hoping that something will work,
but the only known algorithm to limit this search requires reducing
modulo a prime, and that requires techniques that weren't developed
until the 1960s.  Before heading into such {\it modern algebraic
geometry}, however, let's see how far we can get with the classical
algebraic geometry of the nineteenth century.

\mysection{Classical Algebraic Geometry}

The roots of algebraic geometry lie in studying the zeros of
polynomial equations.  We began with a single polynomial in a single
variable, and have learnt a great deal about it.  We know how to solve
it (at least in terms of radicals) if its degree is less than 5.
Galois proved that no such solution (in radicals) exists (in the
general case) for larger degree, though abstract algebra provides us
with a suitable general theory to handle this case.  Simple long
division tells us that it can have no more roots than its degree, and
Gauss showed that all of the roots exist as complex numbers --- the
Fundamental Theorem of Algebra.

The next logical step is to consider zeros of a single polynomial in
two variables, and this equation has also received a great deal of
attention from mathematicians.  Like the univariate case, we have
theories devoted to low-order special cases --- {\it linear equations}
(all terms first degree or constant), the {\it conic sections} (all
terms second degree or less), and the {\it elliptic curves} (one term
third degree; all others second degree or less).  In the general case,
$\sum a_{ij} x^i y^j = 0$ is called an {\it algebraic curve}, and a
rational function in $x$ and $y$ is called an {\it algebraic
function}.  These will be our main focus of attention in this chapter.
Note that an algebraic curve's rational functions form a field, the
{\it function field} of the curve.

One of the first problems we face when dealing with algebraic curves
is the multi-valued nature of their solutions.  Consider the algebraic
function $y$ defined on the algebraic curve $y^2 = x^2 - 1$.  There
are, in fact, two seperate algebraic functions that solve this
equation --- both $y$ and $-y$ are solutions.  Conventionally, we
express this by writing something like $y = \pm\sqrt{x-1}$, but for
higher degree curves this kind of notation becomes unsuitable.  How,
for example, do you express the three possible solutions to a cube
root, and how do you deal with the general case where $y$ appears
multiple times in the curve's defining polynomial?

Our solution to this problem is to regard the entire algebraic curve
as a two-dimensional surface in a four-dimensional space.  Why four
dimensions?  Well, just as in the univariate case, we find it
convenient to work with complex numbers, so as to deal easily with
roots of negative numbers.  Regarding both $x$ and $y$ as complex
numbers (two dimensions each), and plotting them against each other,
we obtain a four dimensional space.  Just as in the real case, where
an equation like $x^2 + y^2 = 1$ defines a circle, an algebraic curve
defines a surface, the loci of $x$ and $y$ that satisfy the defining
polynomial.

The defining polynomial can be regarded as a polynomial in $y$, whose
coefficients are polynomials in $x$, simply by collecting terms with
like powers of $y$.  If we fix a given complex value of $x$, we have a
polynomial in $y$ with complex coefficients that can be solved as a
univariate polynomial and yields at most $n$ solutions for $y$.  We
can be more specific.  For any given value of $x$, we have {\it
exactly} $n$ solutions for $y$ {\it unless} one of two things happen.
Either the leading ($y^n$) coefficient is zero, in which case we have
less than $n$ solutions due to having a polynomial of degree less than
$n$, or the polynomial has multiple identical roots.

There are only a finite number of these special points, as can be seen
by considering the {\it discriminant} of the defining polynomial,
which is the resultant of the polynomial with its partial derivative
with respect to one of its variables.  The discriminant will be zero
at special points, so these points can be located by computing the
zeros of a univariate polynomial.

\begin{maximacommon}
discriminant(f,y) :=
  factor(resultant(diff(f,y), f, y));
\end{maximacommon}

These points are further classified according to whether or not the
curve is locally Euclidean in their neighborhood.  Geometrically, this
corresponds to looping around the point until you return to your
starting point.  If a single such {\it cycle} covers all the sheets of
the curve, the curve is locally Euclidean, and we have an {\it
ordinary point} of the curve, albeit one with {\it ramification}, the
{\it ramification index} being how many times we had to circle the
point.  Otherwise, multiple cycles are required to cover all of the
sheets, and we have a {\it singular point}.  Analytically, both
partial derivatives of the curve's polynomial are zero at a singular
point, while at least one is non-zero at ordinary points.

Now, the coefficient of $y^n$ in the defining polynomial will be a
polynomial in $x$, which has a finite number of roots at which it is
zero, so there are only a finite number of points where the defining
polynomial is of degree less than $n$ in $y$.  As $x$ approaches one
of these points, the value of the $y^n$ coefficient approaches zero,
which causes at least one of the roots to approach infinity.  We'll
deal with these points by introducing a line at infinity, forming
{\it projective space} and creating a {\it compact} surface.

There are several crucial theorems that depend on the topological
property of {\it compactness}.  The complex plane is not compact; we
remedy this by adding a point at infinity.  Likewise, two-dimensional
complex space is not compact, either; we remedy this by adding a
line at infinity and obtaining {\it projective space}.

Projective space.  Compactness.

Another highly desirable property is to be locally isomorphic to
Euclidean space.  A differentiable surface that is everywhere locally
Euclidean is called a {\it manifold}.

By adding a line at infinity and resolving our singularities, we can
coax our algebraic curve into a compact, connected, complex manifold.
The primary utility of this construction is embodied in the following
theorems.

\theorem
\label{holomorphic functions on compact manifolds are constant}

Every holomorphic function $M \to C$ on a compact, connected, complex manifold $M$ is constant.

\proof

{\tt https://math.stackexchange.com/questions/881742}

\cite{guillemin} Lecture 2 contains a proof of the Maximum Modulus Priciple.

\endtheorem

\theorem
\label{algebraic functions are characterized by their principal parts}

An algebraic function on an algebraic curve is completely characterized, up to an additive
constant, by its principal parts.

\proof

Consider two algebraic functions $f$ and $g$ with identical principal
parts.  Taking the difference between them, we obtain a function $f-g$
with no principal parts, i.e, a holomorphic function.  By
Theorem \ref{holomorphic functions on compact manifolds are constant},
$f-g$ must be constant.

\endtheorem

Given the importance of an algebraic function's principal parts, we will
now develop tools to calculate them.

\vfill\eject

\mysection{Puiseux Expansions}

Given an algebraic function on an algebraic curve, we wish to compute
its principal parts by locating its poles and computing series
expansions there.  Since the powers of $y$ form a ${\cal C}(x)$ basis
for the curve's function field (PROOF), our primary goal is to compute
series expansions for $y$ at arbitrary points on the curve.  With such
expansions in hand, it is straightforward to construct expansions for
any algebraic function.

At any point where the discriminant is non-zero, a series expansion
for $y$ exists as a power series in $(x-\alpha)$, which is a
straightforward application of the Implicit Function Theorem.

\theorem
{\bf Implicit Function Theorem}
\label{implicit function theorem}

\cite{baby rudin} Theorem 9.28 is a real version of the theorem.

\cite{guillemin} Lecture 7 starts with a complex version of the theorem.

See {\tt https://math.stackexchange.com/questions/489789}

The two-dimensional complex analytic version:

Let f be an analytic
mapping of an open set $E \in {\mathbb C}^2$ into ${\mathbb C}$, such
that $f(x,y)=0$ and $\frac{df}{dy} \ne 0$, then an analytic
function $g(x)$ exists such that $f(x,g(x))=0$.
\endtheorem

% For infinity and/or poles, substitute z=1/x or v=1/y.

For ramification points, we'll use a substitution of the form
$x=t^r+\alpha$, where $r$ is the {\it ramification index}, and
construct a power series, not in $(x-\alpha)$, but in
$t=(x-\alpha)^{1/r}$, a {\it Puiseux series}.

As a function of $t$, both $x$ and $y$ are analytic in an open
neighborhood of $t=0$.  $x$ is analytic as a function of $t$ because
of the I.F.T. applied to $x=t^r+\alpha$.  $y$ is analytic as a
function of $x$ because of the I.F.T. applied to the defining equation
of the curve.  Composition of analytic functions are analytic,
showing that $y$ is analytic as function of $t$.

These arguments hold in an open neighborhood of $t=0$.  Continuity of
the roots shows that $y$ is continuous at $t=0$.  Then use existence
of the Laurent series (Silverman 11.2) and continuity of the roots
(HOW?) to establish analyticity at the ramification point.

Singular points will admit multiple Puiseux series, each one
corresponding to a single cycle.

% This analysis is facilitated by Newton polygons.

The most straightforward way to compute Puiseux series is to use {\it Newton
polygons} to determine ramification, then setup a trial series with
the correct ramification and substitute it into the curve's defining
equation.

Let's assume that we're expanding around the point $(0,0)$, as this
simplifies the analysis with no loss of generality.  Consider
factoring the defining polynomial of the algebraic curve:

$$p_n y^n + p_{n-1}y^{n-1} + \cdots + p_0 = (y-r_1)(y-r_2)\cdots(y-r_n)$$

How might we do this, if the polynomial is irreducible?  We need to
extend to a larger field where the polynomial's roots exist.  The
analysis above shows that Puiseux series form a suitable extension.

For each root $r_i$, define its {\it order} as the lowest power of $t$
that appears in its Puiseux expansion, divided by its ramification
index.  Multiplying factors together adds their orders, so
$p_0$'s order will be the sum of all of the $r_i$'s orders.

% $p_i$ is a sum of terms, each term with $y^i$ and $n-i$ of the $r$'s
% multiplied together.  The term with the lowest order will be formed by
% multiplying the $n-i$ lowest order roots, but there may be
% cancellation between multiple sets of $n-i$ such roots.


Now let's consider increasing $i$ by one.  How does $p_0$'s order
change?  $p_1$ is formed by adding together all products of $n-1$
roots, so $p_1$'s order will be lower than $p_0$'s order by the
largest of $r_i$'s orders, unless there are multiple $r_i$'s with the
same order.  In this case, cancellation between these multiple terms
could result in $p_1$ having a larger order than otherwise expected.

If there are $j$ $r_i$'s with the same largest order, increasing $i$
by $j$ will lower $p_i$'s order by $j$ times that largest order.

The Newton polygon is formed by plotting the orders of the $p_i$
coefficients, with $i$ varying along the horizonal axis and the order
plotted vertically.  The easiest way to do this is to plot the powers
of the monomials that appear in the equation, and construct the
polygon's lower convex hull.

Thus, a segment on the lower convex hull of the Newton polygon will
correspond to as many solutions as the width of the line segment, each
with order equal to the change in height divided by the width, i.e,
the negative slope of the line segment.  The denominator of the slope
will be the ramification index, and the numerator of the slope will be
the lowest exponent expected in the expansion of $y$.

Consider a Puiseux series
corresponding to a single line segment of the Newton polygon.
Letting
$\alpha$ be the $x$ exponent and $\beta$ be the $y$ exponent, so the
monomials in $f$ have the form $x^\alpha y^\beta$, then the equation
of the line segment is $r\alpha + s\beta = p$, where $r$, $s$, and $p$
are integers and $r$ and $s$ are relatively prime.  Making the
substitution $x=t^r$ and $y=t^s u(t)$, we obtain:

$$f(x,y) = \sum A_{\alpha\beta} x^\alpha y^\beta$$
$$ = \sum A_{\alpha\beta} t^{r \alpha} t^{s \beta} u(t)^\beta$$
$$ = t^p \underbrace{\sum A_{\alpha\beta} t^{r \alpha + s \beta - p} u(t)^\beta}_{g(t,u)}$$

At least two of the $(r \alpha + s \beta - p)$ exponents will be zero
(those monomials corresponding to the endpoints of the line segment on
the Newton polygon); all of the remaining exponents will be positive.
This means that if we expand $u(t)$ in a power series in $t$:

$$u(t) = u_0 + u_1 t + u_2 t^2 + u_3 t^3 + \cdots$$

then any power of $u(t)$ will have the form:

$$u(t)^\beta = U_0(u_0) + U_1(u_0, u_1) t + U_2(u_0, u_1, u_2) t^2 + U_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

and $g(t,u)$ will also have the form:

$$g(t,u) = G_0(u_0) + G_1(u_0, u_1) t + G_2(u_0, u_1, u_2) t^2 + G_3(u_0,u_1,u_2,u_3) t^3 + \cdots$$

In order for $g(t,u)=0$ at $t=0$, $G_0(u_0)$ must be zero, and since $G_0(u_0)$ is a polynomial
in $u_0$, this gives us a finite number of values for $u_0$ that can solve our equation.

Now, by setting $g(t,u)=0$, can we obtain $u(t)$ as a function of $t$?

The Implicit Function Theorem states that we can, if $\frac{\delta g}{\delta u}$ is not zero
at the point we wish to expand around.

$$\frac{\delta g}{\delta u}(0,u_0) = \frac{\delta}{\delta u} G_0(u_0)$$

In short, the roots of $G_0(u_0)$ give us the starting values for our series expansion,
and if the roots are simple, then the Implicit Function Theorem guarantees that we'll
have a unique series expansion for $u(t)$ as a function of $t$.  If any of the roots
are not simple, then we can repeat this procedure for $g(t,u)$.  It can be shown
(\cite{bliss} \S 15) that this procedure always terminates.

\vfill\eject

{\tt puiseux(f,x,y,x0,y0,deg,[ratfunc])}, adopted from {\tt
implicit_taylor} in the Maxima distribution, computes the Puiseux
series of a function {\tt f} in variables $x$ and $y$, centered around
place {\tt (x0, y0)}, to degree {\tt deg}.  An optional seventh
argument {\tt ratfunc} specifies a rational function in $x$ and $y$ to
be expanded; the default is $y$.  {\tt ratfunc} may also be a
differential.  Either {\tt x0} or {\tt y0}, or both, may be {\tt inf}
to request expansions at infinity.  {\tt y0} may be {\tt false}
to request expansions at all points of the curve lying over {\tt x0}.

{\bf Limitations.}  {\tt puiseux()} constructs the Puiseux series
coefficients using Maxima's built-in {\tt solve()} routine, which is
limited to solving polynomials using radicals.  Therefore, {\tt
puiseux()} can fail on algebraic curves of fifth degree and higher.
Also, {\tt puiseux()} does not yet implement the recursion case
described above (when $\frac{\delta g}{\delta u}$ is zero).

{\tt puiseux_fracexp} controls whether {\tt puiseux}'s results
should be returned as fractional powers in $x$, or using
an auxilary variable $t$.

{\tt newton_polygon} graphs the Newton polygon corresponding to
an algebraic curve.

Several auxilary functions are also used; these are internal to
the namespace {\tt puiseux}:

{\tt puiseux4} computes a Puiseux series corresponding to a single
place on the algebraic curve; it is called multiple times by
{\tt puiseux} if {\tt y0} is {\tt false}.

{\tt puiseux3} computes a Puiseux series corresponding to a single
line segment of the Newton polygon.

{\tt puiseux2} computes a Puiseux series of $y$ corresponding to a
single line segment of the Newton polygon.

{\tt distribute_denominator(frac, var, lim)} distributes the numerator
of a fraction over its denominator, limiting the powers of {\tt var}
to be no higher than {\tt lim}.

{\tt recenter_curve(f,x,y,x0,y0)} changes variables on an algebraic curve
so that a given point {\tt (x0,y0)} is moved to {\tt (0,0)}.

{\tt monomial_powers(f,x,y,x0,y0)} returns a list of the powers that
appear in the monomials of a recentered curve.

{\tt lower_convex_points(list)} takes a {\tt monomial_powers} list as
input and returns the subset corresponding to the lower convex hull
of the Newton polygon.

{\tt lower_convex_hull(list, x)} is used by {\tt newton_polygon}
to graph the lower convex hull.

\vfill\eject

\begin{maximacommon}
/* in_namespace(puiseux)$ */

recenter_curve(f,x,y,x0,y0) := block([xcoeff, newf:0],
   if x0 = inf then (f:subst(x=1/x, f), x0:0),
   if y0 = inf then (f:subst(y=1/y, f), y0:0),
   f : num(ratsimp(f)),
   f : expand(subst([y=y+y0, x=x+x0], f)),
   for xpow: 0 thru hipow(f, x) do (
      xcoeff : coeff(f, x, xpow),
      for ypow: 0 thru hipow(xcoeff, y) do
         newf : newf + radcan(ratsimp(coeff(xcoeff, y, ypow)))*x^xpow*y^ypow
   ),
   newf
);
\end{maximacommon}

\begin{maximablock}
/* why doesn't this produce cdots? */
taylor(sin(x),x,0,3);
\end{maximablock}

\begin{maximacommon}
monomial_powers(f,x,y,x0,y0) := block(
  [result: [], xcoeff],
  f : recenter_curve(f,x,y,x0,y0),
  for xpow: 0 thru hipow(f, x) do (
    xcoeff : coeff(f, x, xpow),
    if xcoeff # 0 then
       for ypow: 0 thru hipow(xcoeff, y) do (
          if coeff(xcoeff, y, ypow) # 0 then
             result: endcons([ypow,xpow], result)
       )
  ),
  result
);
\end{maximacommon}

\begin{maximacommonsmall}
/* list is a list of pairs, like [[0,1], [0,2], [2,0]] */
/* this function returns the points on the lower convex hull */

lower_convex_points(list) := block(
   [result, nxt, slopes, lowest_slope],

   /* step 1 - sort the list first by increasing x values, */
   /*    then by increasing y values */

   list: sort(list, lambda([u,v],
      u[1] < v[1] or (u[1] = v[1] and u[2] < v[2]))),

   /* step 2 - discard all points directly above other points */

   list: lreduce(lambda([U,v],
        if last(U)[1] = v[1] then U else endcons(v,U)),
      list, [list[1]]),

   /* step 3 - start with the point on the y-axis, */
   /*    then rotate around the lower convex hull, */
   /*    adding points as we go */

   result: [pop(list)],

   while length(list) > 0 do (
      nxt: last(result),
      slopes: map(lambda([u],
         ((nxt - u)[2] / (nxt - u)[1])), list),
      lowest_slope: sort(slopes)[1],
      while (length(slopes) > 0 and slopes[1] > lowest_slope) do
         (pop(slopes), pop(list)),
      while (length(slopes) > 0 and slopes[1] = lowest_slope) do
         (pop(slopes), result:endcons(pop(list), result))
   ),
   result
);

/* this function graphs a line around the lower convex hull */

lower_convex_hull(inlist, x) := block([list],
  list: sort(inlist, lambda([u,v], u[1] < v[1])),
  while length(list) >= 2 and list[2][1] < x do pop(list),
  if length(list) >= 2 then
     list[1][2] + (x - list[1][1])
                  * (list[2][2] - list[1][2])
                  / (list[2][1] - list[1][1])
  else
     %nan
);

newton_polygon(f, x, y, x0, y0) := block(
   [newton_points, convex_hull_points, maxx, maxy, maxxy, filename],

   newton_points : monomial_powers(f,x,y,x0,y0),
   convex_hull_points : lower_convex_points(newton_points),
   maxx: lreduce(max, map(first, convex_hull_points)),
   maxy: lreduce(max, map(second, convex_hull_points)),
   maxxy: max(maxx, maxy),
   filename: next_pdf_filename(),

   plot2d([[discrete, newton_points],
           'lower_convex_hull(convex_hull_points, l)],
       [l,0,maxxy+1], [y,0,maxxy+1],
       [style, points, [lines, 5]],
       [color, red, blue],
       [point_type, bullet],
       [legend, false],
       [xlabel, false],
       [xtics, 1], [ytics, 1],
       [ylabel, false],
       [pdf_file, filename]),
   embed_latex_graphic(filename)
);

\end{maximacommonsmall}

\begin{maximacommonsmall}
puiseux2_cache : [];

puiseux2(f,x,y,x0,y0,deg,r,s,p) :=
 block([n:max(deg,s), xexpansion, yexpansion, res,
        allans, ans, eqns, deqn, cache, result],

  cache: assoc([f,x,y,x0,y0,r,s,p], puiseux2_cache),
  if listp(cache) and cache[1] >= deg then return(cache[2]),

  if x0 # inf then
    xexpansion: x0 + t^r
  else
    xexpansion: (1/t)^r,

  yexpansion: y0 + sum(t^i*a[i],i,s,n),

  res: subst([x=xexpansion, y=yexpansion], f/t^p),

  res: expand(num(radcan(ratsimp(res)))),
  eqns: create_list(coeff(res,t,i),i,0,(n-s)),

  /* eqns[1] should be a polynomial in a[s] */
  /* We can only handle its simple roots, so compute */
  /* its derivative to check that */

  deqn: diff(eqns[1], a[s]),
  allans: sort(radcan(solve(eqns[1],a[s]))),

  /* Maxima's solve() can't handle arbitrary high powered */
  /* polynomials.  Check to see if it worked. */

  if apply("+", multiplicities) # hipow(eqns[1], a[s]) then
     error("NYI: solve didn't work in puiseux2"),

  /* If r > 1, then we expect each solution to be duplicated */
  /* r times, differing only by r'th roots of unity.  These */
  /* different solutions all correspond to the same branch, */
  /* so we'll remove all but one of each. */

  /* We exclude zero as a solution, since zero corresponds */
  /* to a higher degree solution than the ones we're seeking. */

  ans: [],
  for a in allans do
     if rhs(a) # 0
        and not member(ratsimp(a^r), ratsimp(ans^r)) then
           if ratsimp(subst(a[s]=a, deqn)) = 0 then
              error("NYI: multiple root in puiseux2")
           else
              ans: endcons(a, ans),

  /* For each solution to eqns[1], solve the remaining eqns. */
  /* IFT guarantees unique solutions to all eqns. */

  ans: map(makelist, ans),
  result: map(lambda([oneans], block([eqns2:subst(oneans, eqns)],
    for i:1 thru n-s do block([s:radcan(solve(eqns2[i+1],a[s+i]))],
      if not listp(s) or length(s) # 1 then
         error("puiseux2: internal error"),
      oneans: endcons(s[1], oneans),
      eqns2: subst(oneans, eqns2)),
    subst(oneans, yexpansion))), ans),

  puiseux2_cache: cons([[f,x,y,x0,y0,r,s,p], [deg, result]], puiseux2_cache),
  result
  );
\end{maximacommonsmall}

\begin{maximacommonsmall}
/* puiseux_fracexp controls whether series are returned with
 * fractional exponents, or using an auxilary variable (t).
 */

puiseux_fracexp : false ;

\end{maximacommonsmall}

\begin{maximacommonsmall}
distribute_denominator(frac, var, lim) := block(
  [result: 0, n: num(expand(frac)), d: denom(expand(frac))],
  for i: lopow(n, var) thru min(hipow(n, var), lim + hipow(d, var)) do
    result : result + radcan(rectform(ratsimp(coeff(n, var, i)/d)))*var^i,
  result
);
\end{maximacommonsmall}

\begin{maximacommontiny}
puiseux3(f,x,y,x0,y0,orig_y0,deg,r,s,p,b) :=
 block([xsubst, tsubst, differential: 1, adjustment: 1, maxpow, maxypow, rf:0, yserieslist, result],

   if x0 # inf then (
      xsubst: x = x0 + t^r,
      tsubst: t^r = x - x0
   ) else (
      xsubst: x = 1 / t^r,
      tsubst: t^r = 1/x
   ),

   /* deg is the highest t-degree we want in our final result */

   maxpow: deg,

   if not freeof(del(x), b) then (
      b: ratsimp(b/del(x)),
      if not freeof(del(x), b) then
         error("puiseux: requested differential can't be normalized"),
      differential: del(t),
      if x0 # inf then (
         /* x= t^r  ->  dx = r t^(r-1) dt */
         /* so we don't need as many terms to get to maxpow */
         maxpow : maxpow - (r-1),
         adjustment : r * t^(r-1)
      ) else (
         /* x= t^-r  ->  dx = -r t^(-r-1) dt */
         /* so we need more terms to get to maxpow */
         maxpow : maxpow + (r+1),
         adjustment : -r * t^(-r-1)
      )
   ),

   /* Expand each of the coefficients of the requested */
   /* function as a Taylor or Laurent series in x, */
   /* leaving y alone.  We'll later substitute a Puiseux */
   /* series for y. */

   maxypow : 0,
   for i: 0 thru hipow(b, y) do block([icomponent],
      /* s is the lowest t-degree expected in the expansion of y */
      /* if y0 is non-zero, then use min(s,0) instead of s */
      /* therefore, maxpow-s*i is the highest t-degree required in rf's expansion */
      /* ceiling((maxpow-s*i)/r) is the highest x-degree required */
      icomponent: ratsimp(subst(xsubst, taylor(ratsimp(coeff(expand(b), y, i)),
                                            x, x0, ceiling((maxpow-min(s,0)*i)/r)))
                          * adjustment),
      /* maxpow(rf) = minpow(icomponent) + maxpow(y) * i */
      if i # 0 then
         maxypow: ceiling((deg - lopow(icomponent,t))/i),
      rf: rf + icomponent * y^i
   ),

   /* expand y to the requested degree */
   yserieslist: puiseux2(f,x,y,x0,y0,maxypow,r,s,p),

   map(lambda([yseries],
      result: ratsimp(subst(y=yseries, rf)),
      result: distribute_denominator(result, t, deg),

      if puiseux_fracexp then
         subst(t=(x-x0)^(1/r), result * differential)
      else
         [result * differential, tsubst, y=orig_y0]
   ), yserieslist)
 );
\end{maximacommontiny}

\begin{maximacommonsmall}
puiseux4(f,x,y,x0,y0,deg,rf) :=
 block([result: [], lastslope : 0, orig_y0 : y0, sign, mp, pairs],

   if y0 # inf then (
      sign: -1
   ) else (
      /* If we're expanding y at infinity, don't recenter y, */
      /* but use upward sloping segments on the Newton polygon */

      y0 : 0,
      sign: 1
   ),

   /* Make a list of all line segments on the Newton polygon */

   mp: lower_convex_points(monomial_powers(f,x,y,x0,y0)),

   if sign = -1 and mp[1] = [0,0] then
      error(concat("puiseux: coordinates do not solve equation of curve")),
   if sign = 1 and last(mp)[2] = 0 then
      error(concat("puiseux: coordinates do not solve equation of curve")),

   pairs: makelist([mp[i], mp[i+1]], i, length(mp)-1),

   for pair in pairs do block([delta, g, r, s, p],
     delta: pair[2]-pair[1],
     if delta[2] * sign > 0 then (
       g: gcd(delta[1],delta[2]),
       r: delta[1]/g,
       s: -delta[2]/g,
       p: pair[1][2]*r + pair[1][1]*s,

       if s/r # lastslope then
          result: append(result, puiseux3(f,x,y,x0,y0,orig_y0,deg,r,s,p,rf)),

       lastslope : s/r
     )
   ),
   result
 );

\end{maximacommonsmall}

\begin{maximacommonsmall}
puiseux(f,x,y,x0,y0,deg,[ratfunc]) :=
 block([result: [], rf],

   /* If the caller requested a specific rational function to */
   /* be expaned, use modulo to normalize it.  Default is y. */

   if length(ratfunc) > 0 then
      rf: modulo(ratfunc[1], f, y)
   else
      rf: y,

   /* If y0 is false, compute the possible solutions for y0. */

   if y0 = false then block([ys, f0],
      if x0 # inf then
         f0 : subst(x=x0, f)
      else
         f0 : subst(x=0, num(ratsimp(subst(x=1/x, f)))),
      ys : unique(radcan(rectform(solve(f0, y)))),

      if apply("+", multiplicities) # hipow(f0, y) then
         error("NYI: solve didn't work in puiseux"),
      for yi in ys do
         result: append(result, puiseux4(f,x,y,x0,rhs(yi),deg,rf)),

      /* If substituting x0 into f reduced hipow(y), then */
      /* some of our solutions are at infinity. */

      if hipow(f,y) # hipow(f0,y) then
         result: append(result, puiseux4(f,x,y,x0,inf,deg,rf))
   ) else
      result: puiseux4(f,x,y,x0,y0,deg,rf),

   result
 );

/* export(puiseux, puiseux_fracexp, newton_polygon)$ */
/* in_namespace(maxima)$ */
/* import(puiseux)$ */
\end{maximacommonsmall}

\begin{maximacode}

/* This is a small test suite to verify puiseux's correct operation. */

load("taylor1.mac")$

ev((
  test1: puiseux(y^2 - x^2 - 1, x, y, 0, 1, 5),
  test2: implicit_taylor(y^2 - x^2 - 1, 0, 5, 1),
  if is(test1[1] # test2) then error("puiseux: test1/2 failed"),

  test3: puiseux(b^2-a^2-1, a, b, 0, 1, 5),
  if is(test1 # subst(a=x, test3)) then error("puiseux: test3 failed"),

  test4: puiseux(y^2 + y - x^3, x, y, 0, 0, 10),
  test5: implicit_taylor(y^2 + y - x^3, 0, 10, 0),
  if test4[1] # test5 then error("puiseux: test4/5 failed"),

  test6: puiseux(y^2 - x, x, y, 0, 0, 10),
  if test6[1] # -sqrt(x) then error("puiseux: test6 failed")),
puiseux_fracexp: true)$

ev((
  test7: puiseux(y^2 + x^2 - 1, x, y, inf, inf, 12, x/y),
  test8: puiseux(y^2 + x^2 - 1, x, y, inf, inf, 10, x/y * del(x)),
  if not(test8[1][1] === - test7[1][1] * t^-2 * del(t)) then error("puiseux7/8: test failed")),
puiseux_fracexp: false)$

/* I'd like to check that these throw errors, but I've got no */
/* good way to remove error messages from the output. */

/* puiseux(y^2 + x^2 - 1, x, y, 0, 0, 5); */
/* puiseux(y^2 + x^2 - 1, x, y, 0, inf, 5); */
/* puiseux(y^2 + x^2 - 1, x, y, inf, 0, 5); */
/* puiseux(x*y - 1, x, y, inf, inf, 5); */
\end{maximacode}

\vfill\eject

\begin{sageblock}

singular.lib('hnoether.lib')

def recenter_curve(f, x0, y0):

    # XXX expect parent to be QQ[x,y] or something very similar
    (x,y) = f.parent().gens()

    if x0 == oo:
        f = f.subs({x : 1/x}).numerator()
        x0 = 0
    if y0 == oo:
        f = f.subs({y : 1/y}).numerator()
        y0 = 0

    return f.subs({x : x+x0, y : y+y0})

def puiseux_cycle(f, x0, y0, n, hne):

    ed = singular.extdevelop(hne, n)

    # We get a flag indicating which variable was used as a
    # uniformizing parameter.

    (x,y) = f.parent().gens()
    if ed[3] == 1:
        (x,y) = (y,x)
        (x0,y0) = (y0,x0)

    numfield = f.parent().base_ring()

    eds = ed[1].sage()

    # eds is a matrix over a bivariate polynomial ring over a number field,
    # but it might not be the same number field use to define the polynomial
    # Sage Trac #25176

    # a homomorphism that maps its number field to our number field

    h = eds.base_ring().base_ring().hom(numfield.gens())

    # a homomorphism that maps its polynomial ring to ours

    h2 = eds.base_ring().hom(h, f.parent())

    # map the matrix back to the polynomial's original ring

    eds = eds.apply_map(h2)

    # Construct a Laurent power series ring in a new variable

    PSring = Frac(numfield[['t']])
    t = PSring.0

    # convert the matrix into power series
    # XXX the matrix might include 'x' (or 'y') in some of its slots, right?

    p = eds * matrix([t^i for i in range(1, eds.ncols()+1)]).transpose()

    # XXX Can't handle a really complex HN expansion yet
    assert(p.nrows() <= 2)

    if x0 == oo:
        x1 = 0
    else:
        x1 = x0

    if y0 == oo:
        y1 = 0
    else:
        y1 = y0

    if p.nrows() == 1:
        xexpansion = x1 + t
        PSelem = y1 + PSring(p[0,0]).add_bigoh(n+1)
    else:
        xexpansion = x1 + PSring(p[1,0]).add_bigoh(n+1)
        PSelem = y1 + (p[0,0].subs({x: xexpansion})/t).add_bigoh(n+1)

    if x0 == oo:
        xexpansion = 1/xexpansion
    if y0 == oo:
        PSelem = 1/PSelem

    if x0 == oo:
       xequality = (SR.var(str(x)) == SR(1/t))
    else:
       xequality = (SR.var(str(x)) == SR(x0 + t))

    if ed[3] == 0:
        return [xexpansion, PSelem]
    else:
        return [PSelem, xexpansion]

    # Symbolic Ring equation format (pretty to print, hard to work with)

    xequality = (SR.var(str(x)) == SR(xexpansion))

    if ed[3] == 0:
        return [xequality, (SR.var(str(y)) == SR(PSelem))]
    else:
        return [(SR.var(str(y)) == SR(PSelem)), xequality]

def apply_map_to_laurent_series(morphism, series):
    R = series.parent().change_ring(morphism.codomain())
    result = R(map(morphism, list(series)), series.valuation())
    return result.add_bigoh(series.prec())

# This version puts the coefficients in the Symbolic Ring
# so that we can see them in radical form

def apply_map_to_laurent_series(morphism, series):
    R = series.parent().change_ring(SR)
    result = R(map(lambda x: x.radical_expression(), map(morphism, list(series))), series.valuation())
    return result.add_bigoh(series.prec())

def puiseux(origf, x0, y0, n):

    # Singular can't work directly with QQbar, so if that's our base
    # ring, then we need to convert to a NumberField.

    if origf.parent().base_ring() == QQbar:
        orig_elems = origf.coefficients()
        if x0 != oo: orig_elems.append(x0)
        if y0 != oo: orig_elems.append(y0)

        nfe = number_field_elements_from_algebraics(orig_elems)
        numfield = nfe[0]  # The number field
        new_elems = nfe[1] # The converted elements
        morphism = nfe[2]  # A morphism to get back to QQbar

        elem_dict = dict(zip(orig_elems + [oo], new_elems + [oo]))
        def elem_map(e): return elem_dict[e]

        newf = origf.map_coefficients(elem_map, new_base_ring=numfield)

        p = puiseux(newf, elem_dict[x0], elem_dict[y0], n)

        # We may have extended further to construct the results
        # If so, 'morphism' will no longer work, so replace it
        # with the first possible embedding into QQbar.

        # XXX First, we shouldn't just pick an embedding at random
        # Second, we should embed into the original NumberField, and
        # then use 'morphism' (a particular pre-selected embedding)
        # to get the rest of the way into QQbar

        if p[0][0].base_ring() is not newf.parent().base_ring():
            morphism = p[0][0].base_ring().embeddings(QQbar)[0]

        return [[apply_map_to_laurent_series(morphism, cycle[0]), apply_map_to_laurent_series(morphism, cycle[1])] for cycle in p]

    f = recenter_curve(origf, x0, y0)
    hne = f._singular_().hnexpansion()

    if len(hne) == 0:
       raise ValueError("specified coordinates not on curve")

    if hne[1].type() == 'ring':
       extensionRing = hne[1].sage()
       # XXX We've got hne computed and stored in the ring,
       # but I'm not sure how to get to it from Sage,
       # so just recurse

       return puiseux(extensionRing(origf), x0, y0, n)

    return [puiseux_cycle(f, x0, y0, n, cycle) for cycle in hne]


\end{sageblock}

\vfill\eject

\example Construct Puiseux expansions of $y$ at the multiple points of the
curve $y^2 = 1 - x^2$

We normalize the defining polynomial by writing it as
$y^2 + x^2 - 1 = 0$.  Where does it have multiple points?
We compute the discriminant:

\begin{maximablock}
discriminant(y^2 + x^2 - 1, y);
\end{maximablock}

\begin{sageblock}
R.<x,y> = QQ[];
(y^2 + x^2 - 1).discriminant(y).factor()
\end{sageblock}

The multiple points of $y^2 = 1 - x^2$ lie at the roots of the
discriminant, which are $x = \pm 1$.  In both cases, $y=0$ is the only
solution, so the curve has multiple points at $(x,y)=(\pm 1, 0)$.  The
partial derivative of the defining polynomial with respect to $x$
is $2x$, which is non-zero, so neither of these multiple
points are singular; we'll get ramification instead.
The analysis is almost the same in both cases, so I'll just do $(1,0)$.

First, construction of the Newton polygon requires recasting the
curve's polynomial into a form centered about the point being
analyzed, i.e, $y^2 + (x-1)^2 + 2(x-1) = 0$.  Next, we construct the Newton
polygon by plotting the monomial powers, putting the $y$ exponents on the horizontal axis and the
$(x-1)$ exponents on the vertical:

\begin{figure}[H]
\begin{center}
\maximac{newton_polygon(y^2+x^2-1, x, y, 1, 0)$}
\end{center}
\end{figure}

The only segment on the Newton polygon's lower convex hull has slope
$-1/2$ and width 2, telling us that two of our roots (the width of the
segment) will require a single Puiseux series with ramification index
2 (the denominator of the slope):

$$x=t^2+1$$

We know that y can be expressed as a power series in $t$ with
initial exponent 1 (the numerator of the slope):

$$y= a_1 t + a_2 t^2 + a_3 t^3 + \cdots$$

Now, substituting these expressions for $x^2$ and $y^2$ into the
curve's defining equation $y^2 + x^2 - 1 = 0$ and setting all
coefficients of $t$ to zero, we find:

\begin{maximablock}
ratsimp(subst(
   [x=t^2+1, y=a[1]*t+a[2]*t^2+a[3]*t^3],
   y^2+x^2-1));
\end{maximablock}

$$2 + a_1^2 = 0 \qquad 2 a_1 a_2 = 0 \qquad 1 + 2 a_1 a_3 + a_2^2 = 0$$

The first equation tells us that $a_1 = \pm\sqrt{2}i$,
the second equation tells us that $a_2=0$ and the
third equation tells us that $a_3 = \pm \frac{\sqrt{2}}{4} i$, so

% $$a_0=0; \qquad a_1 = \pm\sqrt{2}i; \qquad a_2 = 0; \qquad a_3 = \pm \frac{\sqrt{2}}{4} i$$

$$x = t^2 +1; \qquad y = \pm\left[ \sqrt{2}it + \frac{\sqrt{2}}{4} it^3 + \cdots \right]$$

It would seem that we have two different series to chose from.  This
is not really the case, as they differ by only a $180^\circ$ rotation
in the t-plane, as can been seen by substituting $t=-t$, which
transforms one of the y-series into the other, while leaving the
x-series unchanged.  Let's confirm this result with Maxima:

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, 1, 0, 3);
\end{maximablock}

\begin{sageblock}
puiseux(y^2 + x^2 - 1, 1, 0, 3)
\end{sageblock}

Now, let's analyze the point at infinity.  We move infinity to a
finite point (0) with the substitution $x=u^{-1}$, then combine all of
our terms over a common denominator and discard the denominator.  Our
curve becomes:

$$y^2 u^2 + 1 - u^2 = 0$$

\begin{figure}[H]
\begin{center}
\maximac{newton_polygon(y^2+x^2-1, x, y, inf, 0)$}
\end{center}
\end{figure}

The Newton polygon's lower convex hull has a single line segment,
slope $1$, length $2$, telling us that we'll have two separate
poles, each with ramification index 1.  Thus, $u$ can be used
directly as a uniformizing variable, and we postulate an expansion for
$y$ in the form:

$$y = a_{-1} \frac{1}{u} + a_0 + a_1 u + a_2 u^2 + a_3 u^3 + \cdots$$

Plugging this into $y^2 u^2 + 1 - u^2$ and setting all the resulting
coefficients to zero, we conclude:

\begin{maximablock}
ratsimp(subst(
   [y=a[-1]*(1/u)+a[0]+a[1]*u+a[2]*u^2+a[3]*u^3],
   y^2*u^2+1-u^2));
\end{maximablock}

%% $$a_{-1}^2 + 1 = 0; \qquad 2 a_{-1} a_0 = 0 \qquad (2a_{-1}a_1+a_0^2-1)=0$$

$$a_{-1} = \pm i; \qquad a_0 = 0; \qquad a_1 = \mp \frac{1}{2}i; \qquad a_2 = 0; \qquad a_3 = \mp \frac{1}{8}i$$

$$y = \pm i \frac{1}{u} \mp \frac{1}{2} i u \mp \frac{1}{8} i u^3 + \cdots$$

This time, there is no ramification, since $u$, and not a power of
$u$, is $\frac{1}{x}$.  We actually have two distinct series that will
yield two different values of $y$ for each value of $u$.
Geometrically, we have two sheets that approach each other and touch
at a singular point where the curve is not locally Euclidean.

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, inf, inf, 3, y);
\end{maximablock}

\endexample

\example \cite{bliss} \S 68
Compute expansions at all multiple points of

$$y^3+x^3y+x=0$$

We begin by computing the discriminant of the
equation, which gives us the locations of the multiple points.

\begin{maximablock}
discriminant(y^3 + x^3*y + x, y);
\end{maximablock}

\begin{sageblock}
f = y^3 + x^3*y + x
f.discriminant(y).factor()
\end{sageblock}

The multiple points lie over the roots of this equation: $x=0$ and
the seven roots of $4x^7+27=0$.  Infinity also needs to be
examined.  We begin with $x=0$:

\begin{maximablock}
puiseux(y^3 + x^3*y +x, x, y, 0, false, 10);
\end{maximablock}

\begin{sageblock}
puiseux(f, 0, 0, 10)
\end{sageblock}

This result shows that we have a single cycle at $(x,y)=(0,0)$ with
three sheets.  Now, let's look at a specimen root
of $4x^7+27=0$:

\begin{comment}
puiseux(y^3 + x^3*y +x, x, y, g, -3/(2*g^2), 1);
puiseux(y^3 + x^3*y +x, x, y, g, 3/g^2, 1);
puiseux(y^3 + x^3*y +x, x, y, g, -(3/8)^(1/7), 1);
\end{comment}

\begin{maximablock}
g: (-27/4)^(1/7);
puiseux(y^3 + x^3*y +x, x, y, g, false, 1);
\end{maximablock}

\begin{comment}
\begin{sageblock}
# Not correct.  See below.
R.<x,y,a> = QQ[];
I = ideal(4*a^7+27)
Q = R.quo(I)

f = y^3 + x^3*y + x
# puiseux(Q(f), a, 3/a^2, 10)
\end{sageblock}
\end{comment}

\begin{sageblock}
R.<x,y> = QQbar[]

f = y^3 + x^3*y + x
puiseux(f, 0, 0, 10)

g = QQbar(-27/4)^(1/7)
puiseux(f, g, -3/(2*g^2), 2)
puiseux(f, g, 3/g^2, 1)
\end{sageblock}

\begin{sageblock}
R1.<g> = QQ[]
S.<g> = NumberField(4*g^7+27)
R.<x,y> = S[]

f = y^3 + x^3*y + x
puiseux(f, g, -3/(2*g^2), 2)
puiseux(f, g, 3/g^2, 1)

# Why doesn't this work?
#
# try:
#     puiseux(f, g, 0, 1)
#     raise AssertionError()
# except ValueError:
#     pass
\end{sageblock}

We have one sheet of two cycles at $(g,-3/(2g^2))$
and an ordinary point at $(g,3/g^2)$.

Finally, let's look at what happens when $x$ goes to infinity:

\begin{maximablock}
puiseux(y^3 + x^3*y +x, x, y, inf, false, 10);
\end{maximablock}

\begin{sageblock}
puiseux(f, oo, 0, 10)
puiseux(f, oo, oo, 10)
\end{sageblock}

Here we have an ordinary point at $(\infty,0)$ and
a single cycle of two sheets at $(\infty,\infty)$.

We have examined all of this curve's ramification points,
including those at infinity (since we analyzed all of its
points at infinity), and found that all of them admitted
a single Puiseux expansion.

Therefore, this curve is {\it non-singular}, and according to the
genus-degree formula (MORE INFO), its geometric and arithmetic genus
are the same.  Its arithmetic genus is $\frac{1}{2}(d-1)(d-2) = 3$,
where $d=4$ is the degree of the defining polynomial.  Computing
the geometric genus is more difficult\footnote{
{\tt https://www.singular.uni-kl.de/Overview/Examples/Genus/genus1.html}

{\tt https://en.wikipedia.org/wiki/Algebraic_curve\#Classification_of_singularities}

{\tt https://math.stackexchange.com/questions/150840}

{\tt http://mathforum.org/library/drmath/view/71229.html}
}, but we can verify our
information with Sage, being careful to work in {\it projective} space:

\begin{sageblock}
PP.<x,y,z> = ProjectiveSpace(QQ, 2)
C = Curve(y^3*z + x^3*y + x*z^3)
C.is_singular()
C.arithmetic_genus()
C.geometric_genus()
\end{sageblock}


\endexample

%\mysection{meromorphic functions are analytic}
%
%first, trace of a meromorphic function is meromorphic on C(x), and is
%thus a rational function
%
%Liouville's theorem: a bounded entire function is constant
%
%Proof A: (Silverman) use a Taylor series expansion around z=0, which
%is valid in the entire plane (since the function is entire).  Cauchy's
%inequality $|f| \le M ==> |c_n| \le M/{R^n}$ (eq. 10.8') as R->infty
%implies that the function is constant.
%
%Lemma: A entire function with no singularities, even at infinity, is
%constant.
%
%Proof: We can do a Taylor series expansion at the origin, whose
%non-zero terms will correspond to the principal part of the expansion
%at infinity, which must therefore be zero.
%
%Next: A entire function with only a pole at infinity is a polynomial.
%The principal part at infinity will be a polynomial.  Subtract it out
%to get a function with no singularities, which must be constant.
%
%Next: Given a function with only a finite number of finite poles,
%multiply it by a polynomial (the denominator) matching the poles with
%zeros.  Now we've got a function with only a pole at infinity, which
%must be a polynomial (the numerator).

\example Find the principal parts of $\frac{1}{y}$ on the curve
$y^2 = 1 - x^2$

The {\it principal part} of an algebraic function is the part
of its series expansion with negative exponents.  Theorem
\ref{algebraic functions are characterized by their principal parts}
states that an algebraic function is completely determined,
up to adding a constant, by its principal parts.

The first step is to locate the function's poles, which in this case is
simply the places where the denominator is zero, and that's just
$x=\pm 1$.  Now, if we use {\tt puiseux}, we can just request a series
truncated at the $-1$ term:

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, 1, 0, -1, 1/y);
\end{maximablock}

\endexample

\example Find the principal parts of $\frac{x}{y} \ud x$ on the curve
$y^2 = 1 - x^2$

Differential forms are not functions, and have different series
expansions, due to the presence of the
differential, which must be adjusted at ramification points.

Let's expand $\frac{x}{y}$ at $x=1$:

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, 1, 0, 5, x/y);
\end{maximablock}

Now $x=t^2+1$, so $\ud x=2t\ud t$.  Thus, multiplying $\frac{x}{y}$
by $\ud x$ and changing our variable to $t$ will multiply
all of the terms in our expansion by $2t$:

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, 1, 0, 6, x/y*del(x));
\end{maximablock}

Even though $\frac{x}{y}$ has a pole
at $x=1$, $\frac{x}{y} \ud x$ does not!

Its behavior at infinity also requires analysis.

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, inf, inf, 1, x/y);
\end{maximablock}

\begin{sageblock}
R.<x,y> = QQbar[];
f = y^2 + x^2 - 1
puiseux(f, oo, oo, 1)
\end{sageblock}

$\frac{x}{y}$ has no poles at infinity, and approaches
the limiting values $\pm i$ as $x$ and $y$ approach
infinity.  The differential $\frac{x}{y} \ud x$,
on the other hand, requires us to multiply by $\ud x$,
and since $x=\frac{1}{t}$, $\ud x = - \frac{1}{t^2} \ud t$.

\begin{maximablock}
puiseux(y^2 + x^2 - 1, x, y, inf, inf, -1,
        x/y * del(x));
\end{maximablock}

In short, while $\frac{x}{y}$ has poles only at $(\pm 1,0)$,
$\frac{x}{y} \ud x$ has poles only at infinity.

\endexample

\example
Compute expansions at all multiple points of
the exercises in \cite{bliss} \S 68.

To facilitate this example, let's define an
auxiliary function to perform the analysis:

\begin{maximacommon}
analyze_multiple_points(f, [args]) := block(
  [discriminant: discriminant(f, y), deg],
  display(discriminant),
  deg : if length(args) > 0 then args[1] else 2,
  for r in solve(discriminant, x) do
    disp(puiseux(f, x, y, rhs(r), false, deg)),
  disp(puiseux(f, x, y, inf, false, 5))
);
\end{maximacommon}

%% \begin{comment}

$$y^3-3y+2x=0$$

\begin{maximablock}
analyze_multiple_points(y^3 - 3*y + 2*x)$
\end{maximablock}

$$y^3+3y-x$$

\begin{maximablock}
analyze_multiple_points(y^3 + 3*y - x)$
\end{maximablock}

$$y^3-3y^2-x$$

\begin{maximablock}
analyze_multiple_points(y^3 - 3*y^2 - x)$
\end{maximablock}

$$y^4-4y-x$$

This curve produces coordinates that aren't {\tt recenter}'ed
properly.

\begin{maximablock}
errcatch(analyze_multiple_points(y^4 - 4*y - x))$
\end{maximablock}

$$y^4+2(1-2x)y^2+1$$

\begin{maximablock}
analyze_multiple_points(y^4+2*(1-2*x)*y^2+1)$
\end{maximablock}

$$y^3-3y^2+x^6$$

\begin{maximablock}
analyze_multiple_points(y^3-3*y^2+x^6, 6)$
\end{maximablock}

$$y^3-3y+2x^2(2-x^2)$$

\begin{maximablock}
analyze_multiple_points(y^3-3*y+2*x^2*(2-x^2))$
\end{maximablock}

$$y^3-3y+2x^3(2-x^3)$$

\begin{maximablock}
analyze_multiple_points(y^3-3*y+2*x^3*(2-x^3))$
\end{maximablock}

$$3x(x-1)y^4 -4(x-1)(x-2)y^3 + (4/27)(x-2)^4$$

\begin{maximablocksmall}
analyze_multiple_points(3*x*(x-1)*y^4 -4*(x-1)*(x-2)*y^3 + (4/27)*(x-2)^4)$
\end{maximablocksmall}

$$y^5 + (x^2-1)y^4 - (4^4)/(5^5)x^2(x^2-1)$$

This function produces complicated fifth degree equations that
Maxima can't {\tt solve()}.

\begin{maximablocksmall}
errcatch(analyze_multiple_points(
   y^5 + (x^2-1)*y^4 - (4^4)/(5^5)*x^2*(x^2-1)))$
\end{maximablocksmall}

$$y^3 - 3axy + x^3$$

This function contains an extra variable and is, in fact, a family of
algebraic curves.

\begin{maximablock}
analyze_multiple_points(y^3 - 3*a*x*y + x^3)$
\end{maximablock}

$$y^3 - xy - x^2$$

\begin{maximablock}
analyze_multiple_points(y^3 - x*y - x^2)$
\end{maximablock}

$$y^3 - 3x^2y + 2x$$

\begin{maximablock}
analyze_multiple_points(y^3 - 3*x^2*y + 2*x)$
\end{maximablock}

$$y^3 - 3xy + 2x^2$$

\begin{maximablock}
analyze_multiple_points(y^3 - 3*x*y + 2*x^2)$
\end{maximablock}

$$y^3 - 3y + x^6$$

\begin{maximablock}
analyze_multiple_points(y^3 - 3*y + x^6)$
\end{maximablock}

%% \end{comment}

\endexample

% A {\it Riemann surface}, more precisely, is a two-dimensional manifold
% with a {\it complex analytic structure}.

\vfill\eject
\mysection{Riemann-Roch spaces}

A {\it Riemann-Roch space} is a subspace of an algebraic curve's
function field characterized by specifying a minimum order that the
function must obtain at all of the curve's points.  Aside from having
great theoretical significance, Riemann-Roch spaces are practically
useful because they are finite dimensional, and algorithms exist for
constructing Riemann-Roch bases.  Finding a basis for a Riemman-Roch
space in a crucial first step in solving a Mittag-Leffler problem.

Numerous algorithms have been developed for computing bases of
Riemann-Roch spaces.  I've implemented in Maxima one of the oldest,
from \cite{bliss}, though it probably dates back
to \cite{dedekind-weber}.

We begin the process with a ${\mathrm C}(x)$-basis for the entire
function field, namely $\{1, y, \ldots, y^{n-1}\}$.

Next, we want to convert this into a ${\mathrm C}[x]$-basis for the
finite portion of the divisor.  First, we multiple the basis by
whatever polynomials in $x$ are required to place the basis elements
into the divisor's function space, then for each value of $x$ form
a matrix of coefficients, and keep reducing until its determinant is zero.

Finally, we need to adjust this basis to match the divisor's requirements at infinity.

A divisor's basis can be transformed to another basis for the same
divisor by multiplying by a matrix in ${\mathrm C}[x]$ with
determinant a constant not equal to zero. (Bliss Th. 21.1)

If we have a cycle at infinity, multiplying by x will multiply
the expansions by (1/t^r).

{\tt riemannroch(f,x,y,divisor)} computes a basis for the Riemann-Roch
space $L(D)$.  {\tt divisor} is a list of elements, each in the form
{\tt [[$x_i$, $y_i$], $\nu_i$]}, where $(x_i, y_i)$ is a point on the
curve, and $\nu_i$ is the order of the divisor at that point.  For
singular points, either the standard syntax can be used, indicating
that the order of the divisor is the same at all points of the
singularity, or $\nu_i$ can be replaced with a list of values, one for
each sheet at the singularity.  The order of sheets is the same
returned by {\tt puiseux}.  Specifying multiple orders at
singularities with cycles is currently not supported.

\begin{maximacommonsmall}
/* in_namespace(riemannroch)$ */
/* import(puiseux)$ */

riemannroch_trace: false;

extract_one_point(f,x,y,x0,y0,orders,func) :=
  if listp(orders) then block(
    [p: puiseux(f,x,y,x0,y0,apply(max, orders),func)],
    makelist(coeff(p[i][1], t, orders[i]), i, 1, length(orders))
  )
  else
    map(lambda([exp], coeff(exp[1], t, orders)), puiseux(f,x,y,x0,y0,orders,func))
;

matrix_for_basis_at_x0(f,x,y,divisor,basis,x0) :=
 block([ys, cycles, A, required_degrees, pairs, expansions], local(A),
    /* First, compute the y-coordinates of places over x0 */
    ys : map(rhs, solve(subst(x=x0, f), y)),
    ys : unique(ratsimp(ys)),

    if hipow(ratsimp(subst(x=x0,f)),y) < length(basis) then
      ys : append(ys, [inf]),

    /* Now we handle multiple points */
    /* compute sheets in cycle at each point */
    /* XXX: surely we should have a better way than calling puiseux */

    expansions: puiseux(f,x,y,x0,false,0,1),
    cycles: map(lambda([expansion], hipow(lhs(expansion[2]), t)), expansions),

    if apply("+", cycles) # length(basis) then
       error("wrong basis length in riemannroch"),

    /* Next, what degrees does our divisor require for each of these places? */
    /* required_degrees is a list of degrees, the same length as ys */

    required_degrees : map(lambda([y0], assoc([x0,y0], divisor, 0)), ys),

    /* Construct pairs of y-coordinate and required degree */
    /* pairs can contain multiple duplicate y-values (at cycles), */
    /*   but might not be the same length as basis (at singularities) */

    pairs : apply(append, map(lambda([i], if cycles[i] = 1 then [[ys[i], required_degrees[i]]]
                                          else makelist([ys[i],j], j, required_degrees[i], required_degrees[i] + cycles[i] - 1)),
        makelist(i, i, 1, length(ys)))),

    /* Now compute a matrix of the coefficients in the Puiseux expansion */

    transpose(apply(matrix, map(lambda([basis_func],
               apply(append, map(lambda([pair], extract_one_point(f,x,y,x0,pair[1],pair[2],basis_func)),
               pairs))),
        basis)))
);

expansions_at_x0(f,x,y,divisor,basis,x0) :=
 block([ys, A, required_degrees], local(A),
    /* First, compute the y-coordinates of places over x0 */
    ys : map(rhs, solve(subst(x=x0, f), y)),
    /* assert: length(ys) == length(basis) */
    /* XXX handle multiple points */
    if length(ys) # length(basis) then return(0),

    /* Now compute a matrix of the principal parts in the Puiseux expansion */
    A[i,j] := puiseux(f,x,y,x0,ys[i],-1,basis[j]),
    ev(genmatrix(A,length(ys),length(ys)), puiseux_fracexp: false)
);

basis_of_divisor_except_at_inf(f,x,y,divisor) :=
 block([basis,xs],

    /* Find the x's that lie under our divisor or the curve's discriminant */
    xs : map(rhs, solve(discriminant(f, y), x)),
    xs : append(xs, map(first, map(first, divisor))),
    xs : unique(ratsimp(xs)),

    if riemannroch_trace then display(xs),

    /* First, construct a C(x)-basis for all rational functions */
    basis : makelist(y^i, i, 0, hipow(f, y)-1),

    /* Next, adjust the basis to put its elements in the Riemann-Roch space */
    /* Remove poles at xs */
    for x0 in xs do block([target],
      /* What degree does our divisor require for this x0? */
      target: lmax(flatten(cons(0, map(lambda([d], d[2]), sublist(divisor, lambda([d], d[1][1] = x0)))))),
      /* if riemannroch_trace then display(basis), */
      for i: 1 thru length(basis) do
        /* XXX doesn't take ramification into account */
        basis[i] : basis[i] * (x-x0)^max(0, target-lopow(puiseux(f,x,y,x0,false,0,basis[i])[1][1], t))
    ),

    if riemannroch_trace then display(basis),

    /* Finally, reduce until the determinant for the basis is non-zero */
    for x0 in xs do block(
      [unstable: true],
      while unstable do block(
        [m : matrix_for_basis_at_x0(f,x,y,divisor,basis,x0)],
        if riemannroch_trace then block([expansions],
           display(x0),
           expansions : expansions_at_x0(f,x,y,divisor,basis,x0),
           display(expansions),
           display(m)
        ),
        if determinant(m) = 0 then block(
          [v : args(nullspace(m))[1]],
          if riemannroch_trace then display(v),
          basis[sublist_indices (args(transpose(v))[1], lambda ([x], x # 0))[1]] : ratsimp((basis.v) / (x-x0)),
          if riemannroch_trace then display(basis)
        ) else block(
          unstable: false
        )
      )
    ),
    basis
);

/* XXX if divisor specifies a single number at a singularity, doesn't sum it up right */

total_degree(divisor) :=
  lsum(if listp(i) then lsum(j, j, i) else i, i, map(second, divisor))
;

column_orders(f,x,y,func,maxcolord) :=
  /* Could use a function to return the degree of an Puiseux expansion */
  lmin(map(lambda([pe], lopow(pe[1], t)), puiseux(f,x,y,inf,false,maxcolord,func)))
;

normal_matrix_at_inf(f,x,y,basis,colord) := block([expansions, cycles], local(A),

  expansions: puiseux(f,x,y,inf,false,0,1),
  cycles: map(lambda([expansion], hipow(lhs(expansion[2]), t)), expansions),

  if apply("+", cycles) # length(basis) then
     error("wrong basis length in riemannroch"),

  A[j,i] := coeff(  puiseux(f,x,y,inf,false,colord[i],basis[i])  [j][1], t, colord[i]),
  genmatrix(A,length(basis),length(basis))
);

basis_of_divisor_normal_at_inf(f,x,y,divisor) := block([basis, nb, d, colord],
  basis : basis_of_divisor_except_at_inf(f,x,y,divisor),

  /* maximum column order is negative of the divisor's total degree */
  maxcolord : -total_degree(divisor),

  /* this determinant has to be non-zero for the basis to be normal at inf */
  colord : map(lambda([i], column_orders(f,x,y,i,maxcolord)), basis),
  nb : normal_matrix_at_inf(f,x,y,basis,colord),
  d : ratsimp(determinant(nb)),

  if riemannroch_trace then block([expansions],
    display(basis),
    disp("normalizing at infinity"),
    expansions : map(lambda([i], puiseux(f,x,y,inf,false,5,i)), basis),
    display(expansions),
    display(nb),
    display(d)
  ),

  while d = 0 do block([v, vt, sli],
    v : args(nullspace(nb))[1],
    vt : args(transpose(v))[1],

    if riemannroch_trace then block([expansions],
      display(v),
      expansions : map(lambda([i], puiseux(f,x,y,inf,false,5,i)), basis),
      display(expansions),
      display(colord)
    ),

    /* make a list of [index, v-entry, column order] for each function in basis */
    sli : makelist([i,vt[i],colord[i]],i,1,length(basis)),

    /* pick out the subset with non-zero v-entry's */
    sli : sublist(sli, lambda([i], i[2] # 0)),

    /* sort by increasing column order */
    sli : sort(sli, lambda([i,j], orderlessp(i[3],j[3]))),

    /* multiple v by powers of x based on minimum column order from last step */
    v : v * makelist(x^(colord[i]-sli[1][3]),i,1,length(basis)),

    /* rewrite the basis element with lowest column order */
    basis[sli[1][1]] : ratsimp(basis.v),

    colord : map(lambda([i], column_orders(f,x,y,i,maxcolord)), basis),
    nb : normal_matrix_at_inf(f,x,y,basis,colord),
    d : determinant(nb),

    if riemannroch_trace then block(
      display(v),
      display(basis),
      display(nb),
      display(d)
    )
  ),

  if riemannroch_trace then disp("normal at infinity"),

  basis
);

riemannroch2(f,x,y,divisor) := block([basis, maxcolord, colord, a],

  basis : basis_of_divisor_normal_at_inf(f,x,y,divisor),

  /* maximum column order is negative of the divisor's total degree */
  maxcolord : -total_degree(divisor),

  colord : map(lambda([i], column_orders(f,x,y,i,maxcolord)), basis),

  if riemannroch_trace then block(
    disp("normal at infinity"),
    display(colord)
  ),

  flatten(basis * map(lambda([i], makelist(x^j, j, 0, i)), colord))
);

recenter_curve2(f,x,y,a) := block([xcoeff, newf:0],
   f : expand(subst([x=1/x+a], f)),
   f : expand(num(ratsimp(f))),
   for xpow: 0 thru hipow(f, x) do (
      xcoeff : coeff(f, x, xpow),
      for ypow: 0 thru hipow(xcoeff, y) do
         newf : newf + radcan(ratsimp(coeff(xcoeff, y, ypow)))*x^xpow*y^ypow
   ),
   newf
);

load('grobner)$   /* For poly_content() */

simplify_basis(basis,x,y) :=
  map(lambda([e], ratsimp(e/poly_content(e,[x,y]))), basis)
;

riemannroch(f,x,y,divisor) := block([finite_divisor, infinite_divisor, basis, colord],

  finite_divisor : sublist(divisor, lambda([d], first(first(d)) # inf)),
  infinite_divisor : sublist(divisor, lambda([d], first(first(d)) = inf)),

  if length(infinite_divisor) # 0
     or length(puiseux(f,x,y,inf,false,0,1)) # hipow(f,y) then block([xs, a],

     /* need to recenter the curve */
     xs : map(rhs, solve(discriminant(f, y), x)),
     xs : append(xs, map(first, map(first, divisor))),
     xs : unique(xs),

     /* find an integer not in xs */
     xs : sort(sublist(xs, nonnegintegerp)),
     a : if length(xs) # 0 then last(xs)+1 else 0,

     f : recenter_curve2(f,x,y,a),
     finite_divisor : map(lambda([d], [[1/(d[1][1]-a), d[1][2]], d[2]]), finite_divisor),
     infinite_divisor : map(lambda([d], [[0, d[1][2]], d[2]]), infinite_divisor),

     divisor : append(finite_divisor, infinite_divisor),

     if riemannroch_trace then block(
        disp("transforming curve to normalize infinity"),
        display(a),
        display(f),
        display(divisor)
     ),

     basis : riemannroch2(f,x,y,divisor),
     simplify_basis(map(lambda([f], ratsimp(subst([x=1/(x-a)], f))), basis), x, y)

  ) else simplify_basis(riemannroch2(f,x,y,divisor), x, y)
);

/* export(riemannroch)$ */
/* in_namespace(maxima)$ */
/* import(riemannroch)$ */
\end{maximacommonsmall}

Here's a simple example\footnote{From
{\tt https://math.stackexchange.com/questions/294644}}
of a Riemann-Roch space calculation:

\begin{maximablock}
f : y^2 - (x^3 - x);
div : [[[0, 0], -4]];
/* riemannroch_trace : true; */
riemannroch(f, x, y, div);
\end{maximablock}

Here are the examples from \cite{alvanos} \S6.3:

\begin{maximablock}
f : y^2 - x^3 - 1;
P1 : [2,3]$
P2 : [2,-3]$
Pinf : [inf, inf]$

/* riemannroch_trace : true; */
riemannroch(f, x, y, [[Pinf, -1], [P1, 1]]);
riemannroch(f, x, y, [[Pinf, -2], [P1, 1]]);
riemannroch(f, x, y, [[Pinf, -3], [P1, 1]]);
riemannroch(f, x, y, [[Pinf, -4], [P1, 1]]);
riemannroch(f, x, y, [[Pinf, -1]]);
riemannroch(f, x, y, [[Pinf, -2]]);
riemannroch(f, x, y, [[Pinf, -3]]);
\end{maximablock}

\vfill\eject
\mysection{Mittag-Leffler Problems}

Theorem ? tells us that a rational function on an algebraic
curve is completely characterized, up to an additive constant,
by the principal parts of the Puiseux expansions at its poles.

A {\it Mittag-Leffler problem} is the practical application of this
theorem -- given a set of principal parts, find a function that
matches them all, or prove that no such function exists.

The first step in solving a Mittag-Leffler problem is to identify the
maximum strengths of the poles, and construct a basis for a
Riemann-Roch space that includes all functions with poles of such
strength.  We now have a finite basis for a vector space that must
include the function we are looking for.  We construct Puiseux
expansions for the basis functions, and use them to construct
a matrix equation that, when solved, gives the coefficients
needed to form the function we seek from the basis functions.

The input data is a set of principal parts or, alternately, a divisor
combined with a vector of coefficients.

Let's assume that we've got our data in the latter form, so we can run
{\tt riemannroch} on the divisor and obtain a set of basis functions.
Now let's construct a Maxima function to extract the principal parts
of the basis functions and form them into a matrix:

\begin{maximacommon}
principal_parts(f,x,y, func, divisor) :=
  flatten(
  map(lambda([d], map(lambda([e], map(lambda([order], coeff(e[1], t, order)), makelist(order, order, d[2], -1))),
                      puiseux(f, x, y, d[1][1], d[1][2], 0, func))), divisor)
  )
;

principal_parts_matrix(f,x,y, basis, divisor) :=
  transpose(apply(matrix, map(lambda([func], principal_parts(f,x,y,func,divisor)), basis)))
;
\end{maximacommon}

Given a vector {\tt b} of coefficients, we now want to
solve a matrix equation:

\begin{maximablock}
m . v = b;
\end{maximablock}

This will typically be an overspecified system -- a non-square matrix
that may or may not have a solution.  That's fine; since some
integrals have no elementary form, this doesn't represent a limitation
in our theory.  Failure to solve this matrix equation would only show
that no function exists on this curve with the coefficients {\tt b}.

To proceed, we'll use the
{\it Moore-Penrose pseudoinverse}\footnote{\tt https://en.wikipedia.org/wiki/System_of_linear_equations\#Matrix_solution},
which has the property that if a solution exists, it can
be found by multiplying the $b$ vector by the pseudoinverse:

\begin{maximacommon}
pseudoinverse(m) := block([mm, z : ?gensym(), scalarmatrixp : false],
  require_unblockedmatrix(m, "first", "pseudoinverse"),
  mm : m . ctranspose(m),       
  subst(z=0, ratsimp(ctranspose(m) . ratsimp((mm - z * identfor(mm))^^-1)))
);
\end{maximacommon}

To find out if there actually is a solution, we first compute a trial
solution by multiplying the pseudoinverse by {\tt b}, then checking
to see if the trial solution actual solves the original equation.

\example
Let's say that we've identified a divisor on an algebraic
curve (example \ref{an integral Maxima can't solve}):

\begin{maximablock}
f: y^2 - x^8 - 1$
div : [[[%i,  sqrt(2)], -1],
       [[-%i,  sqrt(2)], -1],
       [[%i, -sqrt(2)], -1],
       [[-%i, -sqrt(2)], -1],
       [[inf, inf], -2]]$
basis : riemannroch(f, x, y, div);
\end{maximablock}

We now compute its principal parts matrix:

\begin{maximablock}
m : principal_parts_matrix(f,x,y, basis, div);

pi : pseudoinverse(m);
\end{maximablock}

{\bf TODO}

Introduce a sample vector {\tt b} and show how to proceed.

\endexample
